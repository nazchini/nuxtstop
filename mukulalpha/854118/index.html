<!doctype html>
<html data-n-head-ssr lang="en" data-n-head="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">
  <head>
    <title>System Design 101-Everything to know about Distributed Systems…</title><meta data-n-head="ssr" charset="utf-8"><meta data-n-head="ssr" name="viewport" content="width=device-width,initial-scale=1"><meta data-n-head="ssr" data-hid="description" name="description" content="Using Nuxt.js fetch() hook to build dev.to with a new look"><meta data-n-head="ssr" name="format-detection" content="telephone=no"><base href="/nuxtstop/"><link data-n-head="ssr" rel="icon" type="image/x-icon" href="/favicon.ico"><link data-n-head="ssr" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:400,500,600&display=swap"><link rel="preload" href="/nuxtstop/_nuxt/f6e87fb.js" as="script"><link rel="preload" href="/nuxtstop/_nuxt/6474719.js" as="script"><link rel="preload" href="/nuxtstop/_nuxt/9b75090.js" as="script"><link rel="preload" href="/nuxtstop/_nuxt/18df600.js" as="script"><link rel="preload" href="/nuxtstop/_nuxt/dc9ce94.js" as="script"><style data-vue-ssr-id="c650fd98:0 af4684f0:0 a9c71758:0 dcafa518:0 4b9cec49:0 b093d766:0 9d98bcb4:0 6b6a11ea:0 0248ed80:0 ea8e4264:0">html{box-sizing:border-box;font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}*,:after,:before{box-sizing:inherit}html{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,:after,:before{border:0 solid #e0e0e0}blockquote,body,dd,dl,figure,h1,h2,h3,h4,h5,h6,p,pre{margin:0}button{background:0 0;padding:0}button:focus{outline:1px dotted;outline:5px auto -webkit-focus-ring-color}fieldset,ol,ul{margin:0;padding:0}ol,ul{list-style:none}hr{border-width:1px}img{border-style:solid}textarea{resize:vertical}input::-moz-placeholder,textarea::-moz-placeholder{color:inherit;opacity:.5}input:-ms-input-placeholder,textarea:-ms-input-placeholder{color:inherit;opacity:.5}input::placeholder,textarea::placeholder{color:inherit;opacity:.5}[role=button],button{cursor:pointer}table{border-collapse:collapse}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit;font-family:sans-serif}a{color:inherit;text-decoration:inherit}button,input,optgroup,select,textarea{padding:0;line-height:inherit;color:inherit;font-family:inherit;font-size:100%}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;overflow:auto;word-break:break-word;white-space:normal}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{max-width:100%;height:auto}html{height:100%;font-size:18px;-ms-overflow-style:scrollbar;-webkit-tap-highlight-color:transparent;-webkit-touch-callout:none}@media(min-width:640px){html{font-size:20px}}body{height:100%;min-width:320px;font-family:Inter,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-weight:400;line-height:1.5;color:#000;background-color:#eff4f7;-webkit-text-rendering:optimizeLegibility;text-rendering:optimizeLegibility;font-synthesis:none;font-kerning:normal;font-feature-settings:"normal","kern";-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;-webkit-overflow-scrolling:touch;overflow-x:hidden;overflow-y:scroll}h1,h2,h3,h4,h5,h6{color:#000;font-family:Inter,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-weight:600;font-feature-settings:"normal";line-height:1.2}pre{background:#29292e;border-radius:2px;overflow:auto;padding:1rem;color:#eff1f9;line-height:1.42em;font-size:13px}@media screen and (min-width:380px){pre{font-size:15px}}pre code{background:#29292e;color:#eff0f9;white-space:pre}div.highlight pre.highlight code{font-size:inherit;padding:0}div.inner-comment div.body div.highlight pre.highlight{background:#29292e}div.inner-comment div.body div.highlight pre.highlight code{font-size:inherit;white-space:inherit;background:inherit;color:inherit}.highlight .hll{background-color:#49483e}.highlight{background:#29292e;color:#f8f8f2}.highlight .c{color:grey}.highlight .err{text-shadow:0 0 7px #f9690e}.highlight .k{color:#f39c12}.highlight .l{color:plum}.highlight .n{color:#f8f8f2}.highlight .o{color:#f9690e}.highlight .p{color:#f8f8f2}.highlight .c1,.highlight .ch,.highlight .cm,.highlight .cp,.highlight .cpf,.highlight .cs{color:grey}.highlight .gd{color:#f9690e}.highlight .ge{font-style:italic}.highlight .gi{color:#7ed07e}.highlight .gs{font-weight:700}.highlight .gu{color:grey}.highlight .kc,.highlight .kd{color:#f39c12}.highlight .kn{color:#f9690e}.highlight .kp,.highlight .kr,.highlight .kt{color:#f39c12}.highlight .ld{color:#f2ca27}.highlight .m{color:plum}.highlight .s{color:#f2ca27}.highlight .na{color:#7ed07e}.highlight .nb{color:#f8f8f2}.highlight .nc{color:#7ed07e}.highlight .no{color:#f39c12}.highlight .nd{color:#7ed07e}.highlight .ni{color:#f8f8f2}.highlight .ne,.highlight .nf{color:#7ed07e}.highlight .nl,.highlight .nn{color:#f8f8f2}.highlight .nx{color:#7ed07e}.highlight .py{color:#f8f8f2}.highlight .nt{color:#f9690e}.highlight .nv{color:#f8f8f2}.highlight .ow{color:#f9690e}.highlight .w{color:#f8f8f2}.highlight .mb,.highlight .mf,.highlight .mh,.highlight .mi,.highlight .mo{color:plum}.highlight .dl,.highlight .s2,.highlight .sa,.highlight .sb,.highlight .sc,.highlight .sd{color:#f2ca27}.highlight .se{color:plum}.highlight .s1,.highlight .sh,.highlight .si,.highlight .sr,.highlight .ss,.highlight .sx{color:#f2ca27}.highlight .bp{color:#f8f8f2}.highlight .fm{color:#7ed07e}.highlight .vc,.highlight .vg,.highlight .vi,.highlight .vm{color:#f8f8f2}.highlight .il{color:plum}.vue-content-placeholders-heading__img,.vue-content-placeholders-heading__subtitle,.vue-content-placeholders-heading__title,.vue-content-placeholders-img,.vue-content-placeholders-text__line{background:#bfcdec!important}.vue-content-placeholders-is-animated .vue-content-placeholders-heading__img:before,.vue-content-placeholders-is-animated .vue-content-placeholders-heading__subtitle:before,.vue-content-placeholders-is-animated .vue-content-placeholders-heading__title:before,.vue-content-placeholders-is-animated .vue-content-placeholders-img:before,.vue-content-placeholders-is-animated .vue-content-placeholders-text__line:before{background:linear-gradient(90deg,transparent 0,#d3ddf9 15%,transparent 30%)!important}header[data-v-27046cca]{max-width:1280px;margin:auto;padding:1rem;height:6rem;border-bottom:1px solid rgba(0,0,0,.2)}header .logo-wrapper[data-v-27046cca],header[data-v-27046cca]{display:flex;align-items:center;justify-content:space-between}header .logo-wrapper[data-v-27046cca]{margin:0 .5rem}header .logo-wrapper svg[data-v-27046cca]{width:3rem;height:100%}header .logo-wrapper .name-wrapper[data-v-27046cca]{margin-left:.6em}header .logo-wrapper .name-wrapper .subtitle[data-v-27046cca]{font-size:1rem}header .logo-wrapper .name-wrapper .app-name[data-v-27046cca]{font-weight:700;font-size:2.25rem;line-height:1.25}header nav[data-v-27046cca]{letter-spacing:-.025rem;font-weight:600;text-transform:uppercase}header nav ul[data-v-27046cca]{display:flex}header nav ul li[data-v-27046cca]{margin:0 .5rem}header nav ul li a[data-v-27046cca]{box-shadow:-4px -4px 8px #f8fafe,4px 4px 8px #ced2db;padding:.25rem 1rem;border-radius:.5rem;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}header nav ul li a[data-v-27046cca]:hover{background:linear-gradient(135deg,rgba(0,0,0,.09),hsla(0,0%,100%,0))}header nav ul li a.nuxt-link-exact-active[data-v-27046cca]{cursor:default}header nav ul li a.nuxt-link-exact-active[data-v-27046cca],header nav ul li a[data-v-27046cca]:active{background:0 0;box-shadow:inset -4px -4px 8px #f0f3f9,inset 4px 4px 8px #ced2db,inset -1px -1px 4px #8e8e8e}.page-wrapper[data-v-10d06ee8]{max-width:1280px;margin:auto;padding:1rem}.article-content-wrapper[data-v-10d06ee8]{display:flex;flex-direction:column;align-items:center;margin:auto auto 2rem}@media(min-width:1024px){.article-content-wrapper[data-v-10d06ee8]{align-items:normal;flex-direction:row}}.article-content-wrapper .article-block[data-v-10d06ee8]{width:100%;max-width:880px}@media(min-width:1024px){.article-content-wrapper .article-block[data-v-10d06ee8]{margin-right:1rem;width:66.66666%;margin-bottom:2rem}}.article-content-wrapper .aside-username-wrapper[data-v-10d06ee8]{max-width:880px;width:100%;position:relative}@media(min-width:1024px){.article-content-wrapper .aside-username-wrapper[data-v-10d06ee8]{display:block;width:33.33333%}}.article-content-wrapper .aside-username-wrapper .aside-username-block[data-v-10d06ee8]{position:-webkit-sticky;position:sticky;top:1rem}@media(min-width:1280px){.comments-block[data-v-10d06ee8]{margin:.5rem}}article[data-v-70afb46a]{padding:.5rem;border-radius:1rem}header h1[data-v-70afb46a],header[data-v-70afb46a]{margin-bottom:1rem}header h1[data-v-70afb46a]{font-size:2.25rem;letter-spacing:-.025rem}header .tags[data-v-70afb46a]{display:flex;flex-wrap:wrap;margin-bottom:1.5rem}header .tags .tag[data-v-70afb46a]{font-weight:500;line-height:1;padding:.5rem;margin:0 .5rem .5rem 0;border-radius:.25rem;box-shadow:-4px -4px 8px #f8fafe,4px 4px 8px #ced2db}header .tags .tag[data-v-70afb46a]:hover{background:linear-gradient(135deg,rgba(0,0,0,.09),hsla(0,0%,100%,0))}header .tags .tag[data-v-70afb46a]:active{background:0 0;box-shadow:inset -4px -4px 8px #f0f3f9,inset 4px 4px 8px #ced2db,inset -1px -1px 4px #8e8e8e}header .image-wrapper[data-v-70afb46a]{position:relative;padding-bottom:56.25%;background-color:#d4dfe8;margin-bottom:1.5rem;border-radius:.5rem;overflow:hidden}@media(min-width:834px){header .image-wrapper[data-v-70afb46a]{margin-bottom:1.5rem}}header .image-wrapper img[data-v-70afb46a]{position:absolute;top:0;left:0;width:100%;height:100%;-o-object-fit:cover;object-fit:cover}header .meta[data-v-70afb46a]{line-height:1;font-size:.875rem;text-transform:uppercase;font-weight:500;letter-spacing:-.025rem;display:flex;align-items:center;justify-content:space-between}header .meta .scl[data-v-70afb46a]{display:flex}header .meta .scl span[data-v-70afb46a]{display:flex;align-items:center;margin-right:1rem}header .meta .scl span svg[data-v-70afb46a]{margin-right:.25rem}header .meta .scl .comments[data-v-70afb46a]{cursor:pointer}[data-v-70afb46a] .content .ltag__user{display:none}[data-v-70afb46a] .content iframe{max-width:100%}[data-v-70afb46a] .content h1{font-size:1.875rem}[data-v-70afb46a] .content h1,[data-v-70afb46a] .content h2{margin-top:2rem;margin-bottom:1rem;letter-spacing:-.025rem}[data-v-70afb46a] .content h2{font-size:1.5rem}[data-v-70afb46a] .content h3{font-size:1.25rem}[data-v-70afb46a] .content h3,[data-v-70afb46a] .content h4{margin-top:2rem;margin-bottom:1rem;letter-spacing:-.025rem}[data-v-70afb46a] .content h4{font-size:1rem}[data-v-70afb46a] .content a{color:#6e87d2}[data-v-70afb46a] .content p{margin-bottom:1rem;line-height:1.4}[data-v-70afb46a] .content p code{background-color:#d2f3e1;border-radius:.25rem;padding:.25rem}[data-v-70afb46a] .content img{width:100%;border-radius:.5rem}[data-v-70afb46a] .content .highlight{margin-bottom:1rem;border-radius:.5rem}[data-v-70afb46a] .content ul{list-style:numeral;margin-bottom:1rem}[data-v-70afb46a] .content ul li p{margin-bottom:0}[data-v-70afb46a] .content ol{margin-bottom:1rem}aside[data-v-37984f8c]{padding:1rem;background-color:#dfe8ef;border-radius:1rem}aside .username-heading[data-v-37984f8c]{display:flex;margin-bottom:1rem}aside .username-heading[data-v-37984f8c]:hover{color:#6e87d2}aside .username-heading img[data-v-37984f8c]{width:3rem;height:3rem;border-radius:50%;margin-right:1rem}aside .username-heading .text[data-v-37984f8c]{display:flex;flex-direction:column;justify-content:center}aside .username-heading .text a[data-v-37984f8c]{line-height:1}aside .username-heading .text a[data-v-37984f8c]:first-child{font-size:1.25rem;font-weight:500;letter-spacing:-.025rem;margin-bottom:.25rem}aside .username-heading .text a[data-v-37984f8c]:last-child{color:#999;font-size:.875rem}aside .username-heading.loading[data-v-37984f8c]{display:block}aside .f-button[data-v-37984f8c]{display:block;width:100%;padding:.5rem;border-radius:.5rem;box-shadow:-4px -4px 8px #f8fafe,4px 4px 8px #ced2db;text-transform:uppercase;text-align:center;font-weight:600;letter-spacing:-.025rem;margin-bottom:1rem}aside .f-button[data-v-37984f8c]:hover{background:linear-gradient(135deg,rgba(0,0,0,.09),hsla(0,0%,100%,0))}aside .f-button[data-v-37984f8c]:active{background:0 0;box-shadow:inset -4px -4px 8px #f0f3f9,inset 4px 4px 8px #ced2db,inset -1px -1px 4px #8e8e8e}aside .info>div[data-v-37984f8c]{margin-bottom:.5rem}aside .info .title[data-v-37984f8c]{font-size:.666666rem;letter-spacing:-.0125rem;font-weight:500;color:#999;text-transform:uppercase;margin-bottom:.1rem}aside .info .content[data-v-37984f8c]{font-size:.875rem;line-height:1.4}.add-comment[data-v-8c4375bc]{display:block;width:100%;padding:.5rem;border-radius:.5rem;box-shadow:-4px -4px 8px #f8fafe,4px 4px 8px #ced2db;text-transform:uppercase;text-align:center;font-weight:600;letter-spacing:-.025rem;margin-bottom:1rem}.add-comment[data-v-8c4375bc]:hover{background:linear-gradient(135deg,rgba(0,0,0,.09),hsla(0,0%,100%,0))}.add-comment[data-v-8c4375bc]:active{background:0 0;box-shadow:inset -4px -4px 8px #f0f3f9,inset 4px 4px 8px #ced2db,inset -1px -1px 4px #8e8e8e}footer[data-v-22cb8fd0]{padding:2rem;text-align:center;display:flex;align-items:center;justify-content:center}footer span[data-v-22cb8fd0]{display:inline-block;line-height:1;text-transform:uppercase;letter-spacing:-.025rem;font-size:.75rem;font-weight:500}footer a svg[data-v-22cb8fd0]{width:3rem;height:3rem;margin:0 .5rem}footer a .nuxt-icon[data-v-22cb8fd0]{width:2.5rem;height:2.5rem;margin:0 .25rem}</style>
  </head>
  <body>
    <div data-server-rendered="true" id="__nuxt"><div id="__layout"><div><header data-v-27046cca><a href="/nuxtstop/" class="logo-wrapper nuxt-link-active" data-v-27046cca><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-v-27046cca><path d="M13.5599 8.54348L12.8055 9.87164L10.2257 5.3282L2.306 19.274H7.66815C7.66815 20.0075 8.25298 20.6021 8.97441 20.6021H2.306C1.83937 20.6021 1.40822 20.3489 1.17494 19.9379C0.941664 19.527 0.941687 19.0208 1.175 18.6099L9.09469 4.66412C9.32802 4.25316 9.75926 4 10.226 4C10.6926 4 11.1239 4.25316 11.3572 4.66412L13.5599 8.54348V8.54348Z" fill="#00C58E" data-v-27046cca></path><path d="M19.2769 18.6099L14.3143 9.87165L13.5599 8.54348L12.8055 9.87165L7.84343 18.6099C7.61011 19.0208 7.61009 19.527 7.84337 19.9379C8.07665 20.3489 8.50779 20.6021 8.97443 20.6021H18.1443C18.611 20.6021 19.0424 20.3491 19.2758 19.9382C19.5092 19.5272 19.5092 19.0209 19.2758 18.6099H19.2769ZM8.97443 19.274L13.5599 11.1998L18.1443 19.274H8.97443H8.97443Z" fill="#2F495E" data-v-27046cca></path><path d="M22.825 19.938C22.5917 20.3489 22.1606 20.6021 21.694 20.6021H18.1443C18.8657 20.6021 19.4505 20.0075 19.4505 19.274H21.6913L15.3331 8.07696L14.3142 9.87164L13.5599 8.54348L14.2021 7.41287C14.4354 7.00192 14.8667 6.74875 15.3334 6.74875C15.8001 6.74875 16.2313 7.00192 16.4646 7.41287L22.825 18.6099C23.0583 19.0208 23.0583 19.5271 22.825 19.938V19.938Z" fill="#108775" data-v-27046cca></path></svg> <div class="name-wrapper" data-v-27046cca><span class="app-name" data-v-27046cca>Nuxtstop</span> <p class="subtitle" data-v-27046cca>For all things nuxt.js</p></div></a> <nav data-v-27046cca><ul data-v-27046cca><li data-v-27046cca><a href="/nuxtstop/" class="nuxt-link-active" data-v-27046cca>
          New
        </a></li><li data-v-27046cca><a href="/nuxtstop/top" data-v-27046cca>
          Top
        </a></li></ul></nav></header> <div class="page-wrapper" data-v-10d06ee8><div class="article-content-wrapper" data-v-10d06ee8><article data-fetch-key="data-v-70afb46a:0" class="article-block" data-v-70afb46a data-v-10d06ee8><header data-v-70afb46a><h1 data-v-70afb46a>System Design 101-Everything to know about Distributed Systems…</h1> <div class="tags" data-v-70afb46a><a href="/nuxtstop/t/programming" class="tag" data-v-70afb46a>
          #programming
        </a><a href="/nuxtstop/t/webdev" class="tag" data-v-70afb46a>
          #webdev
        </a><a href="/nuxtstop/t/showdev" class="tag" data-v-70afb46a>
          #showdev
        </a><a href="/nuxtstop/t/challenge" class="tag" data-v-70afb46a>
          #challenge
        </a></div> <div class="image-wrapper" data-v-70afb46a><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--AAufNDl1--/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/h3p8zdhgw38ev4une3ev.png" alt="System Design 101-Everything to know about Distributed Systems…" data-v-70afb46a></div> <div class="meta" data-v-70afb46a><div class="scl" data-v-70afb46a><span data-v-70afb46a><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-v-70afb46a data-v-70afb46a><path d="M16.4444 3C14.6733 3 13.0333 3.94162 12 5.34C10.9667 3.94162 9.32667 3 7.55556 3C4.49222 3 2 5.52338 2 8.625C2 14.8024 11.0267 20.586 11.4122 20.829C11.5922 20.9426 11.7956 21 12 21C12.2044 21 12.4078 20.9426 12.5878 20.829C12.9733 20.586 22 14.8024 22 8.625C22 5.52338 19.5078 3 16.4444 3Z" fill="#FF0000" data-v-70afb46a data-v-70afb46a></path></svg>
            21
          </span> <span class="comments" data-v-70afb46a><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-v-70afb46a data-v-70afb46a><path d="M6.11765 22H4.94118L5.64706 21.05C6.11765 20.3969 6.41176 19.5656 6.58824 18.5563C3.64706 17.1906 2 14.6375 2 11.3125C2 6.20625 5.82353 3 12 3C18.1765 3 22 6.20625 22 11.3125C22 16.5375 18.2353 19.625 12 19.625H11.5882C10.6471 20.7531 9 22 6.11765 22ZM12 4.1875C6.47059 4.1875 3.17647 6.85937 3.17647 11.3125C3.17647 15.1125 5.47059 16.8938 7.41177 17.6656L7.82353 17.8437L7.76471 18.3187C7.64706 19.2687 7.47059 20.1 7.11765 20.8125C9.05882 20.575 10.1765 19.5656 10.8235 18.7344L11 18.4969H12C19.9412 18.4969 20.8235 13.5094 20.8235 11.3719C20.8235 6.85938 17.5294 4.1875 12 4.1875Z" fill="black" data-v-70afb46a data-v-70afb46a></path></svg>
            0
          </span></div> <time data-v-70afb46a>Oct 7 '21</time></div></header> <div class="content" data-v-70afb46a><p>In this Article, we are going to learn about Distributed Systems. Here we are focusing on these things,<br>
.What is Distributed Systems<br>
.How does Distributed Systems work<br>
.Why Distributed Systems<br>
.Characteristic of Distributed Systems<br>
.Challenges of Distributed Systems.<br>
.Risks using Distributed Systems<br>
.Distributed System Architecture & Types<br>
.Distributed Data Stores(Cassandra)<br>
.CAP Theorem<br>
.Distributed Messaging<br>
.Data Copying<br>
.Decentralized vs Distributed Systems<br>
.Cloud vs Distributed Systems</p>

<h3>
  <a name="what-is-a-distributed-system" href="#what-is-a-distributed-system">
  </a>
  <strong>What is a distributed system?</strong>
</h3>

<p>A distributed system in its most simplest definition is a group of computers working together as to appear as a single computer to the end-user.<br>
These machines have a shared state, operate concurrently and can fail independently without affecting the whole system’s uptime.<br>
A distributed system is any network structure that consists of autonomous computers that are connected using a distribution middleware. Distributed systems facilitate sharing different resources and capabilities, to provide users with a single and integrated coherent network.<br>
The opposite of a distributed system is a centralized system. If all of the components of a computing system reside in one machine.<br>
Unlike traditional databases, which are stored on a single machine, in a distributed system, a user must be able to communicate with any machine without knowing it is only one machine. Most applications today use some form of a distributed database and must account for their homogenous or heterogenous nature.<br>
In a homogenous distributed database, each system shares a data model and database management system and data model. Generally, these are easier to manage by adding nodes. On the other hand, heterogeneous databases make it possible to have multiple data models or varied database management systems using gateways to translate data between nodes.</p>

<p><strong>Distributed Information Systems:</strong> distribute information across different servers via multiple communication models.<br>
<strong>Distributed Pervasive Systems:</strong> use embedded computer devices (i.e. ECG monitors, sensors, mobile devices).<br>
<strong>Distributed Computing Systems:</strong> computers in a network communicate via message passing.</p>

<p><em>Distributed systems must have a shared network to connect its components, which could be connected using an IP address or even physical cables.</em></p>

<h3>
  <a name="how-does-a-distributed-system-work" href="#how-does-a-distributed-system-work">
  </a>
  <strong>How does a distributed system work?</strong>
</h3>

<p>Distributed systems have evolved over time, but today’s most common implementations are largely designed to operate via the internet and, more specifically, the cloud. A distributed system begins with a task, such as rendering a video to create a finished product ready for release. The web application, or distributed applications, managing this task — like a video editor on a client computer — splits the job into pieces. In this simple example, the algorithm that gives one frame of the video to each of a dozen different computers (or nodes) to complete the rendering. Once the frame is complete, the managing application gives the node a new frame to work on. This process continues until the video is finished and all the pieces are put back together. A system like this doesn’t have to stop at just 12 nodes — the job may be distributed among hundreds or even thousands of nodes, turning a task that might have taken days for a single computer to complete into one that is finished in a matter of minutes.</p>

<p>There are many models and architectures of distributed systems in use today. Client-server systems, the most traditional and simple type of distributed system, involve a multitude of networked computers that interact with a central server for data storage, processing or other common goal. Cell phone networks are an advanced type of distributed system that share workloads among handsets, switching systems and internet-based devices. Peer-to-peer networks, in which workloads are distributed among hundreds or thousands of computers all running the same software, are another example of a distributed system architecture. The most common forms of distributed systems in the enterprise today are those that operate over the web, handing off workloads to dozens of cloud-based virtual server instances that are created as needed, then terminated when the task is complete.</p>

<h3>
  <a name="why-distribute-a-system" href="#why-distribute-a-system">
  </a>
  <strong>Why distribute a system?</strong>
</h3>

<p>Systems are always distributed by necessity. The truth of the matter is — managing distributed systems is a complex topic chock-full of pitfalls and landmines. It is a headache to deploy, maintain and debug distributed systems, so why go there at all?<br>
What a distributed system enables you to do is scale horizontally. Going back to our previous example of the single database server, the only way to handle more traffic would be to upgrade the hardware the database is running on. This is called scaling vertically.</p>

<p>Scaling vertically is all well and good while you can, but after a certain point you will see that even the best hardware is not sufficient for enough traffic, not to mention impractical to host.</p>

<p>Scaling horizontally simply means adding more computers rather than upgrading the hardware of a single one.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--bssbcr3O--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/89lntk6dcyhja5yeg8tr.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--bssbcr3O--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/89lntk6dcyhja5yeg8tr.png" alt="Alt Text" loading="lazy"></a></p>

<p>It is significantly cheaper than vertical scaling after a certain threshold but that is not its main case for preference.</p>

<p>Vertical scaling can only bump your performance up to the latest hardware’s capabilities. These capabilities prove to be insufficient for technological companies with moderate to big workloads.<br>
The best thing about horizontal scaling is that you have no cap on how much you can scale whenever performance degrades you simply add another machine, up to infinity potentially.<br>
Easy scaling is not the only benefit you get from distributed systems. Fault tolerance and low latency are also equally as important.</p>

<p><strong>Fault Tolerance</strong> - a cluster of ten machines across two data centers is inherently more fault-tolerant than a single machine. Even if one data center catches on fire, your application would still work.</p>

<p><em>An important part of distributed systems is the CAP theorem, which states that a distributed data store cannot simultaneously be consistent, available, and partition tolerant</em>.</p>

<p><strong>Low Latency</strong> — The time for a network packet to travel the world is physically bounded by the speed of light. For example, the shortest possible time for a request‘s <strong>round-trip</strong> time (that is, go back and forth) in a fiber-optic cable between New York to Sydney is 160ms. Distributed systems allow you to have a node in both cities, allowing traffic to hit the node that is closest to it.</p>

<p>For a distributed system to work, though, you need the software running on those machines to be specifically designed for running on multiple computers at the same time and handling the problems that come along with it. This turns out to be no easy feat.</p>

<h2>
  <a name="key-characteristics-of-distributed-systems" href="#key-characteristics-of-distributed-systems">
  </a>
  <strong>Key Characteristics of Distributed Systems</strong>
</h2>

<h3>
  <a name="scalability" href="#scalability">
  </a>
  <strong>Scalability</strong>
</h3>

<p>Scalability is the capability of a system, process, or network to grow and manage increased demand. Any distributed system that can continuously evolve to support the growing amount of work is considered scalable.</p>

<p>A system may have to scale because of many reasons like increased data volume or increased work, e.g., number of transactions. A scalable system would like to achieve this scaling without performance loss.</p>

<p>Generally, although designed (or claimed) to be scalable, the performance of a system declines with the system size due to the management or environmental cost. For instance, network speed may become slower because machines tend to be far apart from one another. More generally, some tasks may not be distributed, either because of their inherent atomic nature or some flaw in the system design. At some point, such tasks would limit the speed-up obtained by distribution. A scalable architecture avoids this situation and attempts to balance the load on all the participating nodes evenly.</p>

<p><strong>Horizontal vs. Vertical Scaling</strong><br>
Horizontal scaling means that you scale by adding more servers into your pool of resources, whereas Vertical scaling means that you scale by adding more power (CPU, RAM, Storage, etc.) to an existing server.</p>

<p>With horizontal scaling, it is often easier to scale dynamically by adding more machines into the existing pool; Vertical-scaling is usually limited to the capacity of a single server. Scaling beyond that capacity often involves downtime and comes with an upper limit.<br>
Good examples of horizontal scaling are <strong>Cassandra **and **MongoDB</strong>, as they both provide an easy way to scale horizontally by adding more machines to meet growing needs. Similarly, a good example of vertical scaling is MySQL, as it allows for an easy way to scale vertically by switching from small to bigger machines. However, this process often involves downtime.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--OuUK0-OC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1vvpawoanwa8gwec0ml9.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--OuUK0-OC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1vvpawoanwa8gwec0ml9.png" alt="Alt Text" loading="lazy"></a></p>

<h3>
  <a name="reliability" href="#reliability">
  </a>
  <strong>Reliability</strong>
</h3>

<p>By definition, reliability is the probability a system will fail in a given period. In simple terms, a distributed system is considered reliable if it keeps delivering its services even when one or several of its software or hardware components fail. Thus, reliability represents one of the main characteristics of any distributed system. Any failing machine can always be replaced by another healthy one in such systems, ensuring the completion of the requested task.</p>

<p>Take the example of a large electronic commerce store (like Amazon), where one of the primary requirements is that any user transaction should never be canceled due to a failure of the machine running that transaction. For instance, if a user has added an item to their shopping cart, the system is expected not to lose it. A reliable distributed system achieves this through the redundancy of both the software components and data. If the server carrying the user’s shopping cart fails, another server with the replica of the shopping cart should replace it.<br>
Redundancy has a cost, and a reliable system has to pay to achieve such resilience for services by eliminating every single point of failure.</p>

<h3>
  <a name="availability" href="#availability">
  </a>
  <strong>Availability</strong>
</h3>

<p>By definition, availability is when a system remains operational to perform its required function in a specific period. It is a simple measure of the percentage of time that a system, service, or machine remains operational under normal conditions. For example, an aircraft that can be flown for many hours a month without much downtime has high availability. Availability takes into account maintainability, repair time, spares availability, and other logistics considerations. If an aircraft is down for maintenance, it is considered not available during that time.</p>

<p>Reliability is availability over time, considering the full range of possible real-world conditions that can occur. For example, an aircraft that can make it through any possible weather safely is more reliable than one that has vulnerabilities to possible conditions.</p>

<h3>
  <a name="reliability-vs-availability" href="#reliability-vs-availability">
  </a>
  <strong>Reliability Vs. Availability</strong>
</h3>

<p>If a system is reliable, it is available. However, if it is available, it is not necessarily reliable.<br>
In other words, high reliability contributes to high availability. Still, it is possible to achieve a high availability even with an unreliable product by minimizing repair time and ensuring that spares are always available when they are needed.</p>

<p>Let’s take the example of an online retail store with 99.99% availability for the first two years after its launch. However, the system was launched without any information security testing. The customers were happy with the system, but they don’t realize that it isn’t very reliable as it is vulnerable to likely risks. In the third year, the system experiences a series of information security incidents that suddenly result in extremely low availability for extended periods. This results in reputational and financial damage to the customers.</p>

<h3>
  <a name="efficiency" href="#efficiency">
  </a>
  <strong>Efficiency</strong>
</h3>

<p>To understand how to measure the efficiency of a distributed system, let’s assume an operation that runs in a distributed manner and delivers a set of items as a result.</p>

<p>Two standard measures of its efficiency are the response time (or latency) that denotes the delay to obtain the first item, and the throughput (or bandwidth), which denotes the number of items delivered in a given time unit (e.g., a second). The two measures correspond to the following unit costs:</p>

<ul>
<li><p>The number of messages globally sent by the nodes of the system, regardless of the message size.</p></li>
<li><p>Size of messages representing the volume of data exchanges.</p></li>
</ul>

<p>The complexity of operations supported by distributed data structures (e.g., searching for a specific key in a distributed index) can be characterized as a function of one of these cost units. Generally speaking, the analysis of a distributed structure in terms of the ‘number of messages’ is over-simplistic. It ignores the impact of many aspects, including the network topology, the network load, variation, the possible heterogeneity of the software and hardware components involved in data processing and routing, etc. However, it is quite difficult to develop a precise cost model that would accurately consider all these performance factors. Therefore, we’ve to live with rough but robust estimates of the system behavior.</p>

<h3>
  <a name="serviceability-or-manageability" href="#serviceability-or-manageability">
  </a>
  <strong>Serviceability or Manageability</strong>
</h3>

<p>Another important consideration while designing a distributed system is how easy it is to operate and maintain. Serviceability or manageability is the simplicity and speed with which a system can be repaired or maintained</p>

<p>If the time to fix a failed system increases, then availability will decrease. Things to consider for manageability are the ease of diagnosing and understanding problems when they occur, ease of making updates or modifications, and how simple the system is to operate (i.e., does it routinely operate without failure or exceptions?).</p>

<p>Early detection of faults can decrease or avoid system downtime. For example, some enterprise systems can automatically call a service center (without human intervention) when the system experiences a system fault.</p>

<h3>
  <a name="challenges-of-distributed-systems" href="#challenges-of-distributed-systems">
  </a>
  <strong>Challenges of distributed systems</strong>
</h3>

<p>Distributed systems are considerably more complex than monolithic computing environments, and raise a number of challenges around design, operations and maintenance. These include:</p>

<p><strong>Increased opportunities for failure:</strong> The more systems added to a computing environment, the more opportunity there is for failure. If a system is not carefully designed and a single node crashes, the entire system can go down. While distributed systems are designed to be fault tolerant, that fault tolerance isn’t automatic or foolproof.</p>

<p><strong>Synchronization process challenges:</strong> Distributed systems work without a global clock, requiring careful programming to ensure that processes are properly synchronized to avoid transmission delays that result in errors and data corruption. In a complex system — such as a multiplayer video game — synchronization can be challenging, especially on a public network that carries data traffic.</p>

<p><strong>Imperfect scalability:</strong> Doubling the number of nodes in a distributed system doesn’t necessarily double performance. Architecting an effective distributed system that maximizes scalability is a complex undertaking that needs to take into account load balancing, bandwidth management and other issues.</p>

<p><strong>More complex security:</strong> Managing a large number of nodes in a heterogeneous or globally distributed environment creates numerous security challenges. A single weak link in a file system or larger distributed system network can expose the entire system to attack.</p>

<p><strong>Increased complexity:</strong> Distributed systems are more complex to design, manage and understand than traditional computing environments.</p>

<h3>
  <a name="risks-of-distributed-systems" href="#risks-of-distributed-systems">
  </a>
  <strong>Risks of distributed systems</strong>
</h3>

<p>The challenges of distributed systems as outlined above create a number of correlating risks. These include:</p>

<p><strong>Security</strong>: Distributed systems are as vulnerable to attack as any other system, but their distributed nature creates a much larger attack surface that exposes organizations to threats.</p>

<p><strong>Risk of network failure:</strong> Distributed systems are beholden to public networks in order to transmit and receive data. If one segment of the internet becomes unavailable or overloaded, distributed system performance may decline.</p>

<p><strong>Governance and control issues:</strong> Distributed systems lack the governability of monolithic, single-server-based systems, creating auditing and adherence issues around global privacy laws such as GDPR. Globally distributed environments can impose barriers to providing certain levels of assurance and impair visibility into where data resides.</p>

<p><strong>Cost control:</strong> Unlike centralized systems, the scalability of distributed systems allows administrators to easily add additional capacity as needed, which can also increase costs. Pricing for cloud-based distributed computing systems are based on usage (such as the number of memory resources and CPU power consumed over time). If demand suddenly spikes, organizations.</p>

<h3>
  <a name="distributed-system-architecture" href="#distributed-system-architecture">
  </a>
  <strong>Distributed System Architecture</strong>
</h3>

<p><strong>Distributed systems must have a network that connects all components (machines, hardware, or software) together so they can transfer messages to communicate with each other.</strong></p>

<ul>
<li><p>That network could be connected with an IP address or use cables or even on a circuit board.</p></li>
<li><p>The messages passed between machines contain forms of data that the systems want to share like databases, objects, and files.</p></li>
<li><p>The way the messages are communicated reliably whether it’s sent, received, acknowledged or how a node retries on failure is an important feature of a distributed system.</p></li>
<li><p>Distributed systems were created out of necessity as services and applications needed to scale and new machines needed to be added and managed. In the design of distributed systems, the major trade-off to consider is complexity vs performance.</p></li>
</ul>

<h3>
  <a name="types-of-distributed-system-architectures" href="#types-of-distributed-system-architectures">
  </a>
  <strong>Types of Distributed System Architectures:</strong>
</h3>

<p>Distributed applications and processes typically use one of four architecture types below:</p>

<p><strong>Client-server:</strong><br>
In the early days, distributed systems architecture consisted of a server as a shared resource like a printer, database, or a web server. It had multiple clients (for example, users behind computers) that decide when to use the shared resource, how to use and display it, change data, and send it back to the server. Code repositories like git is a good example where the intelligence is placed on the developers committing the changes to the code.</p>

<p>Today, distributed systems architecture has evolved with web applications into:</p>

<p><strong>Three-tier:</strong> In this architecture, the clients no longer need to be intelligent and can rely on a middle tier to do the processing and decision making. Most of the first web applications fall under this category. The middle tier could be called an agent that receives requests from clients, that could be stateless, processes the data and then forwards it on to the servers.</p>

<p><strong>Multi-tier:</strong> Enterprise web services first created n-tier or multi-tier systems architectures. This popularized the application servers that contain the business logic and interacts both with the data tiers and presentation tiers.</p>

<p><strong>Peer-to-peer:</strong> There are no centralized or special machine that does the heavy lifting and intelligent work in this architecture. All the decision making and responsibilities are split up amongst the machines involved and each could take on client or server roles. Blockchain is a good example of this.</p>

<h3>
  <a name="distributed-data-stores" href="#distributed-data-stores">
  </a>
  <strong>Distributed Data Stores</strong>
</h3>

<p>Distributed Data Stores are most widely used and recognized as Distributed Databases. Most distributed databases are **NoSQL **non-relational databases, limited to key-value semantics. They provide incredible performance and scalability at the cost of consistency or availability.</p>

<p>We cannot go into discussions of distributed data stores without first introducing the <strong>CAP Theorem.</strong></p>

<p><strong>CAP Theorem</strong><br>
The CAP theorem states that a distributed data store cannot simultaneously be consistent, available and partition tolerant.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--SjQ53v4o--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ru214t75dc7ilm0rp31o.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--SjQ53v4o--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ru214t75dc7ilm0rp31o.png" alt="Alt Text" loading="lazy"></a></p>

<p><strong>Some quick definitions:</strong></p>

<ul>
<li><p>Consistency — What you read and write sequentially is what is expected</p></li>
<li><p>Availability — the whole system does not die — every non-failing node always returns a response.</p></li>
<li><p>Partition Tolerant — The system continues to function and uphold its consistency/availability guarantees in spite of network partitions</p></li>
</ul>

<p>In reality, partition tolerance must be a given for any distributed data store. As mentioned in many places, one of which this great article, you cannot have consistency and availability without partition tolerance.</p>

<p>Think about it: if you have two nodes which accept information and their connection dies — how are they both going to be available and simultaneously provide you with consistency? They have no way of knowing what the other node is doing and as such have can either become offline (unavailable) or work with stale information (inconsistent).</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--i1bzVlmb--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/929a1od2b16l8r4ff384.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i1bzVlmb--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/929a1od2b16l8r4ff384.png" alt="Alt Text" loading="lazy"></a></p>

<p>In the end you’re left to choose if you want your system to be strongly consistent or highly available under a network partition.</p>

<p>Practice shows that most applications value availability more. You do not necessarily always need strong consistency. Even then, that trade-off is not necessarily made because you need the 100% availability guarantee, but rather because network latency can be an issue when having to synchronize machines to achieve strong consistency. These and more factors make applications typically opt for solutions which offer high availability.</p>

<p>Such databases settle with the weakest consistency model — eventual consistency (strong vs eventual consistency explanation). This model guarantees that if no new updates are made to a given item, eventually all accesses to that item will return the latest updated value.</p>

<p>Those systems provide BASE properties (as opposed to traditional databases’ ACID)</p>

<ul>
<li><p><strong>Basically Available</strong> — The system always returns a response</p></li>
<li><p><strong>Soft state</strong> — The system could change over time, even during times of no input (due to eventual consistency)</p></li>
<li><p><strong>Eventual consistency</strong> — In the absence of input, the data will spread to every node sooner or later — thus becoming consistent</p></li>
</ul>

<p>Examples of such available distributed databases — Cassandra, Riak, Voldemort</p>

<p>Of course, there are other data stores which prefer stronger consistency — HBase, Couchbase, Redis, Zookeeper</p>

<p>The CAP theorem is worthy of multiple articles on its own — some regarding how you can tweak a system’s CAP properties depending on how the client behaves and others on how it is not understood properly.</p>

<h3>
  <a name="cassandra" href="#cassandra">
  </a>
  <strong>Cassandra</strong>
</h3>

<p>Cassandra, as mentioned above, is a distributed No-SQL database which prefers the AP properties out of the CAP, settling with eventual consistency. I must admit this may be a bit misleading, as Cassandra is highly configurable — you can make it provide strong consistency at the expense of availability as well, but that is not its common use case.</p>

<p>Cassandra uses consistent hashing to determine which nodes out of your cluster must manage the data you are passing in. You set a replication factor, which basically states to how many nodes you want to replicate your data.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--sFvcF9vo--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1633620691075/BbS74SXnE.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--sFvcF9vo--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1633620691075/BbS74SXnE.png" alt="cassandra.png" loading="lazy"></a></p>

<p>When reading, you will read from those nodes only.</p>

<p>Cassandra is massively scalable, providing absurdly high write throughput.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--6z9gRa4w--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9456g67ui7ubk6tbc9qn.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--6z9gRa4w--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9456g67ui7ubk6tbc9qn.png" alt="Alt Text" loading="lazy"></a></p>

<p>Even though this diagram might be biased and it looks like it compares Cassandra to databases set to provide strong consistency (otherwise I can’t see why MongoDB would drop performance when upgraded from 4 to 8 nodes), this should still show what a properly set up Cassandra cluster is capable of.</p>

<p>Regardless, in the distributed systems trade-off which enables horizontal scaling and incredibly high throughput, Cassandra does not provide some fundamental features of ACID databases — namely, transactions.</p>

<h3>
  <a name="distributed-messaging" href="#distributed-messaging">
  </a>
  <strong>Distributed Messaging</strong>
</h3>

<p>Messaging systems provide a central place for storage and propagation of messages/events inside your overall system. They allow you to decouple your application logic from directly talking with your other systems.</p>

<p>Known Scale — LinkedIn’s Kafka cluster processed 1 trillion messages a day with peaks of 4.5 millions messages a second.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Tn_DrH1u--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5yvo5fb8o0jv9c17ssst.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Tn_DrH1u--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5yvo5fb8o0jv9c17ssst.png" alt="Alt Text" loading="lazy"></a></p>

<p>Simply put, a messaging platform works in the following way:<br>
A message is broadcast from the application which potentially create it (called a producer), goes into the platform and is read by potentially multiple applications which are interested in it (called consumers).</p>

<p>If you need to save a certain event to a few places (e.g user creation to database, warehouse, email sending service and whatever else you can come up with) a messaging platform is the cleanest way to spread that message.</p>

<p>Consumers can either pull information out of the brokers (pull model) or have the brokers push information directly into the consumers (push model).</p>

<p>There are a couple of popular top-notch messaging platforms:<br>
*<em>RabbitMQ *</em>— Message broker which allows you finer-grained control of message trajectories via routing rules and other easily configurable settings. Can be called a smart broker, as it has a lot of logic in it and tightly keeps track of messages that pass through it. Provides settings for both AP and CP from CAP. Uses a push model for notifying the consumers.</p>

<p>*<em>Kafka *</em>— Message broker (and all out platform) which is a bit lower level, as in it does not keep track of which messages have been read and does not allow for complex routing logic. This helps it achieve amazing performance. In my opinion, this is the biggest prospect in this space with active development from the open-source community and support from the Confluent team. Kafka arguably has the most widespread use from top tech companies. I wrote a thorough introduction to this, where I go into detail about all of its goodness.</p>

<p><strong>Apache ActiveMQ</strong> — The oldest of the bunch, dating from 2004. Uses the JMS API, meaning it is geared towards Java EE applications. It got rewritten as ActiveMQ Artemis, which provides outstanding performance on par with Kafka.</p>

<p><strong>Amazon SQS</strong> — A messaging service provided by AWS. Lets you quickly integrate it with existing applications and eliminates the need to handle your own infrastructure, which might be a big benefit, as systems like Kafka are notoriously tricky to set up. Amazon also offers two similar services — SNS and MQ, the latter of which is basically ActiveMQ but managed by Amazon.</p>

<h3>
  <a name="data-copying" href="#data-copying">
  </a>
  <strong>Data Copying</strong>
</h3>

<p>Copying is another foundational concept in computing. In software design we use the concept of taking data copies to achieve speed and resilience in several different ways.</p>

<p><strong>Replicas</strong><br>
A data replica is an exact copy of a database. Replicas are constantly, iteratively synced with one another so their contents are kept as identical as possible. They generally come in two flavours, active and passive. Active replicas support read and write access and play an (unsurprisingly) active role in serving clients. Active replicas can help with scale, resilience and location-base performance. Passive replicas are generally maintained for failover purposes; they don’t support read or write and are only synced in one direction (from the master data). The job of a passive replica is to be ready to take over from the main replica if it fails.</p>

<p><strong>Caches</strong><br>
Unlike a replica, a cache is a not-necessarily-identical, read-only, copy of your data, usually maintained as a way to serve read requests faster and more cheaply than querying your main database. Caches may take advantage of</p>

<ul>
<li><p>Location (keeping information physically close to the reading client).</p></li>
<li><p>Medium (keeping information in faster-to-access physical media like memory rather than disk).</p></li>
<li><p>Design (maintaining information in data structures optimised for fast read like key/value stores or graphs).</p></li>
</ul>

<p><strong>Backups</strong><br>
A backup is a copy of your data for emergency use in the case of catastrophic data loss. Usually a backup is written to a permanent storage medium like disk in multiple physical locations. Backups are slow to write to and restore from and usually they are not kept constantly in sync with the master data. Instead, they are periodically updated. They are useful but have severe limitations and can easily lull folk into a false sense of security. This false security becomes especially dangerous as the scale and complexity of a system increases.</p>

<p><strong>Denormalization for Decoupling</strong><br>
Data duplication goes in and out of fashion as an architectural technique. It is often frowned upon because it’s a common cause of bugs. On the other hand, it is very useful for performance. Basically, like everything in life it has pros and cons and we need to make a judgment on whether to use it based on the current circumstances.</p>

<p>A decade ago we spent ages “normalizing” our databases, or removing data duplication. However in a distributed system we really need to have data available in more than one location. Every service cannot be calling back to the same database all the time or we lose all the benefits of distribution. Microservices usually maintain local equivalents of certain pieces of data that they can write and read without worrying about anyone else.</p>

<p>BUT the potential bugs have not mysteriously gone away. We have to somehow keep all those data copies in alignment (that doesn’t necessarily mean identical). By using data duplication we have to accept that we’ve just included an architectural technique that can be very bug prone in our design. It’s a tradeoff. It just means that we have to use development techniques that help resolve those problems such as Domain Driven Design, or operational techniques like managed stateful services that handle some of these issues for us.</p>

<p><strong>Decentralized vs Distributed</strong><br>
Before we go any further I’d like to make a distinction between the two terms.</p>

<p>Even though the words sound similar and can be concluded to mean the same logically, their difference makes a significant technological and political impact.</p>

<p><strong>Decentralized **is still **distributed</strong> in the technical sense, but the whole decentralized systems is not owned by one actor. No one company can own a decentralized system, otherwise it wouldn’t be decentralized anymore.</p>

<p>This means that most systems we will go over today can be thought of as <strong>distributed centralized systems</strong> — and that is what they’re made to be.</p>

<p>Decentralized is essentially distributed on a technical level, but usually a decentralized system is not owned by a single source.</p>

<p>If you think about it — it is harder to create a decentralized system because then you need to handle the case where some of the participants are malicious. This is not the case with normal distributed systems, as you know you own all the nodes.</p>

<p><em>This has been debated a lot and can be confused with others (peer-to-peer, federated). In early literature, it’s been defined differently as well. Regardless, what I gave you as a definition is what I feel is the most widely used now that blockchain and cryptocurrencies popularized the term</em>.</p>

<p><strong>Cloud vs distributed systems</strong><br>
Cloud computing and distributed systems are different, but they use similar concepts. Distributed computing uses distributed systems by spreading tasks across many machines. Cloud computing, on the other hand, uses network hosted servers for storage, process, data management.</p>

<p>Distributed computing aims to create collaborative resource sharing and provide size and geographical scalability. Cloud computing is about delivering an on demand environment using transparency, monitoring, and security.</p>

<p>Compared to distributed systems, cloud computing offers the following advantages:</p>

<ul>
<li><p>Cost effective</p></li>
<li><p>Access to a global market</p></li>
<li><p>Encapsulated change management</p></li>
<li><p>Access storage, servers, and databases on the internet</p></li>
</ul>

<p>However, cloud computing is arguably less flexible than distributed computing, as you rely on other services and technologies to build a system. This gives you less control overall.</p>

<p>Priorities like load-balancing, replication, auto-scaling, and automated back-ups can be made easy with cloud computing. Cloud building tools like Docker, Amazon Web Services (AWS), Google Cloud Services, or Azure make it possible to create such systems quickly, and many teams opt to build distributed systems alongside these technologies.</p>

</div></article> <div class="aside-username-wrapper" data-v-10d06ee8><aside class="aside-username-block" data-v-37984f8c data-v-10d06ee8><div class="username-heading loading" data-v-37984f8c><div class="vue-content-placeholders vue-content-placeholders-is-animated" data-v-37984f8c><div class="vue-content-placeholders-heading" data-v-37984f8c><div class="vue-content-placeholders-heading__img"></div> <div class="vue-content-placeholders-heading__content"><div class="vue-content-placeholders-heading__title"></div> <div class="vue-content-placeholders-heading__subtitle"></div></div></div></div></div> <div class="info" data-v-37984f8c><div class="vue-content-placeholders vue-content-placeholders-is-animated" data-v-37984f8c><div class="vue-content-placeholders-text" data-v-37984f8c><div class="vue-content-placeholders-text__line"></div><div class="vue-content-placeholders-text__line"></div><div class="vue-content-placeholders-text__line"></div></div></div></div></aside></div></div> <div class="comments-block" data-v-8c4375bc data-v-10d06ee8><!----> <a href="https://dev.to/mukulalpha/system-design-101-everything-to-know-about-distributed-systems-2hd6" target="_blank" rel="nofollow noopener noreferer" class="add-comment" data-v-8c4375bc>
    Add comment
  </a></div></div> <footer data-v-22cb8fd0><span data-v-22cb8fd0>Built with</span> <a href="https://nuxtjs.org" target="_blank" data-v-22cb8fd0><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" class="nuxt-icon" data-v-22cb8fd0 data-v-22cb8fd0><path d="M13.5599 8.54348L12.8055 9.87164L10.2257 5.3282L2.306 19.274H7.66815C7.66815 20.0075 8.25298 20.6021 8.97441 20.6021H2.306C1.83937 20.6021 1.40822 20.3489 1.17494 19.9379C0.941664 19.527 0.941687 19.0208 1.175 18.6099L9.09469 4.66412C9.32802 4.25316 9.75926 4 10.226 4C10.6926 4 11.1239 4.25316 11.3572 4.66412L13.5599 8.54348V8.54348Z" fill="#00C58E" data-v-22cb8fd0 data-v-22cb8fd0></path><path d="M19.2769 18.6099L14.3143 9.87165L13.5599 8.54348L12.8055 9.87165L7.84343 18.6099C7.61011 19.0208 7.61009 19.527 7.84337 19.9379C8.07665 20.3489 8.50779 20.6021 8.97443 20.6021H18.1443C18.611 20.6021 19.0424 20.3491 19.2758 19.9382C19.5092 19.5272 19.5092 19.0209 19.2758 18.6099H19.2769ZM8.97443 19.274L13.5599 11.1998L18.1443 19.274H8.97443H8.97443Z" fill="#2F495E" data-v-22cb8fd0 data-v-22cb8fd0></path><path d="M22.825 19.938C22.5917 20.3489 22.1606 20.6021 21.694 20.6021H18.1443C18.8657 20.6021 19.4505 20.0075 19.4505 19.274H21.6913L15.3331 8.07696L14.3142 9.87164L13.5599 8.54348L14.2021 7.41287C14.4354 7.00192 14.8667 6.74875 15.3334 6.74875C15.8001 6.74875 16.2313 7.00192 16.4646 7.41287L22.825 18.6099C23.0583 19.0208 23.0583 19.5271 22.825 19.938V19.938Z" fill="#108775" data-v-22cb8fd0 data-v-22cb8fd0></path></svg></a> <span data-v-22cb8fd0>&</span> <a href="https://docs.dev.to/api" rel="nofollow noopener" target="_blank" data-v-22cb8fd0><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-v-22cb8fd0 data-v-22cb8fd0><path d="M1.5726 5.13748C1.42945 5.20622 1.2411 5.36661 1.15822 5.48117C1 5.69503 1 5.74849 1 11.8739C1 17.9993 1 18.0528 1.15822 18.2667C1.2411 18.3812 1.42945 18.5416 1.5726 18.6104C1.8137 18.7402 2.46164 18.7478 12 18.7478C21.5384 18.7478 22.1863 18.7402 22.4274 18.6104C22.5706 18.5416 22.7589 18.3812 22.8418 18.2667C23 18.0528 23 17.9993 23 11.8739C23 5.74849 23 5.69503 22.8418 5.48117C22.7589 5.36661 22.5706 5.20622 22.4274 5.13748C22.1863 5.00764 21.5384 5 12 5C2.46164 5 1.8137 5.00764 1.5726 5.13748ZM7.7055 8.2613C8.0822 8.45989 8.59454 9.0098 8.77536 9.40694C8.89589 9.66664 8.91095 9.94922 8.91095 12.0649C8.91095 14.3104 8.90344 14.4478 8.75275 14.7839C8.51919 15.288 8.16506 15.6546 7.68288 15.899C7.26096 16.1052 7.22328 16.1128 5.7315 16.1358L4.20206 16.1663V12.1031V8.04744L5.80684 8.07035C7.27602 8.09327 7.42672 8.10854 7.7055 8.2613ZM13.6952 8.89521V9.73538H12.4521H11.2089V10.4991V11.2629H11.9623H12.7158V12.1031V12.9432H11.9623H11.2089V13.707V14.4708H12.4521H13.6952V15.3109V16.151H12C10.1315 16.151 10.0411 16.1358 9.67191 15.6928L9.47603 15.4484V12.1336C9.47603 8.46752 9.46851 8.49807 9.95069 8.20783C10.1692 8.07035 10.3425 8.05508 11.9473 8.05508H13.6952V8.89521ZM16.5658 10.3769C16.8897 11.6295 17.1685 12.6912 17.176 12.7293C17.1911 12.7675 17.4699 11.7441 17.8014 10.461C18.1254 9.17017 18.4343 8.1009 18.4795 8.08563C18.5247 8.06271 18.9541 8.06271 19.4288 8.07035L20.3028 8.09327L19.376 11.6219C18.8713 13.5542 18.4117 15.2269 18.3664 15.3261C18.0123 16.0135 17.274 16.3343 16.7164 16.0441C16.4528 15.899 16.0911 15.4865 15.9705 15.1887C15.9254 15.0665 15.4884 13.4549 15.0062 11.6142C14.524 9.76593 14.1171 8.20783 14.0945 8.15437C14.0644 8.07035 14.2301 8.05508 15.0212 8.07035L15.9856 8.09327L16.5658 10.3769Z" fill="black" data-v-22cb8fd0 data-v-22cb8fd0></path><path d="M5.93491 12.103V14.4707H6.27394C6.66574 14.4707 7.01983 14.3103 7.1404 14.0965C7.18559 14.0048 7.21575 13.2105 7.21575 12.0648V10.1783L6.99725 9.95683C6.80133 9.76591 6.71847 9.73535 6.35683 9.73535H5.93491V12.103Z" fill="black" data-v-22cb8fd0 data-v-22cb8fd0></path></svg></a></footer></div></div></div><script>window.__NUXT__=function(e,t,a,s,i,n){return a.type_of="article",a.id=854118,a.title="System Design 101-Everything to know about Distributed Systems…",a.description="In this Article, we are going to learn about Distributed Systems. Here we are focusing on these...",a.readable_publish_date="Oct 7 '21",a.slug="system-design-101-everything-to-know-about-distributed-systems-2hd6",a.path="/mukulalpha/system-design-101-everything-to-know-about-distributed-systems-2hd6",a.url=s,a.comments_count=0,a.public_reactions_count=21,a.collection_id=e,a.published_timestamp=t,a.positive_reactions_count=21,a.cover_image="https://res.cloudinary.com/practicaldev/image/fetch/s--AAufNDl1--/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/h3p8zdhgw38ev4une3ev.png",a.social_image="https://res.cloudinary.com/practicaldev/image/fetch/s--9Wa4MplS--/c_imagga_scale,f_auto,fl_progressive,h_500,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/h3p8zdhgw38ev4une3ev.png",a.canonical_url=s,a.created_at="2021-10-06T17:31:33Z",a.edited_at="2022-03-20T11:40:05Z",a.crossposted_at=e,a.published_at=t,a.last_comment_at=t,a.reading_time_minutes=19,a.tag_list="programming, webdev, showdev, challenge",a.tags=["programming","webdev",i,"challenge"],a.body_html='<p>In this Article, we are going to learn about Distributed Systems. Here we are focusing on these things,<br>\n.What is Distributed Systems<br>\n.How does Distributed Systems work<br>\n.Why Distributed Systems<br>\n.Characteristic of Distributed Systems<br>\n.Challenges of Distributed Systems.<br>\n.Risks using Distributed Systems<br>\n.Distributed System Architecture &amp; Types<br>\n.Distributed Data Stores(Cassandra)<br>\n.CAP Theorem<br>\n.Distributed Messaging<br>\n.Data Copying<br>\n.Decentralized vs Distributed Systems<br>\n.Cloud vs Distributed Systems</p>\n\n<h3>\n  <a name="what-is-a-distributed-system" href="#what-is-a-distributed-system">\n  </a>\n  <strong>What is a distributed system?</strong>\n</h3>\n\n<p>A distributed system in its most simplest definition is a group of computers working together as to appear as a single computer to the end-user.<br>\nThese machines have a shared state, operate concurrently and can fail independently without affecting the whole system’s uptime.<br>\nA distributed system is any network structure that consists of autonomous computers that are connected using a distribution middleware. Distributed systems facilitate sharing different resources and capabilities, to provide users with a single and integrated coherent network.<br>\nThe opposite of a distributed system is a centralized system. If all of the components of a computing system reside in one machine.<br>\nUnlike traditional databases, which are stored on a single machine, in a distributed system, a user must be able to communicate with any machine without knowing it is only one machine. Most applications today use some form of a distributed database and must account for their homogenous or heterogenous nature.<br>\nIn a homogenous distributed database, each system shares a data model and database management system and data model. Generally, these are easier to manage by adding nodes. On the other hand, heterogeneous databases make it possible to have multiple data models or varied database management systems using gateways to translate data between nodes.</p>\n\n<p><strong>Distributed Information Systems:</strong> distribute information across different servers via multiple communication models.<br>\n<strong>Distributed Pervasive Systems:</strong> use embedded computer devices (i.e. ECG monitors, sensors, mobile devices).<br>\n<strong>Distributed Computing Systems:</strong> computers in a network communicate via message passing.</p>\n\n<p><em>Distributed systems must have a shared network to connect its components, which could be connected using an IP address or even physical cables.</em></p>\n\n<h3>\n  <a name="how-does-a-distributed-system-work" href="#how-does-a-distributed-system-work">\n  </a>\n  <strong>How does a distributed system work?</strong>\n</h3>\n\n<p>Distributed systems have evolved over time, but today’s most common implementations are largely designed to operate via the internet and, more specifically, the cloud. A distributed system begins with a task, such as rendering a video to create a finished product ready for release. The web application, or distributed applications, managing this task — like a video editor on a client computer — splits the job into pieces. In this simple example, the algorithm that gives one frame of the video to each of a dozen different computers (or nodes) to complete the rendering. Once the frame is complete, the managing application gives the node a new frame to work on. This process continues until the video is finished and all the pieces are put back together. A system like this doesn’t have to stop at just 12 nodes — the job may be distributed among hundreds or even thousands of nodes, turning a task that might have taken days for a single computer to complete into one that is finished in a matter of minutes.</p>\n\n<p>There are many models and architectures of distributed systems in use today. Client-server systems, the most traditional and simple type of distributed system, involve a multitude of networked computers that interact with a central server for data storage, processing or other common goal. Cell phone networks are an advanced type of distributed system that share workloads among handsets, switching systems and internet-based devices. Peer-to-peer networks, in which workloads are distributed among hundreds or thousands of computers all running the same software, are another example of a distributed system architecture. The most common forms of distributed systems in the enterprise today are those that operate over the web, handing off workloads to dozens of cloud-based virtual server instances that are created as needed, then terminated when the task is complete.</p>\n\n<h3>\n  <a name="why-distribute-a-system" href="#why-distribute-a-system">\n  </a>\n  <strong>Why distribute a system?</strong>\n</h3>\n\n<p>Systems are always distributed by necessity. The truth of the matter is — managing distributed systems is a complex topic chock-full of pitfalls and landmines. It is a headache to deploy, maintain and debug distributed systems, so why go there at all?<br>\nWhat a distributed system enables you to do is scale horizontally. Going back to our previous example of the single database server, the only way to handle more traffic would be to upgrade the hardware the database is running on. This is called scaling vertically.</p>\n\n<p>Scaling vertically is all well and good while you can, but after a certain point you will see that even the best hardware is not sufficient for enough traffic, not to mention impractical to host.</p>\n\n<p>Scaling horizontally simply means adding more computers rather than upgrading the hardware of a single one.</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--bssbcr3O--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/89lntk6dcyhja5yeg8tr.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--bssbcr3O--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/89lntk6dcyhja5yeg8tr.png" alt="Alt Text" loading="lazy"></a></p>\n\n<p>It is significantly cheaper than vertical scaling after a certain threshold but that is not its main case for preference.</p>\n\n<p>Vertical scaling can only bump your performance up to the latest hardware’s capabilities. These capabilities prove to be insufficient for technological companies with moderate to big workloads.<br>\nThe best thing about horizontal scaling is that you have no cap on how much you can scale whenever performance degrades you simply add another machine, up to infinity potentially.<br>\nEasy scaling is not the only benefit you get from distributed systems. Fault tolerance and low latency are also equally as important.</p>\n\n<p><strong>Fault Tolerance</strong> - a cluster of ten machines across two data centers is inherently more fault-tolerant than a single machine. Even if one data center catches on fire, your application would still work.</p>\n\n<p><em>An important part of distributed systems is the CAP theorem, which states that a distributed data store cannot simultaneously be consistent, available, and partition tolerant</em>.</p>\n\n<p><strong>Low Latency</strong> — The time for a network packet to travel the world is physically bounded by the speed of light. For example, the shortest possible time for a request‘s <strong>round-trip</strong> time (that is, go back and forth) in a fiber-optic cable between New York to Sydney is 160ms. Distributed systems allow you to have a node in both cities, allowing traffic to hit the node that is closest to it.</p>\n\n<p>For a distributed system to work, though, you need the software running on those machines to be specifically designed for running on multiple computers at the same time and handling the problems that come along with it. This turns out to be no easy feat.</p>\n\n<h2>\n  <a name="key-characteristics-of-distributed-systems" href="#key-characteristics-of-distributed-systems">\n  </a>\n  <strong>Key Characteristics of Distributed Systems</strong>\n</h2>\n\n<h3>\n  <a name="scalability" href="#scalability">\n  </a>\n  <strong>Scalability</strong>\n</h3>\n\n<p>Scalability is the capability of a system, process, or network to grow and manage increased demand. Any distributed system that can continuously evolve to support the growing amount of work is considered scalable.</p>\n\n<p>A system may have to scale because of many reasons like increased data volume or increased work, e.g., number of transactions. A scalable system would like to achieve this scaling without performance loss.</p>\n\n<p>Generally, although designed (or claimed) to be scalable, the performance of a system declines with the system size due to the management or environmental cost. For instance, network speed may become slower because machines tend to be far apart from one another. More generally, some tasks may not be distributed, either because of their inherent atomic nature or some flaw in the system design. At some point, such tasks would limit the speed-up obtained by distribution. A scalable architecture avoids this situation and attempts to balance the load on all the participating nodes evenly.</p>\n\n<p><strong>Horizontal vs. Vertical Scaling</strong><br>\nHorizontal scaling means that you scale by adding more servers into your pool of resources, whereas Vertical scaling means that you scale by adding more power (CPU, RAM, Storage, etc.) to an existing server.</p>\n\n<p>With horizontal scaling, it is often easier to scale dynamically by adding more machines into the existing pool; Vertical-scaling is usually limited to the capacity of a single server. Scaling beyond that capacity often involves downtime and comes with an upper limit.<br>\nGood examples of horizontal scaling are <strong>Cassandra **and **MongoDB</strong>, as they both provide an easy way to scale horizontally by adding more machines to meet growing needs. Similarly, a good example of vertical scaling is MySQL, as it allows for an easy way to scale vertically by switching from small to bigger machines. However, this process often involves downtime.</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--OuUK0-OC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1vvpawoanwa8gwec0ml9.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--OuUK0-OC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1vvpawoanwa8gwec0ml9.png" alt="Alt Text" loading="lazy"></a></p>\n\n<h3>\n  <a name="reliability" href="#reliability">\n  </a>\n  <strong>Reliability</strong>\n</h3>\n\n<p>By definition, reliability is the probability a system will fail in a given period. In simple terms, a distributed system is considered reliable if it keeps delivering its services even when one or several of its software or hardware components fail. Thus, reliability represents one of the main characteristics of any distributed system. Any failing machine can always be replaced by another healthy one in such systems, ensuring the completion of the requested task.</p>\n\n<p>Take the example of a large electronic commerce store (like Amazon), where one of the primary requirements is that any user transaction should never be canceled due to a failure of the machine running that transaction. For instance, if a user has added an item to their shopping cart, the system is expected not to lose it. A reliable distributed system achieves this through the redundancy of both the software components and data. If the server carrying the user’s shopping cart fails, another server with the replica of the shopping cart should replace it.<br>\nRedundancy has a cost, and a reliable system has to pay to achieve such resilience for services by eliminating every single point of failure.</p>\n\n<h3>\n  <a name="availability" href="#availability">\n  </a>\n  <strong>Availability</strong>\n</h3>\n\n<p>By definition, availability is when a system remains operational to perform its required function in a specific period. It is a simple measure of the percentage of time that a system, service, or machine remains operational under normal conditions. For example, an aircraft that can be flown for many hours a month without much downtime has high availability. Availability takes into account maintainability, repair time, spares availability, and other logistics considerations. If an aircraft is down for maintenance, it is considered not available during that time.</p>\n\n<p>Reliability is availability over time, considering the full range of possible real-world conditions that can occur. For example, an aircraft that can make it through any possible weather safely is more reliable than one that has vulnerabilities to possible conditions.</p>\n\n<h3>\n  <a name="reliability-vs-availability" href="#reliability-vs-availability">\n  </a>\n  <strong>Reliability Vs. Availability</strong>\n</h3>\n\n<p>If a system is reliable, it is available. However, if it is available, it is not necessarily reliable.<br>\nIn other words, high reliability contributes to high availability. Still, it is possible to achieve a high availability even with an unreliable product by minimizing repair time and ensuring that spares are always available when they are needed.</p>\n\n<p>Let’s take the example of an online retail store with 99.99% availability for the first two years after its launch. However, the system was launched without any information security testing. The customers were happy with the system, but they don’t realize that it isn’t very reliable as it is vulnerable to likely risks. In the third year, the system experiences a series of information security incidents that suddenly result in extremely low availability for extended periods. This results in reputational and financial damage to the customers.</p>\n\n<h3>\n  <a name="efficiency" href="#efficiency">\n  </a>\n  <strong>Efficiency</strong>\n</h3>\n\n<p>To understand how to measure the efficiency of a distributed system, let’s assume an operation that runs in a distributed manner and delivers a set of items as a result.</p>\n\n<p>Two standard measures of its efficiency are the response time (or latency) that denotes the delay to obtain the first item, and the throughput (or bandwidth), which denotes the number of items delivered in a given time unit (e.g., a second). The two measures correspond to the following unit costs:</p>\n\n<ul>\n<li><p>The number of messages globally sent by the nodes of the system, regardless of the message size.</p></li>\n<li><p>Size of messages representing the volume of data exchanges.</p></li>\n</ul>\n\n<p>The complexity of operations supported by distributed data structures (e.g., searching for a specific key in a distributed index) can be characterized as a function of one of these cost units. Generally speaking, the analysis of a distributed structure in terms of the ‘number of messages’ is over-simplistic. It ignores the impact of many aspects, including the network topology, the network load, variation, the possible heterogeneity of the software and hardware components involved in data processing and routing, etc. However, it is quite difficult to develop a precise cost model that would accurately consider all these performance factors. Therefore, we’ve to live with rough but robust estimates of the system behavior.</p>\n\n<h3>\n  <a name="serviceability-or-manageability" href="#serviceability-or-manageability">\n  </a>\n  <strong>Serviceability or Manageability</strong>\n</h3>\n\n<p>Another important consideration while designing a distributed system is how easy it is to operate and maintain. Serviceability or manageability is the simplicity and speed with which a system can be repaired or maintained</p>\n\n<p>If the time to fix a failed system increases, then availability will decrease. Things to consider for manageability are the ease of diagnosing and understanding problems when they occur, ease of making updates or modifications, and how simple the system is to operate (i.e., does it routinely operate without failure or exceptions?).</p>\n\n<p>Early detection of faults can decrease or avoid system downtime. For example, some enterprise systems can automatically call a service center (without human intervention) when the system experiences a system fault.</p>\n\n<h3>\n  <a name="challenges-of-distributed-systems" href="#challenges-of-distributed-systems">\n  </a>\n  <strong>Challenges of distributed systems</strong>\n</h3>\n\n<p>Distributed systems are considerably more complex than monolithic computing environments, and raise a number of challenges around design, operations and maintenance. These include:</p>\n\n<p><strong>Increased opportunities for failure:</strong> The more systems added to a computing environment, the more opportunity there is for failure. If a system is not carefully designed and a single node crashes, the entire system can go down. While distributed systems are designed to be fault tolerant, that fault tolerance isn’t automatic or foolproof.</p>\n\n<p><strong>Synchronization process challenges:</strong> Distributed systems work without a global clock, requiring careful programming to ensure that processes are properly synchronized to avoid transmission delays that result in errors and data corruption. In a complex system — such as a multiplayer video game — synchronization can be challenging, especially on a public network that carries data traffic.</p>\n\n<p><strong>Imperfect scalability:</strong> Doubling the number of nodes in a distributed system doesn’t necessarily double performance. Architecting an effective distributed system that maximizes scalability is a complex undertaking that needs to take into account load balancing, bandwidth management and other issues.</p>\n\n<p><strong>More complex security:</strong> Managing a large number of nodes in a heterogeneous or globally distributed environment creates numerous security challenges. A single weak link in a file system or larger distributed system network can expose the entire system to attack.</p>\n\n<p><strong>Increased complexity:</strong> Distributed systems are more complex to design, manage and understand than traditional computing environments.</p>\n\n<h3>\n  <a name="risks-of-distributed-systems" href="#risks-of-distributed-systems">\n  </a>\n  <strong>Risks of distributed systems</strong>\n</h3>\n\n<p>The challenges of distributed systems as outlined above create a number of correlating risks. These include:</p>\n\n<p><strong>Security</strong>: Distributed systems are as vulnerable to attack as any other system, but their distributed nature creates a much larger attack surface that exposes organizations to threats.</p>\n\n<p><strong>Risk of network failure:</strong> Distributed systems are beholden to public networks in order to transmit and receive data. If one segment of the internet becomes unavailable or overloaded, distributed system performance may decline.</p>\n\n<p><strong>Governance and control issues:</strong> Distributed systems lack the governability of monolithic, single-server-based systems, creating auditing and adherence issues around global privacy laws such as GDPR. Globally distributed environments can impose barriers to providing certain levels of assurance and impair visibility into where data resides.</p>\n\n<p><strong>Cost control:</strong> Unlike centralized systems, the scalability of distributed systems allows administrators to easily add additional capacity as needed, which can also increase costs. Pricing for cloud-based distributed computing systems are based on usage (such as the number of memory resources and CPU power consumed over time). If demand suddenly spikes, organizations.</p>\n\n<h3>\n  <a name="distributed-system-architecture" href="#distributed-system-architecture">\n  </a>\n  <strong>Distributed System Architecture</strong>\n</h3>\n\n<p><strong>Distributed systems must have a network that connects all components (machines, hardware, or software) together so they can transfer messages to communicate with each other.</strong></p>\n\n<ul>\n<li><p>That network could be connected with an IP address or use cables or even on a circuit board.</p></li>\n<li><p>The messages passed between machines contain forms of data that the systems want to share like databases, objects, and files.</p></li>\n<li><p>The way the messages are communicated reliably whether it’s sent, received, acknowledged or how a node retries on failure is an important feature of a distributed system.</p></li>\n<li><p>Distributed systems were created out of necessity as services and applications needed to scale and new machines needed to be added and managed. In the design of distributed systems, the major trade-off to consider is complexity vs performance.</p></li>\n</ul>\n\n<h3>\n  <a name="types-of-distributed-system-architectures" href="#types-of-distributed-system-architectures">\n  </a>\n  <strong>Types of Distributed System Architectures:</strong>\n</h3>\n\n<p>Distributed applications and processes typically use one of four architecture types below:</p>\n\n<p><strong>Client-server:</strong><br>\nIn the early days, distributed systems architecture consisted of a server as a shared resource like a printer, database, or a web server. It had multiple clients (for example, users behind computers) that decide when to use the shared resource, how to use and display it, change data, and send it back to the server. Code repositories like git is a good example where the intelligence is placed on the developers committing the changes to the code.</p>\n\n<p>Today, distributed systems architecture has evolved with web applications into:</p>\n\n<p><strong>Three-tier:</strong> In this architecture, the clients no longer need to be intelligent and can rely on a middle tier to do the processing and decision making. Most of the first web applications fall under this category. The middle tier could be called an agent that receives requests from clients, that could be stateless, processes the data and then forwards it on to the servers.</p>\n\n<p><strong>Multi-tier:</strong> Enterprise web services first created n-tier or multi-tier systems architectures. This popularized the application servers that contain the business logic and interacts both with the data tiers and presentation tiers.</p>\n\n<p><strong>Peer-to-peer:</strong> There are no centralized or special machine that does the heavy lifting and intelligent work in this architecture. All the decision making and responsibilities are split up amongst the machines involved and each could take on client or server roles. Blockchain is a good example of this.</p>\n\n<h3>\n  <a name="distributed-data-stores" href="#distributed-data-stores">\n  </a>\n  <strong>Distributed Data Stores</strong>\n</h3>\n\n<p>Distributed Data Stores are most widely used and recognized as Distributed Databases. Most distributed databases are **NoSQL **non-relational databases, limited to key-value semantics. They provide incredible performance and scalability at the cost of consistency or availability.</p>\n\n<p>We cannot go into discussions of distributed data stores without first introducing the <strong>CAP Theorem.</strong></p>\n\n<p><strong>CAP Theorem</strong><br>\nThe CAP theorem states that a distributed data store cannot simultaneously be consistent, available and partition tolerant.</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--SjQ53v4o--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ru214t75dc7ilm0rp31o.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--SjQ53v4o--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ru214t75dc7ilm0rp31o.png" alt="Alt Text" loading="lazy"></a></p>\n\n<p><strong>Some quick definitions:</strong></p>\n\n<ul>\n<li><p>Consistency — What you read and write sequentially is what is expected</p></li>\n<li><p>Availability — the whole system does not die — every non-failing node always returns a response.</p></li>\n<li><p>Partition Tolerant — The system continues to function and uphold its consistency/availability guarantees in spite of network partitions</p></li>\n</ul>\n\n<p>In reality, partition tolerance must be a given for any distributed data store. As mentioned in many places, one of which this great article, you cannot have consistency and availability without partition tolerance.</p>\n\n<p>Think about it: if you have two nodes which accept information and their connection dies — how are they both going to be available and simultaneously provide you with consistency? They have no way of knowing what the other node is doing and as such have can either become offline (unavailable) or work with stale information (inconsistent).</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--i1bzVlmb--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/929a1od2b16l8r4ff384.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i1bzVlmb--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/929a1od2b16l8r4ff384.png" alt="Alt Text" loading="lazy"></a></p>\n\n<p>In the end you’re left to choose if you want your system to be strongly consistent or highly available under a network partition.</p>\n\n<p>Practice shows that most applications value availability more. You do not necessarily always need strong consistency. Even then, that trade-off is not necessarily made because you need the 100% availability guarantee, but rather because network latency can be an issue when having to synchronize machines to achieve strong consistency. These and more factors make applications typically opt for solutions which offer high availability.</p>\n\n<p>Such databases settle with the weakest consistency model — eventual consistency (strong vs eventual consistency explanation). This model guarantees that if no new updates are made to a given item, eventually all accesses to that item will return the latest updated value.</p>\n\n<p>Those systems provide BASE properties (as opposed to traditional databases’ ACID)</p>\n\n<ul>\n<li><p><strong>Basically Available</strong> — The system always returns a response</p></li>\n<li><p><strong>Soft state</strong> — The system could change over time, even during times of no input (due to eventual consistency)</p></li>\n<li><p><strong>Eventual consistency</strong> — In the absence of input, the data will spread to every node sooner or later — thus becoming consistent</p></li>\n</ul>\n\n<p>Examples of such available distributed databases — Cassandra, Riak, Voldemort</p>\n\n<p>Of course, there are other data stores which prefer stronger consistency — HBase, Couchbase, Redis, Zookeeper</p>\n\n<p>The CAP theorem is worthy of multiple articles on its own — some regarding how you can tweak a system’s CAP properties depending on how the client behaves and others on how it is not understood properly.</p>\n\n<h3>\n  <a name="cassandra" href="#cassandra">\n  </a>\n  <strong>Cassandra</strong>\n</h3>\n\n<p>Cassandra, as mentioned above, is a distributed No-SQL database which prefers the AP properties out of the CAP, settling with eventual consistency. I must admit this may be a bit misleading, as Cassandra is highly configurable — you can make it provide strong consistency at the expense of availability as well, but that is not its common use case.</p>\n\n<p>Cassandra uses consistent hashing to determine which nodes out of your cluster must manage the data you are passing in. You set a replication factor, which basically states to how many nodes you want to replicate your data.</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--sFvcF9vo--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1633620691075/BbS74SXnE.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--sFvcF9vo--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1633620691075/BbS74SXnE.png" alt="cassandra.png" loading="lazy"></a></p>\n\n<p>When reading, you will read from those nodes only.</p>\n\n<p>Cassandra is massively scalable, providing absurdly high write throughput.</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--6z9gRa4w--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9456g67ui7ubk6tbc9qn.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--6z9gRa4w--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9456g67ui7ubk6tbc9qn.png" alt="Alt Text" loading="lazy"></a></p>\n\n<p>Even though this diagram might be biased and it looks like it compares Cassandra to databases set to provide strong consistency (otherwise I can’t see why MongoDB would drop performance when upgraded from 4 to 8 nodes), this should still show what a properly set up Cassandra cluster is capable of.</p>\n\n<p>Regardless, in the distributed systems trade-off which enables horizontal scaling and incredibly high throughput, Cassandra does not provide some fundamental features of ACID databases — namely, transactions.</p>\n\n<h3>\n  <a name="distributed-messaging" href="#distributed-messaging">\n  </a>\n  <strong>Distributed Messaging</strong>\n</h3>\n\n<p>Messaging systems provide a central place for storage and propagation of messages/events inside your overall system. They allow you to decouple your application logic from directly talking with your other systems.</p>\n\n<p>Known Scale — LinkedIn’s Kafka cluster processed 1 trillion messages a day with peaks of 4.5 millions messages a second.</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Tn_DrH1u--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5yvo5fb8o0jv9c17ssst.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Tn_DrH1u--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5yvo5fb8o0jv9c17ssst.png" alt="Alt Text" loading="lazy"></a></p>\n\n<p>Simply put, a messaging platform works in the following way:<br>\nA message is broadcast from the application which potentially create it (called a producer), goes into the platform and is read by potentially multiple applications which are interested in it (called consumers).</p>\n\n<p>If you need to save a certain event to a few places (e.g user creation to database, warehouse, email sending service and whatever else you can come up with) a messaging platform is the cleanest way to spread that message.</p>\n\n<p>Consumers can either pull information out of the brokers (pull model) or have the brokers push information directly into the consumers (push model).</p>\n\n<p>There are a couple of popular top-notch messaging platforms:<br>\n*<em>RabbitMQ *</em>— Message broker which allows you finer-grained control of message trajectories via routing rules and other easily configurable settings. Can be called a smart broker, as it has a lot of logic in it and tightly keeps track of messages that pass through it. Provides settings for both AP and CP from CAP. Uses a push model for notifying the consumers.</p>\n\n<p>*<em>Kafka *</em>— Message broker (and all out platform) which is a bit lower level, as in it does not keep track of which messages have been read and does not allow for complex routing logic. This helps it achieve amazing performance. In my opinion, this is the biggest prospect in this space with active development from the open-source community and support from the Confluent team. Kafka arguably has the most widespread use from top tech companies. I wrote a thorough introduction to this, where I go into detail about all of its goodness.</p>\n\n<p><strong>Apache ActiveMQ</strong> — The oldest of the bunch, dating from 2004. Uses the JMS API, meaning it is geared towards Java EE applications. It got rewritten as ActiveMQ Artemis, which provides outstanding performance on par with Kafka.</p>\n\n<p><strong>Amazon SQS</strong> — A messaging service provided by AWS. Lets you quickly integrate it with existing applications and eliminates the need to handle your own infrastructure, which might be a big benefit, as systems like Kafka are notoriously tricky to set up. Amazon also offers two similar services — SNS and MQ, the latter of which is basically ActiveMQ but managed by Amazon.</p>\n\n<h3>\n  <a name="data-copying" href="#data-copying">\n  </a>\n  <strong>Data Copying</strong>\n</h3>\n\n<p>Copying is another foundational concept in computing. In software design we use the concept of taking data copies to achieve speed and resilience in several different ways.</p>\n\n<p><strong>Replicas</strong><br>\nA data replica is an exact copy of a database. Replicas are constantly, iteratively synced with one another so their contents are kept as identical as possible. They generally come in two flavours, active and passive. Active replicas support read and write access and play an (unsurprisingly) active role in serving clients. Active replicas can help with scale, resilience and location-base performance. Passive replicas are generally maintained for failover purposes; they don’t support read or write and are only synced in one direction (from the master data). The job of a passive replica is to be ready to take over from the main replica if it fails.</p>\n\n<p><strong>Caches</strong><br>\nUnlike a replica, a cache is a not-necessarily-identical, read-only, copy of your data, usually maintained as a way to serve read requests faster and more cheaply than querying your main database. Caches may take advantage of</p>\n\n<ul>\n<li><p>Location (keeping information physically close to the reading client).</p></li>\n<li><p>Medium (keeping information in faster-to-access physical media like memory rather than disk).</p></li>\n<li><p>Design (maintaining information in data structures optimised for fast read like key/value stores or graphs).</p></li>\n</ul>\n\n<p><strong>Backups</strong><br>\nA backup is a copy of your data for emergency use in the case of catastrophic data loss. Usually a backup is written to a permanent storage medium like disk in multiple physical locations. Backups are slow to write to and restore from and usually they are not kept constantly in sync with the master data. Instead, they are periodically updated. They are useful but have severe limitations and can easily lull folk into a false sense of security. This false security becomes especially dangerous as the scale and complexity of a system increases.</p>\n\n<p><strong>Denormalization for Decoupling</strong><br>\nData duplication goes in and out of fashion as an architectural technique. It is often frowned upon because it’s a common cause of bugs. On the other hand, it is very useful for performance. Basically, like everything in life it has pros and cons and we need to make a judgment on whether to use it based on the current circumstances.</p>\n\n<p>A decade ago we spent ages “normalizing” our databases, or removing data duplication. However in a distributed system we really need to have data available in more than one location. Every service cannot be calling back to the same database all the time or we lose all the benefits of distribution. Microservices usually maintain local equivalents of certain pieces of data that they can write and read without worrying about anyone else.</p>\n\n<p>BUT the potential bugs have not mysteriously gone away. We have to somehow keep all those data copies in alignment (that doesn’t necessarily mean identical). By using data duplication we have to accept that we’ve just included an architectural technique that can be very bug prone in our design. It’s a tradeoff. It just means that we have to use development techniques that help resolve those problems such as Domain Driven Design, or operational techniques like managed stateful services that handle some of these issues for us.</p>\n\n<p><strong>Decentralized vs Distributed</strong><br>\nBefore we go any further I’d like to make a distinction between the two terms.</p>\n\n<p>Even though the words sound similar and can be concluded to mean the same logically, their difference makes a significant technological and political impact.</p>\n\n<p><strong>Decentralized **is still **distributed</strong> in the technical sense, but the whole decentralized systems is not owned by one actor. No one company can own a decentralized system, otherwise it wouldn’t be decentralized anymore.</p>\n\n<p>This means that most systems we will go over today can be thought of as <strong>distributed centralized systems</strong> — and that is what they’re made to be.</p>\n\n<p>Decentralized is essentially distributed on a technical level, but usually a decentralized system is not owned by a single source.</p>\n\n<p>If you think about it — it is harder to create a decentralized system because then you need to handle the case where some of the participants are malicious. This is not the case with normal distributed systems, as you know you own all the nodes.</p>\n\n<p><em>This has been debated a lot and can be confused with others (peer-to-peer, federated). In early literature, it’s been defined differently as well. Regardless, what I gave you as a definition is what I feel is the most widely used now that blockchain and cryptocurrencies popularized the term</em>.</p>\n\n<p><strong>Cloud vs distributed systems</strong><br>\nCloud computing and distributed systems are different, but they use similar concepts. Distributed computing uses distributed systems by spreading tasks across many machines. Cloud computing, on the other hand, uses network hosted servers for storage, process, data management.</p>\n\n<p>Distributed computing aims to create collaborative resource sharing and provide size and geographical scalability. Cloud computing is about delivering an on demand environment using transparency, monitoring, and security.</p>\n\n<p>Compared to distributed systems, cloud computing offers the following advantages:</p>\n\n<ul>\n<li><p>Cost effective</p></li>\n<li><p>Access to a global market</p></li>\n<li><p>Encapsulated change management</p></li>\n<li><p>Access storage, servers, and databases on the internet</p></li>\n</ul>\n\n<p>However, cloud computing is arguably less flexible than distributed computing, as you rely on other services and technologies to build a system. This gives you less control overall.</p>\n\n<p>Priorities like load-balancing, replication, auto-scaling, and automated back-ups can be made easy with cloud computing. Cloud building tools like Docker, Amazon Web Services (AWS), Google Cloud Services, or Azure make it possible to create such systems quickly, and many teams opt to build distributed systems alongside these technologies.</p>\n\n',a.body_markdown="In this Article, we are going to learn about Distributed Systems. Here we are focusing on these things,\n.What is Distributed Systems\n.How does Distributed Systems work\n.Why Distributed Systems\n.Characteristic of Distributed Systems\n.Challenges of Distributed Systems.\n.Risks using Distributed Systems\n.Distributed System Architecture & Types\n.Distributed Data Stores(Cassandra)\n.CAP Theorem\n.Distributed Messaging\n.Data Copying\n.Decentralized vs Distributed Systems\n.Cloud vs Distributed Systems\n\n###**What is a distributed system?**\n\nA distributed system in its most simplest definition is a group of computers working together as to appear as a single computer to the end-user.\nThese machines have a shared state, operate concurrently and can fail independently without affecting the whole system’s uptime.\nA distributed system is any network structure that consists of autonomous computers that are connected using a distribution middleware. Distributed systems facilitate sharing different resources and capabilities, to provide users with a single and integrated coherent network.\nThe opposite of a distributed system is a centralized system. If all of the components of a computing system reside in one machine.\nUnlike traditional databases, which are stored on a single machine, in a distributed system, a user must be able to communicate with any machine without knowing it is only one machine. Most applications today use some form of a distributed database and must account for their homogenous or heterogenous nature.\nIn a homogenous distributed database, each system shares a data model and database management system and data model. Generally, these are easier to manage by adding nodes. On the other hand, heterogeneous databases make it possible to have multiple data models or varied database management systems using gateways to translate data between nodes.\n\n**Distributed Information Systems:** distribute information across different servers via multiple communication models.\n**Distributed Pervasive Systems:** use embedded computer devices (i.e. ECG monitors, sensors, mobile devices).\n**Distributed Computing Systems:** computers in a network communicate via message passing.\n\n*Distributed systems must have a shared network to connect its components, which could be connected using an IP address or even physical cables.*\n\n### **How does a distributed system work?**\nDistributed systems have evolved over time, but today’s most common implementations are largely designed to operate via the internet and, more specifically, the cloud. A distributed system begins with a task, such as rendering a video to create a finished product ready for release. The web application, or distributed applications, managing this task — like a video editor on a client computer — splits the job into pieces. In this simple example, the algorithm that gives one frame of the video to each of a dozen different computers (or nodes) to complete the rendering. Once the frame is complete, the managing application gives the node a new frame to work on. This process continues until the video is finished and all the pieces are put back together. A system like this doesn’t have to stop at just 12 nodes — the job may be distributed among hundreds or even thousands of nodes, turning a task that might have taken days for a single computer to complete into one that is finished in a matter of minutes.\n\nThere are many models and architectures of distributed systems in use today. Client-server systems, the most traditional and simple type of distributed system, involve a multitude of networked computers that interact with a central server for data storage, processing or other common goal. Cell phone networks are an advanced type of distributed system that share workloads among handsets, switching systems and internet-based devices. Peer-to-peer networks, in which workloads are distributed among hundreds or thousands of computers all running the same software, are another example of a distributed system architecture. The most common forms of distributed systems in the enterprise today are those that operate over the web, handing off workloads to dozens of cloud-based virtual server instances that are created as needed, then terminated when the task is complete.\n\n### **Why distribute a system?**\nSystems are always distributed by necessity. The truth of the matter is — managing distributed systems is a complex topic chock-full of pitfalls and landmines. It is a headache to deploy, maintain and debug distributed systems, so why go there at all?\nWhat a distributed system enables you to do is scale horizontally. Going back to our previous example of the single database server, the only way to handle more traffic would be to upgrade the hardware the database is running on. This is called scaling vertically.\n\nScaling vertically is all well and good while you can, but after a certain point you will see that even the best hardware is not sufficient for enough traffic, not to mention impractical to host.\n\nScaling horizontally simply means adding more computers rather than upgrading the hardware of a single one.\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/89lntk6dcyhja5yeg8tr.png)\n\nIt is significantly cheaper than vertical scaling after a certain threshold but that is not its main case for preference.\n\nVertical scaling can only bump your performance up to the latest hardware’s capabilities. These capabilities prove to be insufficient for technological companies with moderate to big workloads.\nThe best thing about horizontal scaling is that you have no cap on how much you can scale whenever performance degrades you simply add another machine, up to infinity potentially.\nEasy scaling is not the only benefit you get from distributed systems. Fault tolerance and low latency are also equally as important.\n\n**Fault Tolerance** - a cluster of ten machines across two data centers is inherently more fault-tolerant than a single machine. Even if one data center catches on fire, your application would still work.\n\n*An important part of distributed systems is the CAP theorem, which states that a distributed data store cannot simultaneously be consistent, available, and partition tolerant*.\n\n**Low Latency** — The time for a network packet to travel the world is physically bounded by the speed of light. For example, the shortest possible time for a request‘s **round-trip** time (that is, go back and forth) in a fiber-optic cable between New York to Sydney is 160ms. Distributed systems allow you to have a node in both cities, allowing traffic to hit the node that is closest to it.\n\nFor a distributed system to work, though, you need the software running on those machines to be specifically designed for running on multiple computers at the same time and handling the problems that come along with it. This turns out to be no easy feat.\n\n## **Key Characteristics of Distributed Systems**\n\n### **Scalability**\nScalability is the capability of a system, process, or network to grow and manage increased demand. Any distributed system that can continuously evolve to support the growing amount of work is considered scalable.\n\nA system may have to scale because of many reasons like increased data volume or increased work, e.g., number of transactions. A scalable system would like to achieve this scaling without performance loss.\n\nGenerally, although designed (or claimed) to be scalable, the performance of a system declines with the system size due to the management or environmental cost. For instance, network speed may become slower because machines tend to be far apart from one another. More generally, some tasks may not be distributed, either because of their inherent atomic nature or some flaw in the system design. At some point, such tasks would limit the speed-up obtained by distribution. A scalable architecture avoids this situation and attempts to balance the load on all the participating nodes evenly.\n\n**Horizontal vs. Vertical Scaling**\nHorizontal scaling means that you scale by adding more servers into your pool of resources, whereas Vertical scaling means that you scale by adding more power (CPU, RAM, Storage, etc.) to an existing server.\n\nWith horizontal scaling, it is often easier to scale dynamically by adding more machines into the existing pool; Vertical-scaling is usually limited to the capacity of a single server. Scaling beyond that capacity often involves downtime and comes with an upper limit.\nGood examples of horizontal scaling are **Cassandra **and **MongoDB**, as they both provide an easy way to scale horizontally by adding more machines to meet growing needs. Similarly, a good example of vertical scaling is MySQL, as it allows for an easy way to scale vertically by switching from small to bigger machines. However, this process often involves downtime.\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1vvpawoanwa8gwec0ml9.png)\n\n\n### **Reliability**\nBy definition, reliability is the probability a system will fail in a given period. In simple terms, a distributed system is considered reliable if it keeps delivering its services even when one or several of its software or hardware components fail. Thus, reliability represents one of the main characteristics of any distributed system. Any failing machine can always be replaced by another healthy one in such systems, ensuring the completion of the requested task.\n\nTake the example of a large electronic commerce store (like Amazon), where one of the primary requirements is that any user transaction should never be canceled due to a failure of the machine running that transaction. For instance, if a user has added an item to their shopping cart, the system is expected not to lose it. A reliable distributed system achieves this through the redundancy of both the software components and data. If the server carrying the user’s shopping cart fails, another server with the replica of the shopping cart should replace it.\nRedundancy has a cost, and a reliable system has to pay to achieve such resilience for services by eliminating every single point of failure.\n\n### **Availability**\nBy definition, availability is when a system remains operational to perform its required function in a specific period. It is a simple measure of the percentage of time that a system, service, or machine remains operational under normal conditions. For example, an aircraft that can be flown for many hours a month without much downtime has high availability. Availability takes into account maintainability, repair time, spares availability, and other logistics considerations. If an aircraft is down for maintenance, it is considered not available during that time.\n\nReliability is availability over time, considering the full range of possible real-world conditions that can occur. For example, an aircraft that can make it through any possible weather safely is more reliable than one that has vulnerabilities to possible conditions.\n\n### **Reliability Vs. Availability**\nIf a system is reliable, it is available. However, if it is available, it is not necessarily reliable.\nIn other words, high reliability contributes to high availability. Still, it is possible to achieve a high availability even with an unreliable product by minimizing repair time and ensuring that spares are always available when they are needed.\n\nLet’s take the example of an online retail store with 99.99% availability for the first two years after its launch. However, the system was launched without any information security testing. The customers were happy with the system, but they don’t realize that it isn’t very reliable as it is vulnerable to likely risks. In the third year, the system experiences a series of information security incidents that suddenly result in extremely low availability for extended periods. This results in reputational and financial damage to the customers.\n\n### **Efficiency**\nTo understand how to measure the efficiency of a distributed system, let’s assume an operation that runs in a distributed manner and delivers a set of items as a result.\n\nTwo standard measures of its efficiency are the response time (or latency) that denotes the delay to obtain the first item, and the throughput (or bandwidth), which denotes the number of items delivered in a given time unit (e.g., a second). The two measures correspond to the following unit costs:\n\n- The number of messages globally sent by the nodes of the system, regardless of the message size.\n\n- Size of messages representing the volume of data exchanges.\n\nThe complexity of operations supported by distributed data structures (e.g., searching for a specific key in a distributed index) can be characterized as a function of one of these cost units. Generally speaking, the analysis of a distributed structure in terms of the ‘number of messages’ is over-simplistic. It ignores the impact of many aspects, including the network topology, the network load, variation, the possible heterogeneity of the software and hardware components involved in data processing and routing, etc. However, it is quite difficult to develop a precise cost model that would accurately consider all these performance factors. Therefore, we’ve to live with rough but robust estimates of the system behavior.\n\n### **Serviceability or Manageability**\nAnother important consideration while designing a distributed system is how easy it is to operate and maintain. Serviceability or manageability is the simplicity and speed with which a system can be repaired or maintained\n\nIf the time to fix a failed system increases, then availability will decrease. Things to consider for manageability are the ease of diagnosing and understanding problems when they occur, ease of making updates or modifications, and how simple the system is to operate (i.e., does it routinely operate without failure or exceptions?).\n\nEarly detection of faults can decrease or avoid system downtime. For example, some enterprise systems can automatically call a service center (without human intervention) when the system experiences a system fault.\n\n### **Challenges of distributed systems**\nDistributed systems are considerably more complex than monolithic computing environments, and raise a number of challenges around design, operations and maintenance. These include:\n\n**Increased opportunities for failure:** The more systems added to a computing environment, the more opportunity there is for failure. If a system is not carefully designed and a single node crashes, the entire system can go down. While distributed systems are designed to be fault tolerant, that fault tolerance isn’t automatic or foolproof.\n\n**Synchronization process challenges:** Distributed systems work without a global clock, requiring careful programming to ensure that processes are properly synchronized to avoid transmission delays that result in errors and data corruption. In a complex system — such as a multiplayer video game — synchronization can be challenging, especially on a public network that carries data traffic.\n\n**Imperfect scalability:** Doubling the number of nodes in a distributed system doesn’t necessarily double performance. Architecting an effective distributed system that maximizes scalability is a complex undertaking that needs to take into account load balancing, bandwidth management and other issues.\n\n**More complex security:** Managing a large number of nodes in a heterogeneous or globally distributed environment creates numerous security challenges. A single weak link in a file system or larger distributed system network can expose the entire system to attack.\n\n**Increased complexity:** Distributed systems are more complex to design, manage and understand than traditional computing environments.\n\n### **Risks of distributed systems**\nThe challenges of distributed systems as outlined above create a number of correlating risks. These include:\n\n**Security**: Distributed systems are as vulnerable to attack as any other system, but their distributed nature creates a much larger attack surface that exposes organizations to threats.\n\n**Risk of network failure:** Distributed systems are beholden to public networks in order to transmit and receive data. If one segment of the internet becomes unavailable or overloaded, distributed system performance may decline.\n\n**Governance and control issues:** Distributed systems lack the governability of monolithic, single-server-based systems, creating auditing and adherence issues around global privacy laws such as GDPR. Globally distributed environments can impose barriers to providing certain levels of assurance and impair visibility into where data resides.\n\n**Cost control:** Unlike centralized systems, the scalability of distributed systems allows administrators to easily add additional capacity as needed, which can also increase costs. Pricing for cloud-based distributed computing systems are based on usage (such as the number of memory resources and CPU power consumed over time). If demand suddenly spikes, organizations.\n\n\n### **Distributed System Architecture**\n**Distributed systems must have a network that connects all components (machines, hardware, or software) together so they can transfer messages to communicate with each other.**\n\n\n- That network could be connected with an IP address or use cables or even on a circuit board.\n\n- The messages passed between machines contain forms of data that the systems want to share like databases, objects, and files.\n\n- The way the messages are communicated reliably whether it’s sent, received, acknowledged or how a node retries on failure is an important feature of a distributed system.\n\n- Distributed systems were created out of necessity as services and applications needed to scale and new machines needed to be added and managed. In the design of distributed systems, the major trade-off to consider is complexity vs performance.\n\n### **Types of Distributed System Architectures:**\nDistributed applications and processes typically use one of four architecture types below:\n\n**Client-server:**\nIn the early days, distributed systems architecture consisted of a server as a shared resource like a printer, database, or a web server. It had multiple clients (for example, users behind computers) that decide when to use the shared resource, how to use and display it, change data, and send it back to the server. Code repositories like git is a good example where the intelligence is placed on the developers committing the changes to the code.\n\nToday, distributed systems architecture has evolved with web applications into:\n\n**Three-tier:** In this architecture, the clients no longer need to be intelligent and can rely on a middle tier to do the processing and decision making. Most of the first web applications fall under this category. The middle tier could be called an agent that receives requests from clients, that could be stateless, processes the data and then forwards it on to the servers.\n\n**Multi-tier:** Enterprise web services first created n-tier or multi-tier systems architectures. This popularized the application servers that contain the business logic and interacts both with the data tiers and presentation tiers.\n\n**Peer-to-peer:** There are no centralized or special machine that does the heavy lifting and intelligent work in this architecture. All the decision making and responsibilities are split up amongst the machines involved and each could take on client or server roles. Blockchain is a good example of this.\n\n### **Distributed Data Stores**\nDistributed Data Stores are most widely used and recognized as Distributed Databases. Most distributed databases are **NoSQL **non-relational databases, limited to key-value semantics. They provide incredible performance and scalability at the cost of consistency or availability.\n\nWe cannot go into discussions of distributed data stores without first introducing the **CAP Theorem.**\n\n**CAP Theorem**\nThe CAP theorem states that a distributed data store cannot simultaneously be consistent, available and partition tolerant.\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ru214t75dc7ilm0rp31o.png)\n\n**Some quick definitions:**\n\n- Consistency — What you read and write sequentially is what is expected\n\n- Availability — the whole system does not die — every non-failing node always returns a response.\n\n- Partition Tolerant — The system continues to function and uphold its consistency/availability guarantees in spite of network partitions\n\nIn reality, partition tolerance must be a given for any distributed data store. As mentioned in many places, one of which this great article, you cannot have consistency and availability without partition tolerance.\n\nThink about it: if you have two nodes which accept information and their connection dies — how are they both going to be available and simultaneously provide you with consistency? They have no way of knowing what the other node is doing and as such have can either become offline (unavailable) or work with stale information (inconsistent).\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/929a1od2b16l8r4ff384.png)\n\nIn the end you’re left to choose if you want your system to be strongly consistent or highly available under a network partition.\n\nPractice shows that most applications value availability more. You do not necessarily always need strong consistency. Even then, that trade-off is not necessarily made because you need the 100% availability guarantee, but rather because network latency can be an issue when having to synchronize machines to achieve strong consistency. These and more factors make applications typically opt for solutions which offer high availability.\n\nSuch databases settle with the weakest consistency model — eventual consistency (strong vs eventual consistency explanation). This model guarantees that if no new updates are made to a given item, eventually all accesses to that item will return the latest updated value.\n\nThose systems provide BASE properties (as opposed to traditional databases’ ACID)\n\n\n- **Basically Available** — The system always returns a response\n\n\n- **Soft state** — The system could change over time, even during times of no input (due to eventual consistency)\n\n\n- **Eventual consistency** — In the absence of input, the data will spread to every node sooner or later — thus becoming consistent\n\nExamples of such available distributed databases — Cassandra, Riak, Voldemort\n\nOf course, there are other data stores which prefer stronger consistency — HBase, Couchbase, Redis, Zookeeper\n\nThe CAP theorem is worthy of multiple articles on its own — some regarding how you can tweak a system’s CAP properties depending on how the client behaves and others on how it is not understood properly.\n\n### **Cassandra**\nCassandra, as mentioned above, is a distributed No-SQL database which prefers the AP properties out of the CAP, settling with eventual consistency. I must admit this may be a bit misleading, as Cassandra is highly configurable — you can make it provide strong consistency at the expense of availability as well, but that is not its common use case.\n\nCassandra uses consistent hashing to determine which nodes out of your cluster must manage the data you are passing in. You set a replication factor, which basically states to how many nodes you want to replicate your data.\n\n\n![cassandra.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1633620691075/BbS74SXnE.png)\n\nWhen reading, you will read from those nodes only.\n\nCassandra is massively scalable, providing absurdly high write throughput.\n\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9456g67ui7ubk6tbc9qn.png)\n\nEven though this diagram might be biased and it looks like it compares Cassandra to databases set to provide strong consistency (otherwise I can’t see why MongoDB would drop performance when upgraded from 4 to 8 nodes), this should still show what a properly set up Cassandra cluster is capable of.\n\nRegardless, in the distributed systems trade-off which enables horizontal scaling and incredibly high throughput, Cassandra does not provide some fundamental features of ACID databases — namely, transactions.\n\n### **Distributed Messaging**\nMessaging systems provide a central place for storage and propagation of messages/events inside your overall system. They allow you to decouple your application logic from directly talking with your other systems.\n\nKnown Scale — LinkedIn’s Kafka cluster processed 1 trillion messages a day with peaks of 4.5 millions messages a second.\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5yvo5fb8o0jv9c17ssst.png)\n\nSimply put, a messaging platform works in the following way:\nA message is broadcast from the application which potentially create it (called a producer), goes into the platform and is read by potentially multiple applications which are interested in it (called consumers).\n\nIf you need to save a certain event to a few places (e.g user creation to database, warehouse, email sending service and whatever else you can come up with) a messaging platform is the cleanest way to spread that message.\n\nConsumers can either pull information out of the brokers (pull model) or have the brokers push information directly into the consumers (push model).\n\nThere are a couple of popular top-notch messaging platforms:\n**RabbitMQ **— Message broker which allows you finer-grained control of message trajectories via routing rules and other easily configurable settings. Can be called a smart broker, as it has a lot of logic in it and tightly keeps track of messages that pass through it. Provides settings for both AP and CP from CAP. Uses a push model for notifying the consumers.\n\n**Kafka **— Message broker (and all out platform) which is a bit lower level, as in it does not keep track of which messages have been read and does not allow for complex routing logic. This helps it achieve amazing performance. In my opinion, this is the biggest prospect in this space with active development from the open-source community and support from the Confluent team. Kafka arguably has the most widespread use from top tech companies. I wrote a thorough introduction to this, where I go into detail about all of its goodness.\n\n**Apache ActiveMQ** — The oldest of the bunch, dating from 2004. Uses the JMS API, meaning it is geared towards Java EE applications. It got rewritten as ActiveMQ Artemis, which provides outstanding performance on par with Kafka.\n\n**Amazon SQS** — A messaging service provided by AWS. Lets you quickly integrate it with existing applications and eliminates the need to handle your own infrastructure, which might be a big benefit, as systems like Kafka are notoriously tricky to set up. Amazon also offers two similar services — SNS and MQ, the latter of which is basically ActiveMQ but managed by Amazon.\n\n### **Data Copying**\nCopying is another foundational concept in computing. In software design we use the concept of taking data copies to achieve speed and resilience in several different ways.\n\n**Replicas**\nA data replica is an exact copy of a database. Replicas are constantly, iteratively synced with one another so their contents are kept as identical as possible. They generally come in two flavours, active and passive. Active replicas support read and write access and play an (unsurprisingly) active role in serving clients. Active replicas can help with scale, resilience and location-base performance. Passive replicas are generally maintained for failover purposes; they don’t support read or write and are only synced in one direction (from the master data). The job of a passive replica is to be ready to take over from the main replica if it fails.\n\n**Caches**\nUnlike a replica, a cache is a not-necessarily-identical, read-only, copy of your data, usually maintained as a way to serve read requests faster and more cheaply than querying your main database. Caches may take advantage of\n\n\n- Location (keeping information physically close to the reading client).\n\n- Medium (keeping information in faster-to-access physical media like memory rather than disk).\n\n- Design (maintaining information in data structures optimised for fast read like key/value stores or graphs).\n\n**Backups**\nA backup is a copy of your data for emergency use in the case of catastrophic data loss. Usually a backup is written to a permanent storage medium like disk in multiple physical locations. Backups are slow to write to and restore from and usually they are not kept constantly in sync with the master data. Instead, they are periodically updated. They are useful but have severe limitations and can easily lull folk into a false sense of security. This false security becomes especially dangerous as the scale and complexity of a system increases.\n\n**Denormalization for Decoupling**\nData duplication goes in and out of fashion as an architectural technique. It is often frowned upon because it’s a common cause of bugs. On the other hand, it is very useful for performance. Basically, like everything in life it has pros and cons and we need to make a judgment on whether to use it based on the current circumstances.\n\nA decade ago we spent ages “normalizing” our databases, or removing data duplication. However in a distributed system we really need to have data available in more than one location. Every service cannot be calling back to the same database all the time or we lose all the benefits of distribution. Microservices usually maintain local equivalents of certain pieces of data that they can write and read without worrying about anyone else.\n\nBUT the potential bugs have not mysteriously gone away. We have to somehow keep all those data copies in alignment (that doesn’t necessarily mean identical). By using data duplication we have to accept that we’ve just included an architectural technique that can be very bug prone in our design. It’s a tradeoff. It just means that we have to use development techniques that help resolve those problems such as Domain Driven Design, or operational techniques like managed stateful services that handle some of these issues for us.\n\n**Decentralized vs Distributed**\nBefore we go any further I’d like to make a distinction between the two terms.\n\nEven though the words sound similar and can be concluded to mean the same logically, their difference makes a significant technological and political impact.\n\n**Decentralized **is still **distributed** in the technical sense, but the whole decentralized systems is not owned by one actor. No one company can own a decentralized system, otherwise it wouldn’t be decentralized anymore.\n\nThis means that most systems we will go over today can be thought of as **distributed centralized systems** — and that is what they’re made to be.\n\nDecentralized is essentially distributed on a technical level, but usually a decentralized system is not owned by a single source.\n\nIf you think about it — it is harder to create a decentralized system because then you need to handle the case where some of the participants are malicious. This is not the case with normal distributed systems, as you know you own all the nodes.\n\n*This has been debated a lot and can be confused with others (peer-to-peer, federated). In early literature, it’s been defined differently as well. Regardless, what I gave you as a definition is what I feel is the most widely used now that blockchain and cryptocurrencies popularized the term*.\n\n**Cloud vs distributed systems**\nCloud computing and distributed systems are different, but they use similar concepts. Distributed computing uses distributed systems by spreading tasks across many machines. Cloud computing, on the other hand, uses network hosted servers for storage, process, data management.\n\nDistributed computing aims to create collaborative resource sharing and provide size and geographical scalability. Cloud computing is about delivering an on demand environment using transparency, monitoring, and security.\n\nCompared to distributed systems, cloud computing offers the following advantages:\n\n\n- Cost effective\n\n- Access to a global market\n\n- Encapsulated change management\n\n- Access storage, servers, and databases on the internet\n\nHowever, cloud computing is arguably less flexible than distributed computing, as you rely on other services and technologies to build a system. This gives you less control overall.\n\nPriorities like load-balancing, replication, auto-scaling, and automated back-ups can be made easy with cloud computing. Cloud building tools like Docker, Amazon Web Services (AWS), Google Cloud Services, or Azure make it possible to create such systems quickly, and many teams opt to build distributed systems alongside these technologies.\n\n",a.user={name:"AlphaCodes",username:n,twitter_username:"MAttavania",github_username:n,website_url:"https://linktr.ee/AlphaCodes",profile_image:"https://res.cloudinary.com/practicaldev/image/fetch/s--_tu3CW6f--/c_fill,f_auto,fl_progressive,h_640,q_auto,w_640/https://dev-to-uploads.s3.amazonaws.com/uploads/user/profile_image/597574/ed17557a-2d45-48f9-93df-ec394f9318f5.png",profile_image_90:"https://res.cloudinary.com/practicaldev/image/fetch/s--1rHwOI_y--/c_fill,f_auto,fl_progressive,h_90,q_auto,w_90/https://dev-to-uploads.s3.amazonaws.com/uploads/user/profile_image/597574/ed17557a-2d45-48f9-93df-ec394f9318f5.png"},a.flare_tag={name:i,bg_color_hex:"#091b47",text_color_hex:"#b2ffe1"},{layout:"default",data:[{}],fetch:{"data-v-70afb46a:0":{article:a}},error:e,state:{currentArticle:a},serverRendered:!0,routePath:"/mukulalpha/854118",config:{_app:{basePath:"/nuxtstop/",assetsPath:"/nuxtstop/_nuxt/",cdnURL:e}}}}(null,"2021-10-07T17:16:53Z",{},"https://dev.to/mukulalpha/system-design-101-everything-to-know-about-distributed-systems-2hd6","showdev","mukulalpha")</script><script src="/nuxtstop/_nuxt/f6e87fb.js" defer></script><script src="/nuxtstop/_nuxt/dc9ce94.js" defer></script><script src="/nuxtstop/_nuxt/6474719.js" defer></script><script src="/nuxtstop/_nuxt/9b75090.js" defer></script><script src="/nuxtstop/_nuxt/18df600.js" defer></script>
  </body>
</html>
