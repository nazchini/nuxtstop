<!doctype html>
<html data-n-head-ssr lang="en" data-n-head="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">
  <head>
    <title>Web Scrapping With Python.</title><meta data-n-head="ssr" charset="utf-8"><meta data-n-head="ssr" name="viewport" content="width=device-width,initial-scale=1"><meta data-n-head="ssr" data-hid="description" name="description" content="Using Nuxt.js fetch() hook to build dev.to with a new look"><meta data-n-head="ssr" name="format-detection" content="telephone=no"><base href="/nuxtstop/"><link data-n-head="ssr" rel="icon" type="image/x-icon" href="/favicon.ico"><link data-n-head="ssr" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:400,500,600&display=swap"><link rel="preload" href="/nuxtstop/_nuxt/f6e87fb.js" as="script"><link rel="preload" href="/nuxtstop/_nuxt/6474719.js" as="script"><link rel="preload" href="/nuxtstop/_nuxt/9b75090.js" as="script"><link rel="preload" href="/nuxtstop/_nuxt/18df600.js" as="script"><link rel="preload" href="/nuxtstop/_nuxt/dc9ce94.js" as="script"><style data-vue-ssr-id="c650fd98:0 af4684f0:0 a9c71758:0 dcafa518:0 4b9cec49:0 b093d766:0 9d98bcb4:0 6b6a11ea:0 0248ed80:0 ea8e4264:0">html{box-sizing:border-box;font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}*,:after,:before{box-sizing:inherit}html{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,:after,:before{border:0 solid #e0e0e0}blockquote,body,dd,dl,figure,h1,h2,h3,h4,h5,h6,p,pre{margin:0}button{background:0 0;padding:0}button:focus{outline:1px dotted;outline:5px auto -webkit-focus-ring-color}fieldset,ol,ul{margin:0;padding:0}ol,ul{list-style:none}hr{border-width:1px}img{border-style:solid}textarea{resize:vertical}input::-moz-placeholder,textarea::-moz-placeholder{color:inherit;opacity:.5}input:-ms-input-placeholder,textarea:-ms-input-placeholder{color:inherit;opacity:.5}input::placeholder,textarea::placeholder{color:inherit;opacity:.5}[role=button],button{cursor:pointer}table{border-collapse:collapse}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit;font-family:sans-serif}a{color:inherit;text-decoration:inherit}button,input,optgroup,select,textarea{padding:0;line-height:inherit;color:inherit;font-family:inherit;font-size:100%}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;overflow:auto;word-break:break-word;white-space:normal}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{max-width:100%;height:auto}html{height:100%;font-size:18px;-ms-overflow-style:scrollbar;-webkit-tap-highlight-color:transparent;-webkit-touch-callout:none}@media(min-width:640px){html{font-size:20px}}body{height:100%;min-width:320px;font-family:Inter,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-weight:400;line-height:1.5;color:#000;background-color:#eff4f7;-webkit-text-rendering:optimizeLegibility;text-rendering:optimizeLegibility;font-synthesis:none;font-kerning:normal;font-feature-settings:"normal","kern";-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;-webkit-overflow-scrolling:touch;overflow-x:hidden;overflow-y:scroll}h1,h2,h3,h4,h5,h6{color:#000;font-family:Inter,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-weight:600;font-feature-settings:"normal";line-height:1.2}pre{background:#29292e;border-radius:2px;overflow:auto;padding:1rem;color:#eff1f9;line-height:1.42em;font-size:13px}@media screen and (min-width:380px){pre{font-size:15px}}pre code{background:#29292e;color:#eff0f9;white-space:pre}div.highlight pre.highlight code{font-size:inherit;padding:0}div.inner-comment div.body div.highlight pre.highlight{background:#29292e}div.inner-comment div.body div.highlight pre.highlight code{font-size:inherit;white-space:inherit;background:inherit;color:inherit}.highlight .hll{background-color:#49483e}.highlight{background:#29292e;color:#f8f8f2}.highlight .c{color:grey}.highlight .err{text-shadow:0 0 7px #f9690e}.highlight .k{color:#f39c12}.highlight .l{color:plum}.highlight .n{color:#f8f8f2}.highlight .o{color:#f9690e}.highlight .p{color:#f8f8f2}.highlight .c1,.highlight .ch,.highlight .cm,.highlight .cp,.highlight .cpf,.highlight .cs{color:grey}.highlight .gd{color:#f9690e}.highlight .ge{font-style:italic}.highlight .gi{color:#7ed07e}.highlight .gs{font-weight:700}.highlight .gu{color:grey}.highlight .kc,.highlight .kd{color:#f39c12}.highlight .kn{color:#f9690e}.highlight .kp,.highlight .kr,.highlight .kt{color:#f39c12}.highlight .ld{color:#f2ca27}.highlight .m{color:plum}.highlight .s{color:#f2ca27}.highlight .na{color:#7ed07e}.highlight .nb{color:#f8f8f2}.highlight .nc{color:#7ed07e}.highlight .no{color:#f39c12}.highlight .nd{color:#7ed07e}.highlight .ni{color:#f8f8f2}.highlight .ne,.highlight .nf{color:#7ed07e}.highlight .nl,.highlight .nn{color:#f8f8f2}.highlight .nx{color:#7ed07e}.highlight .py{color:#f8f8f2}.highlight .nt{color:#f9690e}.highlight .nv{color:#f8f8f2}.highlight .ow{color:#f9690e}.highlight .w{color:#f8f8f2}.highlight .mb,.highlight .mf,.highlight .mh,.highlight .mi,.highlight .mo{color:plum}.highlight .dl,.highlight .s2,.highlight .sa,.highlight .sb,.highlight .sc,.highlight .sd{color:#f2ca27}.highlight .se{color:plum}.highlight .s1,.highlight .sh,.highlight .si,.highlight .sr,.highlight .ss,.highlight .sx{color:#f2ca27}.highlight .bp{color:#f8f8f2}.highlight .fm{color:#7ed07e}.highlight .vc,.highlight .vg,.highlight .vi,.highlight .vm{color:#f8f8f2}.highlight .il{color:plum}.vue-content-placeholders-heading__img,.vue-content-placeholders-heading__subtitle,.vue-content-placeholders-heading__title,.vue-content-placeholders-img,.vue-content-placeholders-text__line{background:#bfcdec!important}.vue-content-placeholders-is-animated .vue-content-placeholders-heading__img:before,.vue-content-placeholders-is-animated .vue-content-placeholders-heading__subtitle:before,.vue-content-placeholders-is-animated .vue-content-placeholders-heading__title:before,.vue-content-placeholders-is-animated .vue-content-placeholders-img:before,.vue-content-placeholders-is-animated .vue-content-placeholders-text__line:before{background:linear-gradient(90deg,transparent 0,#d3ddf9 15%,transparent 30%)!important}header[data-v-27046cca]{max-width:1280px;margin:auto;padding:1rem;height:6rem;border-bottom:1px solid rgba(0,0,0,.2)}header .logo-wrapper[data-v-27046cca],header[data-v-27046cca]{display:flex;align-items:center;justify-content:space-between}header .logo-wrapper[data-v-27046cca]{margin:0 .5rem}header .logo-wrapper svg[data-v-27046cca]{width:3rem;height:100%}header .logo-wrapper .name-wrapper[data-v-27046cca]{margin-left:.6em}header .logo-wrapper .name-wrapper .subtitle[data-v-27046cca]{font-size:1rem}header .logo-wrapper .name-wrapper .app-name[data-v-27046cca]{font-weight:700;font-size:2.25rem;line-height:1.25}header nav[data-v-27046cca]{letter-spacing:-.025rem;font-weight:600;text-transform:uppercase}header nav ul[data-v-27046cca]{display:flex}header nav ul li[data-v-27046cca]{margin:0 .5rem}header nav ul li a[data-v-27046cca]{box-shadow:-4px -4px 8px #f8fafe,4px 4px 8px #ced2db;padding:.25rem 1rem;border-radius:.5rem;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}header nav ul li a[data-v-27046cca]:hover{background:linear-gradient(135deg,rgba(0,0,0,.09),hsla(0,0%,100%,0))}header nav ul li a.nuxt-link-exact-active[data-v-27046cca]{cursor:default}header nav ul li a.nuxt-link-exact-active[data-v-27046cca],header nav ul li a[data-v-27046cca]:active{background:0 0;box-shadow:inset -4px -4px 8px #f0f3f9,inset 4px 4px 8px #ced2db,inset -1px -1px 4px #8e8e8e}.page-wrapper[data-v-10d06ee8]{max-width:1280px;margin:auto;padding:1rem}.article-content-wrapper[data-v-10d06ee8]{display:flex;flex-direction:column;align-items:center;margin:auto auto 2rem}@media(min-width:1024px){.article-content-wrapper[data-v-10d06ee8]{align-items:normal;flex-direction:row}}.article-content-wrapper .article-block[data-v-10d06ee8]{width:100%;max-width:880px}@media(min-width:1024px){.article-content-wrapper .article-block[data-v-10d06ee8]{margin-right:1rem;width:66.66666%;margin-bottom:2rem}}.article-content-wrapper .aside-username-wrapper[data-v-10d06ee8]{max-width:880px;width:100%;position:relative}@media(min-width:1024px){.article-content-wrapper .aside-username-wrapper[data-v-10d06ee8]{display:block;width:33.33333%}}.article-content-wrapper .aside-username-wrapper .aside-username-block[data-v-10d06ee8]{position:-webkit-sticky;position:sticky;top:1rem}@media(min-width:1280px){.comments-block[data-v-10d06ee8]{margin:.5rem}}article[data-v-70afb46a]{padding:.5rem;border-radius:1rem}header h1[data-v-70afb46a],header[data-v-70afb46a]{margin-bottom:1rem}header h1[data-v-70afb46a]{font-size:2.25rem;letter-spacing:-.025rem}header .tags[data-v-70afb46a]{display:flex;flex-wrap:wrap;margin-bottom:1.5rem}header .tags .tag[data-v-70afb46a]{font-weight:500;line-height:1;padding:.5rem;margin:0 .5rem .5rem 0;border-radius:.25rem;box-shadow:-4px -4px 8px #f8fafe,4px 4px 8px #ced2db}header .tags .tag[data-v-70afb46a]:hover{background:linear-gradient(135deg,rgba(0,0,0,.09),hsla(0,0%,100%,0))}header .tags .tag[data-v-70afb46a]:active{background:0 0;box-shadow:inset -4px -4px 8px #f0f3f9,inset 4px 4px 8px #ced2db,inset -1px -1px 4px #8e8e8e}header .image-wrapper[data-v-70afb46a]{position:relative;padding-bottom:56.25%;background-color:#d4dfe8;margin-bottom:1.5rem;border-radius:.5rem;overflow:hidden}@media(min-width:834px){header .image-wrapper[data-v-70afb46a]{margin-bottom:1.5rem}}header .image-wrapper img[data-v-70afb46a]{position:absolute;top:0;left:0;width:100%;height:100%;-o-object-fit:cover;object-fit:cover}header .meta[data-v-70afb46a]{line-height:1;font-size:.875rem;text-transform:uppercase;font-weight:500;letter-spacing:-.025rem;display:flex;align-items:center;justify-content:space-between}header .meta .scl[data-v-70afb46a]{display:flex}header .meta .scl span[data-v-70afb46a]{display:flex;align-items:center;margin-right:1rem}header .meta .scl span svg[data-v-70afb46a]{margin-right:.25rem}header .meta .scl .comments[data-v-70afb46a]{cursor:pointer}[data-v-70afb46a] .content .ltag__user{display:none}[data-v-70afb46a] .content iframe{max-width:100%}[data-v-70afb46a] .content h1{font-size:1.875rem}[data-v-70afb46a] .content h1,[data-v-70afb46a] .content h2{margin-top:2rem;margin-bottom:1rem;letter-spacing:-.025rem}[data-v-70afb46a] .content h2{font-size:1.5rem}[data-v-70afb46a] .content h3{font-size:1.25rem}[data-v-70afb46a] .content h3,[data-v-70afb46a] .content h4{margin-top:2rem;margin-bottom:1rem;letter-spacing:-.025rem}[data-v-70afb46a] .content h4{font-size:1rem}[data-v-70afb46a] .content a{color:#6e87d2}[data-v-70afb46a] .content p{margin-bottom:1rem;line-height:1.4}[data-v-70afb46a] .content p code{background-color:#d2f3e1;border-radius:.25rem;padding:.25rem}[data-v-70afb46a] .content img{width:100%;border-radius:.5rem}[data-v-70afb46a] .content .highlight{margin-bottom:1rem;border-radius:.5rem}[data-v-70afb46a] .content ul{list-style:numeral;margin-bottom:1rem}[data-v-70afb46a] .content ul li p{margin-bottom:0}[data-v-70afb46a] .content ol{margin-bottom:1rem}aside[data-v-37984f8c]{padding:1rem;background-color:#dfe8ef;border-radius:1rem}aside .username-heading[data-v-37984f8c]{display:flex;margin-bottom:1rem}aside .username-heading[data-v-37984f8c]:hover{color:#6e87d2}aside .username-heading img[data-v-37984f8c]{width:3rem;height:3rem;border-radius:50%;margin-right:1rem}aside .username-heading .text[data-v-37984f8c]{display:flex;flex-direction:column;justify-content:center}aside .username-heading .text a[data-v-37984f8c]{line-height:1}aside .username-heading .text a[data-v-37984f8c]:first-child{font-size:1.25rem;font-weight:500;letter-spacing:-.025rem;margin-bottom:.25rem}aside .username-heading .text a[data-v-37984f8c]:last-child{color:#999;font-size:.875rem}aside .username-heading.loading[data-v-37984f8c]{display:block}aside .f-button[data-v-37984f8c]{display:block;width:100%;padding:.5rem;border-radius:.5rem;box-shadow:-4px -4px 8px #f8fafe,4px 4px 8px #ced2db;text-transform:uppercase;text-align:center;font-weight:600;letter-spacing:-.025rem;margin-bottom:1rem}aside .f-button[data-v-37984f8c]:hover{background:linear-gradient(135deg,rgba(0,0,0,.09),hsla(0,0%,100%,0))}aside .f-button[data-v-37984f8c]:active{background:0 0;box-shadow:inset -4px -4px 8px #f0f3f9,inset 4px 4px 8px #ced2db,inset -1px -1px 4px #8e8e8e}aside .info>div[data-v-37984f8c]{margin-bottom:.5rem}aside .info .title[data-v-37984f8c]{font-size:.666666rem;letter-spacing:-.0125rem;font-weight:500;color:#999;text-transform:uppercase;margin-bottom:.1rem}aside .info .content[data-v-37984f8c]{font-size:.875rem;line-height:1.4}.add-comment[data-v-8c4375bc]{display:block;width:100%;padding:.5rem;border-radius:.5rem;box-shadow:-4px -4px 8px #f8fafe,4px 4px 8px #ced2db;text-transform:uppercase;text-align:center;font-weight:600;letter-spacing:-.025rem;margin-bottom:1rem}.add-comment[data-v-8c4375bc]:hover{background:linear-gradient(135deg,rgba(0,0,0,.09),hsla(0,0%,100%,0))}.add-comment[data-v-8c4375bc]:active{background:0 0;box-shadow:inset -4px -4px 8px #f0f3f9,inset 4px 4px 8px #ced2db,inset -1px -1px 4px #8e8e8e}footer[data-v-22cb8fd0]{padding:2rem;text-align:center;display:flex;align-items:center;justify-content:center}footer span[data-v-22cb8fd0]{display:inline-block;line-height:1;text-transform:uppercase;letter-spacing:-.025rem;font-size:.75rem;font-weight:500}footer a svg[data-v-22cb8fd0]{width:3rem;height:3rem;margin:0 .5rem}footer a .nuxt-icon[data-v-22cb8fd0]{width:2.5rem;height:2.5rem;margin:0 .25rem}</style>
  </head>
  <body>
    <div data-server-rendered="true" id="__nuxt"><div id="__layout"><div><header data-v-27046cca><a href="/nuxtstop/" class="logo-wrapper nuxt-link-active" data-v-27046cca><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-v-27046cca><path d="M13.5599 8.54348L12.8055 9.87164L10.2257 5.3282L2.306 19.274H7.66815C7.66815 20.0075 8.25298 20.6021 8.97441 20.6021H2.306C1.83937 20.6021 1.40822 20.3489 1.17494 19.9379C0.941664 19.527 0.941687 19.0208 1.175 18.6099L9.09469 4.66412C9.32802 4.25316 9.75926 4 10.226 4C10.6926 4 11.1239 4.25316 11.3572 4.66412L13.5599 8.54348V8.54348Z" fill="#00C58E" data-v-27046cca></path><path d="M19.2769 18.6099L14.3143 9.87165L13.5599 8.54348L12.8055 9.87165L7.84343 18.6099C7.61011 19.0208 7.61009 19.527 7.84337 19.9379C8.07665 20.3489 8.50779 20.6021 8.97443 20.6021H18.1443C18.611 20.6021 19.0424 20.3491 19.2758 19.9382C19.5092 19.5272 19.5092 19.0209 19.2758 18.6099H19.2769ZM8.97443 19.274L13.5599 11.1998L18.1443 19.274H8.97443H8.97443Z" fill="#2F495E" data-v-27046cca></path><path d="M22.825 19.938C22.5917 20.3489 22.1606 20.6021 21.694 20.6021H18.1443C18.8657 20.6021 19.4505 20.0075 19.4505 19.274H21.6913L15.3331 8.07696L14.3142 9.87164L13.5599 8.54348L14.2021 7.41287C14.4354 7.00192 14.8667 6.74875 15.3334 6.74875C15.8001 6.74875 16.2313 7.00192 16.4646 7.41287L22.825 18.6099C23.0583 19.0208 23.0583 19.5271 22.825 19.938V19.938Z" fill="#108775" data-v-27046cca></path></svg> <div class="name-wrapper" data-v-27046cca><span class="app-name" data-v-27046cca>Nuxtstop</span> <p class="subtitle" data-v-27046cca>For all things nuxt.js</p></div></a> <nav data-v-27046cca><ul data-v-27046cca><li data-v-27046cca><a href="/nuxtstop/" class="nuxt-link-active" data-v-27046cca>
          New
        </a></li><li data-v-27046cca><a href="/nuxtstop/top" data-v-27046cca>
          Top
        </a></li></ul></nav></header> <div class="page-wrapper" data-v-10d06ee8><div class="article-content-wrapper" data-v-10d06ee8><article data-fetch-key="data-v-70afb46a:0" class="article-block" data-v-70afb46a data-v-10d06ee8><header data-v-70afb46a><h1 data-v-70afb46a>Web Scrapping With Python.</h1> <div class="tags" data-v-70afb46a><a href="/nuxtstop/t/python" class="tag" data-v-70afb46a>
          #python
        </a><a href="/nuxtstop/t/webscrapping" class="tag" data-v-70afb46a>
          #webscrapping
        </a><a href="/nuxtstop/t/100daysofcode" class="tag" data-v-70afb46a>
          #100daysofcode
        </a></div> <div class="image-wrapper" data-v-70afb46a><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--pWh569hs--/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/aa1qkgg9cflvm5fwqeed.png" alt="Web Scrapping With Python." data-v-70afb46a></div> <div class="meta" data-v-70afb46a><div class="scl" data-v-70afb46a><span data-v-70afb46a><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-v-70afb46a data-v-70afb46a><path d="M16.4444 3C14.6733 3 13.0333 3.94162 12 5.34C10.9667 3.94162 9.32667 3 7.55556 3C4.49222 3 2 5.52338 2 8.625C2 14.8024 11.0267 20.586 11.4122 20.829C11.5922 20.9426 11.7956 21 12 21C12.2044 21 12.4078 20.9426 12.5878 20.829C12.9733 20.586 22 14.8024 22 8.625C22 5.52338 19.5078 3 16.4444 3Z" fill="#FF0000" data-v-70afb46a data-v-70afb46a></path></svg>
            30
          </span> <span class="comments" data-v-70afb46a><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-v-70afb46a data-v-70afb46a><path d="M6.11765 22H4.94118L5.64706 21.05C6.11765 20.3969 6.41176 19.5656 6.58824 18.5563C3.64706 17.1906 2 14.6375 2 11.3125C2 6.20625 5.82353 3 12 3C18.1765 3 22 6.20625 22 11.3125C22 16.5375 18.2353 19.625 12 19.625H11.5882C10.6471 20.7531 9 22 6.11765 22ZM12 4.1875C6.47059 4.1875 3.17647 6.85937 3.17647 11.3125C3.17647 15.1125 5.47059 16.8938 7.41177 17.6656L7.82353 17.8437L7.76471 18.3187C7.64706 19.2687 7.47059 20.1 7.11765 20.8125C9.05882 20.575 10.1765 19.5656 10.8235 18.7344L11 18.4969H12C19.9412 18.4969 20.8235 13.5094 20.8235 11.3719C20.8235 6.85938 17.5294 4.1875 12 4.1875Z" fill="black" data-v-70afb46a data-v-70afb46a></path></svg>
            8
          </span></div> <time data-v-70afb46a>Feb 27</time></div></header> <div class="content" data-v-70afb46a><p>Suppose you want some data of a product from a company? Let's say the price of all commodities to be in a comma separated value(CSV) or photos from a social media! what will you do?<br>
Actually, you can copy information from the respective site and paste it into your own file. But what if you want to get a huge amount of information from the site as soon as possible? Such as large amounts of data from a website to train a Machine Learning algorithm?<br>
In that case, copy and paste will not work! And then you will need to use Web Scraping.Web scraping uses intelligence automation methods to get thousands or even millions of data sets in a smaller amount of time.</p>

<p><strong>What is Web Scraping?</strong></p>

<p>Web scraping is a means of extracting vast volumes of data from websites in an automated manner. The majority of this data is unstructured HTML data that is converted to structured data in a spreadsheet or database before being used in various applications. <br>
To gather data from websites, web scraping can be done in a variety of methods. These options include leveraging internet services, specific APIs, and even writing your own web scraping code from scratch. Many huge websites, such as Google, Twitter, Facebook, StackOverflow, and others, provide APIs that let you access their data in a structured fashion.</p>

<p><strong>Application of web scrapping</strong></p>

<ol>
<li>Market research</li>
<li>Price monitoring</li>
<li>News monitoring</li>
<li>Email marketing</li>
<li>Sentiment Analysis</li>
</ol>

<p><strong>Prerequisites</strong></p>

<ul>
<li>Python</li>
</ul>

<p>Why python🤔, since it is the most popular language for web scraping as it can handle most of the processes easily. It also has a variety of libraries that were created specifically for Web Scraping that is <a href="https://pypi.org/project/Scrapy/">scrapy</a> and <a href="https://pypi.org/project/beautifulsoup4/">beautiful soup</a>.</p>

<p><strong>So let's start 😀😀😀💪💪</strong></p>

<p><strong>1. Installing of python.</strong></p>

<p>Install python 3 and virtualenv then make virtual environment.</p>

<p>Install python 3 first by running following line of code in terminal:</p>

<p><code>$ sudo apt install python3</code></p>

<p>Then install virtual environment, in our terminal type in:</p>

<p><code>$ sudo apt install python3-venv</code></p>

<p>After installing python and virtualenv, create a folder and virtualenv then activate the created virtualenv.</p>

<ul>
<li>Create project folder:</li>
</ul>

<p><code>mkdir web_scrap</code></p>

<p>So lets go to the inside of web_scrap directory :</p>

<p><code>cd web_scrap</code></p>

<ul>
<li>Create virtualenv:</li>
</ul>

<p><code>virtualenv env</code></p>

<ul>
<li>activate virtualenv:</li>
</ul>

<p><code>. env/bin/activate</code></p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--QmpHCYlt--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/c1qjhc57492rpyr8mew9.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--QmpHCYlt--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/c1qjhc57492rpyr8mew9.png" alt="Image description" loading="lazy" width="727" height="325"></a></p>

<p>This are basic steps to setup our coding environment, check out <a href="https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/">this</a> for more.</p>

<p><strong>2. Create python file.</strong></p>

<p>Create a python file scrap.py and open it in <a href="https://code.visualstudio.com">visual studio</a> or on your favorite text editor.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--gE7G13TX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/3hto1lze67bqprxu2hos.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--gE7G13TX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/3hto1lze67bqprxu2hos.png" alt="Image description" loading="lazy" width="727" height="120"></a></p>

<p><strong>3. Import packages.</strong></p>

<p>Download and import packages in the virtual environment.</p>

<p><code>pip install requests</code></p>

<p><code>pip install bs4</code></p>

<p><code>pip install termcolor</code></p>

<p>The python modules that will be using:</p>

<ol>
<li>re - regular expression.</li>
<li>requests- to scrap data directory from Instagram.</li>
<li>beautifulSoup - to get specific filtered part from all data.</li>
<li>urllib - to use request to download from url.</li>
<li>os - to store downloaded file in our media folder.</li>
</ol>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--_cpcRk_x--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uhje5mxvqrtof3436oak.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--_cpcRk_x--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uhje5mxvqrtof3436oak.png" alt="Image description" loading="lazy" width="696" height="262"></a></p>

<p><strong>4. Get website link.</strong></p>

<p>Let's add a simple input system to get any url as an input url:</p>

<p><code>url = input("enter here your url from instagram")</code></p>

<p>Get any url from Instagram then get data from the url using <code>requests</code>.</p>

<p><code>data = requests.get(url)</code></p>

<p>You can print the data and check the results.</p>

<p>print(data)</p>

<p>The codes</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--L-2HQ4RH--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/livaj4z8aey6sv4mvjjw.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--L-2HQ4RH--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/livaj4z8aey6sv4mvjjw.png" alt="Image description" loading="lazy" width="880" height="129"></a></p>

<p>The outuput</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--c1KOcfFK--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/iond3yuvipty0l5u0tbo.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--c1KOcfFK--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/iond3yuvipty0l5u0tbo.png" alt="Image description" loading="lazy" width="726" height="114"></a></p>

<p>Now let's take a case for a video.</p>

<p><code>https://www.instagram.com/p/B_wH2aCnyEh/?utm_medium=copy_link</code></p>

<p>This is the page with the video.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--5Y8Kib74--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hgnt463r7qgsbi8g5v3t.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--5Y8Kib74--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hgnt463r7qgsbi8g5v3t.png" alt="Image description" loading="lazy" width="880" height="540"></a></p>

<p>And here is the source code.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--SsqKz-tQ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vb13xmrobea7uyqcnkw1.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--SsqKz-tQ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vb13xmrobea7uyqcnkw1.png" alt="Image description" loading="lazy" width="880" height="495"></a></p>

<p>And In This Page If you just find(by ctrl + F) ‘mp4’ . Then You will find something like this:</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--c4eNWXPA--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/09rw3swmxmvs8zyy38be.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--c4eNWXPA--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/09rw3swmxmvs8zyy38be.png" alt="Image description" loading="lazy" width="880" height="87"></a></p>

<p>The link that contain the mp4 is the main thing we need:</p>

<p><code>"https://instagram.fnbo9-1.fna.fbcdn.net/v/t50.2886-16/95332972_323221645317471_817729865566514230_n.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLjQ4MC5mZWVkLmRlZmF1bHQiLCJxZV9ncm91cHMiOiJbXCJpZ193ZWJfZGVsaXZlcnlfdnRzX290ZlwiXSJ9\u0026_nc_ht=instagram.fnbo9-1.fna.fbcdn.net\u0026_nc_cat=103\u0026_nc_ohc=Q1fkDGBA2oEAX9xsGin\u0026edm=AABBvjUBAAAA\u0026vs=18035297806253182_2714272676\u0026_nc_vs=HBksFQAYJEdHeXFyZ1ZmVlZybjl5VUJBRGJzVWUtNktGa0xia1lMQUFBRhUAAsgBABUAGCRHSFlhdkFWNG9oRUFsSEFHQVAwaFlDdDdtOVl0YmtZTEFBQUYVAgLIAQAoABgAGwGIB3VzZV9vaWwBMBUAACb8yIvTv8CJQBUCKAJDMywXQCbul41P3zsYEmRhc2hfYmFzZWxpbmVfMV92MREAdeoHAA%3D%3D\u0026ccb=7-4\u0026oe=621DCC10\u0026oh=00_AT_7jbU74b8Fm9-U5y6GQhURJihmzKNI_AEvVNjI4e-Blw\u0026_nc_sid=83d603"</code></p>

<p>Due to Instagram terms instead use the below link for video:</p>

<p><code>https://www.w3schools.com/html/movie.mp4</code></p>

<p><code>match = re.findall(r’url\W\W\W([-\W\w]+)\W\W\Wvideo_view_count’, str)</code></p>

<p>What the code above does is to find the url above whenever we run the code.</p>

<p>To extract the video we have to declare a variable name extraction and inside this variable we will store the file format for video, as shown below.</p>

<p><code>extraction = “.mp4”</code></p>

<p>Also do the same for image but use <code>profile_pic_url</code>:</p>

<p><code>"https://instagram.fnbo9-1.fna.fbcdn.net/v/t51.2885-19/274607143_1204294113308064_418123174948225933_n.jpg?stp=dst-jpg_s150x150\u0026_nc_ht=instagram.fnbo9-1.fna.fbcdn.net\u0026_nc_cat=100\u0026_nc_ohc=L3oR46dvCW0AX-fS68k\u0026edm=AABBvjUBAAAA\u0026ccb=7-4\u0026oh=00_AT_7whkb_tXXNikAlnrI8yBifCb9zDwZK0Zt5q462q93Vw\u0026oe=6222855B\u0026_nc_sid=83d603"</code></p>

<p>as shown below.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--U16eGCUv--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mbtdemuodr515z2nb12m.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--U16eGCUv--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mbtdemuodr515z2nb12m.png" alt="Image description" loading="lazy" width="880" height="639"></a></p>

<p>source code :</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--X-w5CcQz--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/g8uyynaxdkm9xou9l5tc.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--X-w5CcQz--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/g8uyynaxdkm9xou9l5tc.png" alt="Image description" loading="lazy" width="880" height="445"></a></p>

<p>search <code>profile_pic_url</code>:</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--QXhmrtBs--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8mh6w8leahon0wnh7vvk.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--QXhmrtBs--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8mh6w8leahon0wnh7vvk.png" alt="Image description" loading="lazy" width="613" height="66"></a></p>

<p>For image link use:</p>

<p><code>https://www.w3schools.com/html/pic_trulli.jpg</code></p>

<p><code>match = re.findall(r'profile_pic_url\W\W\W([\W\w]+)\W\W\Wdisplay_resources’, str)</code></p>

<p>And Now Our extraction variable value is :</p>

<p><code>extraction = “.jpg”</code></p>

<p>So last line of this step is to collect the actual post video or image’s url in a variable as a regular exp. array to string. To do that :</p>

<p><code>res = match[0]</code></p>

<p><strong>5. Data extraction.</strong></p>

<p>Here we have to download and get the caption of the post.</p>

<p>We will use BeautifulSoup in our code to get the caption or title of the post.We have to assign all data (str) to pass through BS4 and filter it .</p>

<p><code>page = BeautifulSoup(str, "html.parser")<br>
title = page.find("title")<br>
title = title.get_text()</code></p>

<p>So the code will find the title of this page and store the title varible.<br>
After this we have to perform regular expression to make our file name saved and also store in a media folder.</p>

<p><code>title = re.sub(r"\W+", "_", title)<br>
title = "download/web_scrap"+title+"web_scrap"<br>
print("\n"+title)</code></p>

<p>We use <code>download/</code> because we want to store our downloaded file in a new folder called <code>download/</code>.<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>if res != "" :
print('found \n \n'+'\033[1m'+colored(res, 'green')+'\033[0m'+'\n') #'found word:cat'
 download = input("Do you want to download(y/N) : ")
if (download == "y" or download == "Y"):
  try:
   fileName = title
   print("Downloading.....")
   DFU.urlretrieve(res, fileName+extraction)
   print("Download Successfully!")
   os.system("tree download")
except:
   print("Sorry! Download Unsuccessful")
else:
 print('did not find or post is from private account')
 exit()
</code></pre>
<div class="highlight__panel js-actions-panel">
<div class="highlight__panel-action js-fullscreen-code-action">
    <svg xmlns="http://www.w3.org/2000/svg" width="20px" height="20px" viewbox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-on"><title>Enter fullscreen mode</title>
    <path d="M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z"></path>
</svg>

    <svg xmlns="http://www.w3.org/2000/svg" width="20px" height="20px" viewbox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-off"><title>Exit fullscreen mode</title>
    <path d="M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z"></path>
</svg>

</div>
</div>
</div>



<p>So if <code>res</code> variable is not empty then print the actual link of post.Then make a input and this input will ask you that you want to download this file or not.To do so, answer with y or n .If answer is Y or y then it will continue working. </p>

<p><code>if (download == “y”):</code></p>

<p>That's all on how to download an image and a video from a social media Instagram.</p>

<p>Get the source code <a href="https://github.com/oderofrancis/web_scrap">here</a></p>

<p>THank you for taking your time to go through this article.</p>

<h3>
  <a name="keep-moving-on" href="#keep-moving-on">
  </a>
  <strong>KEEP MOVING ON 💪💪💪💪💪💪</strong>
</h3>

<h3>
  <a name="happy-coding" href="#happy-coding">
  </a>
  <strong>HAPPY CODING</strong>
</h3>

</div></article> <div class="aside-username-wrapper" data-v-10d06ee8><aside class="aside-username-block" data-v-37984f8c data-v-10d06ee8><div class="username-heading loading" data-v-37984f8c><div class="vue-content-placeholders vue-content-placeholders-is-animated" data-v-37984f8c><div class="vue-content-placeholders-heading" data-v-37984f8c><div class="vue-content-placeholders-heading__img"></div> <div class="vue-content-placeholders-heading__content"><div class="vue-content-placeholders-heading__title"></div> <div class="vue-content-placeholders-heading__subtitle"></div></div></div></div></div> <div class="info" data-v-37984f8c><div class="vue-content-placeholders vue-content-placeholders-is-animated" data-v-37984f8c><div class="vue-content-placeholders-text" data-v-37984f8c><div class="vue-content-placeholders-text__line"></div><div class="vue-content-placeholders-text__line"></div><div class="vue-content-placeholders-text__line"></div></div></div></div></aside></div></div> <div class="comments-block" data-v-8c4375bc data-v-10d06ee8><!----> <a href="https://dev.to/oderofrancis/python-for-everyone-mastering-python-the-right-way-51pg" target="_blank" rel="nofollow noopener noreferer" class="add-comment" data-v-8c4375bc>
    Add comment
  </a></div></div> <footer data-v-22cb8fd0><span data-v-22cb8fd0>Built with</span> <a href="https://nuxtjs.org" target="_blank" data-v-22cb8fd0><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" class="nuxt-icon" data-v-22cb8fd0 data-v-22cb8fd0><path d="M13.5599 8.54348L12.8055 9.87164L10.2257 5.3282L2.306 19.274H7.66815C7.66815 20.0075 8.25298 20.6021 8.97441 20.6021H2.306C1.83937 20.6021 1.40822 20.3489 1.17494 19.9379C0.941664 19.527 0.941687 19.0208 1.175 18.6099L9.09469 4.66412C9.32802 4.25316 9.75926 4 10.226 4C10.6926 4 11.1239 4.25316 11.3572 4.66412L13.5599 8.54348V8.54348Z" fill="#00C58E" data-v-22cb8fd0 data-v-22cb8fd0></path><path d="M19.2769 18.6099L14.3143 9.87165L13.5599 8.54348L12.8055 9.87165L7.84343 18.6099C7.61011 19.0208 7.61009 19.527 7.84337 19.9379C8.07665 20.3489 8.50779 20.6021 8.97443 20.6021H18.1443C18.611 20.6021 19.0424 20.3491 19.2758 19.9382C19.5092 19.5272 19.5092 19.0209 19.2758 18.6099H19.2769ZM8.97443 19.274L13.5599 11.1998L18.1443 19.274H8.97443H8.97443Z" fill="#2F495E" data-v-22cb8fd0 data-v-22cb8fd0></path><path d="M22.825 19.938C22.5917 20.3489 22.1606 20.6021 21.694 20.6021H18.1443C18.8657 20.6021 19.4505 20.0075 19.4505 19.274H21.6913L15.3331 8.07696L14.3142 9.87164L13.5599 8.54348L14.2021 7.41287C14.4354 7.00192 14.8667 6.74875 15.3334 6.74875C15.8001 6.74875 16.2313 7.00192 16.4646 7.41287L22.825 18.6099C23.0583 19.0208 23.0583 19.5271 22.825 19.938V19.938Z" fill="#108775" data-v-22cb8fd0 data-v-22cb8fd0></path></svg></a> <span data-v-22cb8fd0>&</span> <a href="https://docs.dev.to/api" rel="nofollow noopener" target="_blank" data-v-22cb8fd0><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-v-22cb8fd0 data-v-22cb8fd0><path d="M1.5726 5.13748C1.42945 5.20622 1.2411 5.36661 1.15822 5.48117C1 5.69503 1 5.74849 1 11.8739C1 17.9993 1 18.0528 1.15822 18.2667C1.2411 18.3812 1.42945 18.5416 1.5726 18.6104C1.8137 18.7402 2.46164 18.7478 12 18.7478C21.5384 18.7478 22.1863 18.7402 22.4274 18.6104C22.5706 18.5416 22.7589 18.3812 22.8418 18.2667C23 18.0528 23 17.9993 23 11.8739C23 5.74849 23 5.69503 22.8418 5.48117C22.7589 5.36661 22.5706 5.20622 22.4274 5.13748C22.1863 5.00764 21.5384 5 12 5C2.46164 5 1.8137 5.00764 1.5726 5.13748ZM7.7055 8.2613C8.0822 8.45989 8.59454 9.0098 8.77536 9.40694C8.89589 9.66664 8.91095 9.94922 8.91095 12.0649C8.91095 14.3104 8.90344 14.4478 8.75275 14.7839C8.51919 15.288 8.16506 15.6546 7.68288 15.899C7.26096 16.1052 7.22328 16.1128 5.7315 16.1358L4.20206 16.1663V12.1031V8.04744L5.80684 8.07035C7.27602 8.09327 7.42672 8.10854 7.7055 8.2613ZM13.6952 8.89521V9.73538H12.4521H11.2089V10.4991V11.2629H11.9623H12.7158V12.1031V12.9432H11.9623H11.2089V13.707V14.4708H12.4521H13.6952V15.3109V16.151H12C10.1315 16.151 10.0411 16.1358 9.67191 15.6928L9.47603 15.4484V12.1336C9.47603 8.46752 9.46851 8.49807 9.95069 8.20783C10.1692 8.07035 10.3425 8.05508 11.9473 8.05508H13.6952V8.89521ZM16.5658 10.3769C16.8897 11.6295 17.1685 12.6912 17.176 12.7293C17.1911 12.7675 17.4699 11.7441 17.8014 10.461C18.1254 9.17017 18.4343 8.1009 18.4795 8.08563C18.5247 8.06271 18.9541 8.06271 19.4288 8.07035L20.3028 8.09327L19.376 11.6219C18.8713 13.5542 18.4117 15.2269 18.3664 15.3261C18.0123 16.0135 17.274 16.3343 16.7164 16.0441C16.4528 15.899 16.0911 15.4865 15.9705 15.1887C15.9254 15.0665 15.4884 13.4549 15.0062 11.6142C14.524 9.76593 14.1171 8.20783 14.0945 8.15437C14.0644 8.07035 14.2301 8.05508 15.0212 8.07035L15.9856 8.09327L16.5658 10.3769Z" fill="black" data-v-22cb8fd0 data-v-22cb8fd0></path><path d="M5.93491 12.103V14.4707H6.27394C6.66574 14.4707 7.01983 14.3103 7.1404 14.0965C7.18559 14.0048 7.21575 13.2105 7.21575 12.0648V10.1783L6.99725 9.95683C6.80133 9.76591 6.71847 9.73535 6.35683 9.73535H5.93491V12.103Z" fill="black" data-v-22cb8fd0 data-v-22cb8fd0></path></svg></a></footer></div></div></div><script>window.__NUXT__=function(e,t,a,n,o){return a.type_of="article",a.id=1003302,a.title="Web Scrapping With Python.",a.description="Suppose you want some data of a product from a company? Let's say the price of all commodities to be...",a.readable_publish_date="Feb 27",a.slug="python-for-everyone-mastering-python-the-right-way-51pg",a.path="/oderofrancis/python-for-everyone-mastering-python-the-right-way-51pg",a.url=n,a.comments_count=8,a.public_reactions_count=30,a.collection_id=e,a.published_timestamp=t,a.positive_reactions_count=30,a.cover_image="https://res.cloudinary.com/practicaldev/image/fetch/s--pWh569hs--/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/aa1qkgg9cflvm5fwqeed.png",a.social_image="https://res.cloudinary.com/practicaldev/image/fetch/s--c9WnmKpM--/c_imagga_scale,f_auto,fl_progressive,h_500,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/aa1qkgg9cflvm5fwqeed.png",a.canonical_url=n,a.created_at=t,a.edited_at="2022-03-03T18:42:23Z",a.crossposted_at=e,a.published_at=t,a.last_comment_at="2022-03-03T18:41:48Z",a.reading_time_minutes=5,a.tag_list="python, webscrapping, 100daysofcode",a.tags=["python","webscrapping","100daysofcode"],a.body_html='<p>Suppose you want some data of a product from a company? Let\'s say the price of all commodities to be in a comma separated value(CSV) or photos from a social media! what will you do?<br>\nActually, you can copy information from the respective site and paste it into your own file. But what if you want to get a huge amount of information from the site as soon as possible? Such as large amounts of data from a website to train a Machine Learning algorithm?<br>\nIn that case, copy and paste will not work! And then you will need to use Web Scraping.Web scraping uses intelligence automation methods to get thousands or even millions of data sets in a smaller amount of time.</p>\n\n<p><strong>What is Web Scraping?</strong></p>\n\n<p>Web scraping is a means of extracting vast volumes of data from websites in an automated manner. The majority of this data is unstructured HTML data that is converted to structured data in a spreadsheet or database before being used in various applications. <br>\nTo gather data from websites, web scraping can be done in a variety of methods. These options include leveraging internet services, specific APIs, and even writing your own web scraping code from scratch. Many huge websites, such as Google, Twitter, Facebook, StackOverflow, and others, provide APIs that let you access their data in a structured fashion.</p>\n\n<p><strong>Application of web scrapping</strong></p>\n\n<ol>\n<li>Market research</li>\n<li>Price monitoring</li>\n<li>News monitoring</li>\n<li>Email marketing</li>\n<li>Sentiment Analysis</li>\n</ol>\n\n<p><strong>Prerequisites</strong></p>\n\n<ul>\n<li>Python</li>\n</ul>\n\n<p>Why python🤔, since it is the most popular language for web scraping as it can handle most of the processes easily. It also has a variety of libraries that were created specifically for Web Scraping that is <a href="https://pypi.org/project/Scrapy/">scrapy</a> and <a href="https://pypi.org/project/beautifulsoup4/">beautiful soup</a>.</p>\n\n<p><strong>So let\'s start 😀😀😀💪💪</strong></p>\n\n<p><strong>1. Installing of python.</strong></p>\n\n<p>Install python 3 and virtualenv then make virtual environment.</p>\n\n<p>Install python 3 first by running following line of code in terminal:</p>\n\n<p><code>$ sudo apt install python3</code></p>\n\n<p>Then install virtual environment, in our terminal type in:</p>\n\n<p><code>$ sudo apt install python3-venv</code></p>\n\n<p>After installing python and virtualenv, create a folder and virtualenv then activate the created virtualenv.</p>\n\n<ul>\n<li>Create project folder:</li>\n</ul>\n\n<p><code>mkdir web_scrap</code></p>\n\n<p>So lets go to the inside of web_scrap directory :</p>\n\n<p><code>cd web_scrap</code></p>\n\n<ul>\n<li>Create virtualenv:</li>\n</ul>\n\n<p><code>virtualenv env</code></p>\n\n<ul>\n<li>activate virtualenv:</li>\n</ul>\n\n<p><code>. env/bin/activate</code></p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--QmpHCYlt--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/c1qjhc57492rpyr8mew9.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--QmpHCYlt--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/c1qjhc57492rpyr8mew9.png" alt="Image description" loading="lazy" width="727" height="325"></a></p>\n\n<p>This are basic steps to setup our coding environment, check out <a href="https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/">this</a> for more.</p>\n\n<p><strong>2. Create python file.</strong></p>\n\n<p>Create a python file scrap.py and open it in <a href="https://code.visualstudio.com">visual studio</a> or on your favorite text editor.</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--gE7G13TX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/3hto1lze67bqprxu2hos.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--gE7G13TX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/3hto1lze67bqprxu2hos.png" alt="Image description" loading="lazy" width="727" height="120"></a></p>\n\n<p><strong>3. Import packages.</strong></p>\n\n<p>Download and import packages in the virtual environment.</p>\n\n<p><code>pip install requests</code></p>\n\n<p><code>pip install bs4</code></p>\n\n<p><code>pip install termcolor</code></p>\n\n<p>The python modules that will be using:</p>\n\n<ol>\n<li>re - regular expression.</li>\n<li>requests- to scrap data directory from Instagram.</li>\n<li>beautifulSoup - to get specific filtered part from all data.</li>\n<li>urllib - to use request to download from url.</li>\n<li>os - to store downloaded file in our media folder.</li>\n</ol>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--_cpcRk_x--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uhje5mxvqrtof3436oak.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--_cpcRk_x--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uhje5mxvqrtof3436oak.png" alt="Image description" loading="lazy" width="696" height="262"></a></p>\n\n<p><strong>4. Get website link.</strong></p>\n\n<p>Let\'s add a simple input system to get any url as an input url:</p>\n\n<p><code>url = input("enter here your url from instagram")</code></p>\n\n<p>Get any url from Instagram then get data from the url using <code>requests</code>.</p>\n\n<p><code>data = requests.get(url)</code></p>\n\n<p>You can print the data and check the results.</p>\n\n<p>print(data)</p>\n\n<p>The codes</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--L-2HQ4RH--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/livaj4z8aey6sv4mvjjw.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--L-2HQ4RH--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/livaj4z8aey6sv4mvjjw.png" alt="Image description" loading="lazy" width="880" height="129"></a></p>\n\n<p>The outuput</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--c1KOcfFK--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/iond3yuvipty0l5u0tbo.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--c1KOcfFK--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/iond3yuvipty0l5u0tbo.png" alt="Image description" loading="lazy" width="726" height="114"></a></p>\n\n<p>Now let\'s take a case for a video.</p>\n\n<p><code>https://www.instagram.com/p/B_wH2aCnyEh/?utm_medium=copy_link</code></p>\n\n<p>This is the page with the video.</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--5Y8Kib74--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hgnt463r7qgsbi8g5v3t.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--5Y8Kib74--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hgnt463r7qgsbi8g5v3t.png" alt="Image description" loading="lazy" width="880" height="540"></a></p>\n\n<p>And here is the source code.</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--SsqKz-tQ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vb13xmrobea7uyqcnkw1.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--SsqKz-tQ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vb13xmrobea7uyqcnkw1.png" alt="Image description" loading="lazy" width="880" height="495"></a></p>\n\n<p>And In This Page If you just find(by ctrl + F) ‘mp4’ . Then You will find something like this:</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--c4eNWXPA--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/09rw3swmxmvs8zyy38be.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--c4eNWXPA--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/09rw3swmxmvs8zyy38be.png" alt="Image description" loading="lazy" width="880" height="87"></a></p>\n\n<p>The link that contain the mp4 is the main thing we need:</p>\n\n<p><code>"https://instagram.fnbo9-1.fna.fbcdn.net/v/t50.2886-16/95332972_323221645317471_817729865566514230_n.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLjQ4MC5mZWVkLmRlZmF1bHQiLCJxZV9ncm91cHMiOiJbXCJpZ193ZWJfZGVsaXZlcnlfdnRzX290ZlwiXSJ9\\u0026_nc_ht=instagram.fnbo9-1.fna.fbcdn.net\\u0026_nc_cat=103\\u0026_nc_ohc=Q1fkDGBA2oEAX9xsGin\\u0026edm=AABBvjUBAAAA\\u0026vs=18035297806253182_2714272676\\u0026_nc_vs=HBksFQAYJEdHeXFyZ1ZmVlZybjl5VUJBRGJzVWUtNktGa0xia1lMQUFBRhUAAsgBABUAGCRHSFlhdkFWNG9oRUFsSEFHQVAwaFlDdDdtOVl0YmtZTEFBQUYVAgLIAQAoABgAGwGIB3VzZV9vaWwBMBUAACb8yIvTv8CJQBUCKAJDMywXQCbul41P3zsYEmRhc2hfYmFzZWxpbmVfMV92MREAdeoHAA%3D%3D\\u0026ccb=7-4\\u0026oe=621DCC10\\u0026oh=00_AT_7jbU74b8Fm9-U5y6GQhURJihmzKNI_AEvVNjI4e-Blw\\u0026_nc_sid=83d603"</code></p>\n\n<p>Due to Instagram terms instead use the below link for video:</p>\n\n<p><code>https://www.w3schools.com/html/movie.mp4</code></p>\n\n<p><code>match = re.findall(r’url\\W\\W\\W([-\\W\\w]+)\\W\\W\\Wvideo_view_count’, str)</code></p>\n\n<p>What the code above does is to find the url above whenever we run the code.</p>\n\n<p>To extract the video we have to declare a variable name extraction and inside this variable we will store the file format for video, as shown below.</p>\n\n<p><code>extraction = “.mp4”</code></p>\n\n<p>Also do the same for image but use <code>profile_pic_url</code>:</p>\n\n<p><code>"https://instagram.fnbo9-1.fna.fbcdn.net/v/t51.2885-19/274607143_1204294113308064_418123174948225933_n.jpg?stp=dst-jpg_s150x150\\u0026_nc_ht=instagram.fnbo9-1.fna.fbcdn.net\\u0026_nc_cat=100\\u0026_nc_ohc=L3oR46dvCW0AX-fS68k\\u0026edm=AABBvjUBAAAA\\u0026ccb=7-4\\u0026oh=00_AT_7whkb_tXXNikAlnrI8yBifCb9zDwZK0Zt5q462q93Vw\\u0026oe=6222855B\\u0026_nc_sid=83d603"</code></p>\n\n<p>as shown below.</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--U16eGCUv--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mbtdemuodr515z2nb12m.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--U16eGCUv--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mbtdemuodr515z2nb12m.png" alt="Image description" loading="lazy" width="880" height="639"></a></p>\n\n<p>source code :</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--X-w5CcQz--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/g8uyynaxdkm9xou9l5tc.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--X-w5CcQz--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/g8uyynaxdkm9xou9l5tc.png" alt="Image description" loading="lazy" width="880" height="445"></a></p>\n\n<p>search <code>profile_pic_url</code>:</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--QXhmrtBs--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8mh6w8leahon0wnh7vvk.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--QXhmrtBs--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8mh6w8leahon0wnh7vvk.png" alt="Image description" loading="lazy" width="613" height="66"></a></p>\n\n<p>For image link use:</p>\n\n<p><code>https://www.w3schools.com/html/pic_trulli.jpg</code></p>\n\n<p><code>match = re.findall(r\'profile_pic_url\\W\\W\\W([\\W\\w]+)\\W\\W\\Wdisplay_resources’, str)</code></p>\n\n<p>And Now Our extraction variable value is :</p>\n\n<p><code>extraction = “.jpg”</code></p>\n\n<p>So last line of this step is to collect the actual post video or image’s url in a variable as a regular exp. array to string. To do that :</p>\n\n<p><code>res = match[0]</code></p>\n\n<p><strong>5. Data extraction.</strong></p>\n\n<p>Here we have to download and get the caption of the post.</p>\n\n<p>We will use BeautifulSoup in our code to get the caption or title of the post.We have to assign all data (str) to pass through BS4 and filter it .</p>\n\n<p><code>page = BeautifulSoup(str, "html.parser")<br>\ntitle = page.find("title")<br>\ntitle = title.get_text()</code></p>\n\n<p>So the code will find the title of this page and store the title varible.<br>\nAfter this we have to perform regular expression to make our file name saved and also store in a media folder.</p>\n\n<p><code>title = re.sub(r"\\W+", "_", title)<br>\ntitle = "download/web_scrap"+title+"web_scrap"<br>\nprint("\\n"+title)</code></p>\n\n<p>We use <code>download/</code> because we want to store our downloaded file in a new folder called <code>download/</code>.<br>\n</p>\n\n<div class="highlight js-code-highlight">\n<pre class="highlight plaintext"><code>if res != "" :\nprint(\'found \\n \\n\'+\'\\033[1m\'+colored(res, \'green\')+\'\\033[0m\'+\'\\n\') #\'found word:cat\'\n download = input("Do you want to download(y/N) : ")\nif (download == "y" or download == "Y"):\n  try:\n   fileName = title\n   print("Downloading.....")\n   DFU.urlretrieve(res, fileName+extraction)\n   print("Download Successfully!")\n   os.system("tree download")\nexcept:\n   print("Sorry! Download Unsuccessful")\nelse:\n print(\'did not find or post is from private account\')\n exit()\n</code></pre>\n<div class="highlight__panel js-actions-panel">\n<div class="highlight__panel-action js-fullscreen-code-action">\n    <svg xmlns="http://www.w3.org/2000/svg" width="20px" height="20px" viewbox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-on"><title>Enter fullscreen mode</title>\n    <path d="M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z"></path>\n</svg>\n\n    <svg xmlns="http://www.w3.org/2000/svg" width="20px" height="20px" viewbox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-off"><title>Exit fullscreen mode</title>\n    <path d="M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z"></path>\n</svg>\n\n</div>\n</div>\n</div>\n\n\n\n<p>So if <code>res</code> variable is not empty then print the actual link of post.Then make a input and this input will ask you that you want to download this file or not.To do so, answer with y or n .If answer is Y or y then it will continue working. </p>\n\n<p><code>if (download == “y”):</code></p>\n\n<p>That\'s all on how to download an image and a video from a social media Instagram.</p>\n\n<p>Get the source code <a href="https://github.com/oderofrancis/web_scrap">here</a></p>\n\n<p>THank you for taking your time to go through this article.</p>\n\n<h3>\n  <a name="keep-moving-on" href="#keep-moving-on">\n  </a>\n  <strong>KEEP MOVING ON 💪💪💪💪💪💪</strong>\n</h3>\n\n<h3>\n  <a name="happy-coding" href="#happy-coding">\n  </a>\n  <strong>HAPPY CODING</strong>\n</h3>\n\n',a.body_markdown='Suppose you want some data of a product from a company? Let\'s say the price of all commodities to be in a comma separated value(CSV) or photos from a social media! what will you do?\nActually, you can copy information from the respective site and paste it into your own file. But what if you want to get a huge amount of information from the site as soon as possible? Such as large amounts of data from a website to train a Machine Learning algorithm?\nIn that case, copy and paste will not work! And then you will need to use Web Scraping.Web scraping uses intelligence automation methods to get thousands or even millions of data sets in a smaller amount of time.\n\n**What is Web Scraping?**\n\nWeb scraping is a means of extracting vast volumes of data from websites in an automated manner. The majority of this data is unstructured HTML data that is converted to structured data in a spreadsheet or database before being used in various applications. \nTo gather data from websites, web scraping can be done in a variety of methods. These options include leveraging internet services, specific APIs, and even writing your own web scraping code from scratch. Many huge websites, such as Google, Twitter, Facebook, StackOverflow, and others, provide APIs that let you access their data in a structured fashion.\n\n\n**Application of web scrapping**\n\n1. Market research\n2. Price monitoring\n3. News monitoring\n4. Email marketing\n5. Sentiment Analysis\n\n**Prerequisites**\n\n- Python\n\nWhy python🤔, since it is the most popular language for web scraping as it can handle most of the processes easily. It also has a variety of libraries that were created specifically for Web Scraping that is [scrapy](https://pypi.org/project/Scrapy/) and [beautiful soup](https://pypi.org/project/beautifulsoup4/).\n\n**So let\'s start 😀😀😀💪💪**\n\n**1. Installing of python.**\n\nInstall python 3 and virtualenv then make virtual environment.\n\nInstall python 3 first by running following line of code in terminal:\n\n`$ sudo apt install python3`\n\nThen install virtual environment, in our terminal type in:\n\n`$ sudo apt install python3-venv`\n\nAfter installing python and virtualenv, create a folder and virtualenv then activate the created virtualenv.\n\n- Create project folder:\n\n`mkdir web_scrap `\n\nSo lets go to the inside of web_scrap directory :\n\n`cd web_scrap`\n\n- Create virtualenv:\n\n`virtualenv env`\n\n- activate virtualenv:\n\n`. env/bin/activate`\n\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/c1qjhc57492rpyr8mew9.png)\n\nThis are basic steps to setup our coding environment, check out [this](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) for more.\n\n**2. Create python file.**\n\nCreate a python file scrap.py and open it in [visual studio](https://code.visualstudio.com) or on your favorite text editor.\n\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/3hto1lze67bqprxu2hos.png)\n\n\n**3. Import packages.**\n\nDownload and import packages in the virtual environment.\n\n`pip install requests`\n\n`pip install bs4`\n\n`pip install termcolor`\n\nThe python modules that will be using:\n\n1. re - regular expression.\n2. requests- to scrap data directory from Instagram.\n3. beautifulSoup - to get specific filtered part from all data.\n4. urllib - to use request to download from url.\n5. os - to store downloaded file in our media folder.\n\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uhje5mxvqrtof3436oak.png)\n\n\n**4. Get website link.**\n\nLet\'s add a simple input system to get any url as an input url:\n\n`url = input("enter here your url from instagram")`\n\nGet any url from Instagram then get data from the url using `requests`.\n\n`data = requests.get(url)`\n\nYou can print the data and check the results.\n\nprint(data)\n\nThe codes\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/livaj4z8aey6sv4mvjjw.png)\n\nThe outuput\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/iond3yuvipty0l5u0tbo.png)\n\nNow let\'s take a case for a video.\n\n`https://www.instagram.com/p/B_wH2aCnyEh/?utm_medium=copy_link`\n\nThis is the page with the video.\n\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hgnt463r7qgsbi8g5v3t.png)\n\nAnd here is the source code.\n\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vb13xmrobea7uyqcnkw1.png)\n\nAnd In This Page If you just find(by ctrl + F) ‘mp4’ . Then You will find something like this:\n\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/09rw3swmxmvs8zyy38be.png)\n\nThe link that contain the mp4 is the main thing we need:\n\n`"https://instagram.fnbo9-1.fna.fbcdn.net/v/t50.2886-16/95332972_323221645317471_817729865566514230_n.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLjQ4MC5mZWVkLmRlZmF1bHQiLCJxZV9ncm91cHMiOiJbXCJpZ193ZWJfZGVsaXZlcnlfdnRzX290ZlwiXSJ9\\u0026_nc_ht=instagram.fnbo9-1.fna.fbcdn.net\\u0026_nc_cat=103\\u0026_nc_ohc=Q1fkDGBA2oEAX9xsGin\\u0026edm=AABBvjUBAAAA\\u0026vs=18035297806253182_2714272676\\u0026_nc_vs=HBksFQAYJEdHeXFyZ1ZmVlZybjl5VUJBRGJzVWUtNktGa0xia1lMQUFBRhUAAsgBABUAGCRHSFlhdkFWNG9oRUFsSEFHQVAwaFlDdDdtOVl0YmtZTEFBQUYVAgLIAQAoABgAGwGIB3VzZV9vaWwBMBUAACb8yIvTv8CJQBUCKAJDMywXQCbul41P3zsYEmRhc2hfYmFzZWxpbmVfMV92MREAdeoHAA%3D%3D\\u0026ccb=7-4\\u0026oe=621DCC10\\u0026oh=00_AT_7jbU74b8Fm9-U5y6GQhURJihmzKNI_AEvVNjI4e-Blw\\u0026_nc_sid=83d603"`\n\nDue to Instagram terms instead use the below link for video:\n\n`https://www.w3schools.com/html/movie.mp4`\n\n\n`match = re.findall(r’url\\W\\W\\W([-\\W\\w]+)\\W\\W\\Wvideo_view_count’, str)`\n\nWhat the code above does is to find the url above whenever we run the code.\n\nTo extract the video we have to declare a variable name extraction and inside this variable we will store the file format for video, as shown below.\n\n`extraction = “.mp4”`\n\nAlso do the same for image but use `profile_pic_url`:\n\n`"https://instagram.fnbo9-1.fna.fbcdn.net/v/t51.2885-19/274607143_1204294113308064_418123174948225933_n.jpg?stp=dst-jpg_s150x150\\u0026_nc_ht=instagram.fnbo9-1.fna.fbcdn.net\\u0026_nc_cat=100\\u0026_nc_ohc=L3oR46dvCW0AX-fS68k\\u0026edm=AABBvjUBAAAA\\u0026ccb=7-4\\u0026oh=00_AT_7whkb_tXXNikAlnrI8yBifCb9zDwZK0Zt5q462q93Vw\\u0026oe=6222855B\\u0026_nc_sid=83d603"`\n\n\n\nas shown below.\n\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mbtdemuodr515z2nb12m.png)\n\nsource code :\n\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/g8uyynaxdkm9xou9l5tc.png)\n\nsearch `profile_pic_url`:\n\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8mh6w8leahon0wnh7vvk.png)\n\nFor image link use:\n\n`https://www.w3schools.com/html/pic_trulli.jpg`\n\n\n`match = re.findall(r\'profile_pic_url\\W\\W\\W([\\W\\w]+)\\W\\W\\Wdisplay_resources’, str)`\n\nAnd Now Our extraction variable value is :\n\n`extraction = “.jpg”`\n\n\nSo last line of this step is to collect the actual post video or image’s url in a variable as a regular exp. array to string. To do that :\n\n`res = match[0]`\n\n**5. Data extraction.**\n\nHere we have to download and get the caption of the post.\n\nWe will use BeautifulSoup in our code to get the caption or title of the post.We have to assign all data (str) to pass through BS4 and filter it .\n\n`page = BeautifulSoup(str, "html.parser")\ntitle = page.find("title")\ntitle = title.get_text()`\n\nSo the code will find the title of this page and store the title varible.\nAfter this we have to perform regular expression to make our file name saved and also store in a media folder.\n\n`title = re.sub(r"\\W+", "_", title)\ntitle = "download/web_scrap"+title+"web_scrap"\nprint("\\n"+title)`\n\nWe use `download/` because we want to store our downloaded file in a new folder called `download/`.\n\n\n```\nif res != "" :\nprint(\'found \\n \\n\'+\'\\033[1m\'+colored(res, \'green\')+\'\\033[0m\'+\'\\n\') #\'found word:cat\'\n download = input("Do you want to download(y/N) : ")\nif (download == "y" or download == "Y"):\n  try:\n   fileName = title\n   print("Downloading.....")\n   DFU.urlretrieve(res, fileName+extraction)\n   print("Download Successfully!")\n   os.system("tree download")\nexcept:\n   print("Sorry! Download Unsuccessful")\nelse:\n print(\'did not find or post is from private account\')\n exit()\n```\n\nSo if `res` variable is not empty then print the actual link of post.Then make a input and this input will ask you that you want to download this file or not.To do so, answer with y or n .If answer is Y or y then it will continue working. \n\n`if (download == “y”):`\n\n\nThat\'s all on how to download an image and a video from a social media Instagram.\n\nGet the source code [here](https://github.com/oderofrancis/web_scrap)\n\nTHank you for taking your time to go through this article.\n\n###**KEEP MOVING ON 💪💪💪💪💪💪**\n\n###**HAPPY CODING**\n\n',a.user={name:"FRANCIS ODERO",username:o,twitter_username:"_francisodero_",github_username:o,website_url:e,profile_image:"https://res.cloudinary.com/practicaldev/image/fetch/s--ECkyIS8f--/c_fill,f_auto,fl_progressive,h_640,q_auto,w_640/https://dev-to-uploads.s3.amazonaws.com/uploads/user/profile_image/810404/e233ca2c-9b97-4574-884c-964fe0d34bf5.jpeg",profile_image_90:"https://res.cloudinary.com/practicaldev/image/fetch/s--C0zfZMP2--/c_fill,f_auto,fl_progressive,h_90,q_auto,w_90/https://dev-to-uploads.s3.amazonaws.com/uploads/user/profile_image/810404/e233ca2c-9b97-4574-884c-964fe0d34bf5.jpeg"},{layout:"default",data:[{}],fetch:{"data-v-70afb46a:0":{article:a}},error:e,state:{currentArticle:a},serverRendered:!0,routePath:"/oderofrancis/1003302",config:{_app:{basePath:"/nuxtstop/",assetsPath:"/nuxtstop/_nuxt/",cdnURL:e}}}}(null,"2022-02-27T17:03:41Z",{},"https://dev.to/oderofrancis/python-for-everyone-mastering-python-the-right-way-51pg","oderofrancis")</script><script src="/nuxtstop/_nuxt/f6e87fb.js" defer></script><script src="/nuxtstop/_nuxt/dc9ce94.js" defer></script><script src="/nuxtstop/_nuxt/6474719.js" defer></script><script src="/nuxtstop/_nuxt/9b75090.js" defer></script><script src="/nuxtstop/_nuxt/18df600.js" defer></script>
  </body>
</html>
