<!doctype html>
<html data-n-head-ssr lang="en" data-n-head="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">
  <head>
    <title>Python Web Scraping Tutorial: Step-By-Step</title><meta data-n-head="ssr" charset="utf-8"><meta data-n-head="ssr" name="viewport" content="width=device-width,initial-scale=1"><meta data-n-head="ssr" data-hid="description" name="description" content="Using Nuxt.js fetch() hook to build dev.to with a new look"><meta data-n-head="ssr" name="format-detection" content="telephone=no"><base href="/nuxtstop/"><link data-n-head="ssr" rel="icon" type="image/x-icon" href="/favicon.ico"><link data-n-head="ssr" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:400,500,600&display=swap"><link rel="preload" href="/nuxtstop/_nuxt/f6e87fb.js" as="script"><link rel="preload" href="/nuxtstop/_nuxt/6474719.js" as="script"><link rel="preload" href="/nuxtstop/_nuxt/9b75090.js" as="script"><link rel="preload" href="/nuxtstop/_nuxt/18df600.js" as="script"><link rel="preload" href="/nuxtstop/_nuxt/dc9ce94.js" as="script"><style data-vue-ssr-id="c650fd98:0 af4684f0:0 a9c71758:0 dcafa518:0 4b9cec49:0 b093d766:0 9d98bcb4:0 6b6a11ea:0 0248ed80:0 ea8e4264:0">html{box-sizing:border-box;font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}*,:after,:before{box-sizing:inherit}html{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,:after,:before{border:0 solid #e0e0e0}blockquote,body,dd,dl,figure,h1,h2,h3,h4,h5,h6,p,pre{margin:0}button{background:0 0;padding:0}button:focus{outline:1px dotted;outline:5px auto -webkit-focus-ring-color}fieldset,ol,ul{margin:0;padding:0}ol,ul{list-style:none}hr{border-width:1px}img{border-style:solid}textarea{resize:vertical}input::-moz-placeholder,textarea::-moz-placeholder{color:inherit;opacity:.5}input:-ms-input-placeholder,textarea:-ms-input-placeholder{color:inherit;opacity:.5}input::placeholder,textarea::placeholder{color:inherit;opacity:.5}[role=button],button{cursor:pointer}table{border-collapse:collapse}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit;font-family:sans-serif}a{color:inherit;text-decoration:inherit}button,input,optgroup,select,textarea{padding:0;line-height:inherit;color:inherit;font-family:inherit;font-size:100%}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;overflow:auto;word-break:break-word;white-space:normal}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{max-width:100%;height:auto}html{height:100%;font-size:18px;-ms-overflow-style:scrollbar;-webkit-tap-highlight-color:transparent;-webkit-touch-callout:none}@media(min-width:640px){html{font-size:20px}}body{height:100%;min-width:320px;font-family:Inter,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-weight:400;line-height:1.5;color:#000;background-color:#eff4f7;-webkit-text-rendering:optimizeLegibility;text-rendering:optimizeLegibility;font-synthesis:none;font-kerning:normal;font-feature-settings:"normal","kern";-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;-webkit-overflow-scrolling:touch;overflow-x:hidden;overflow-y:scroll}h1,h2,h3,h4,h5,h6{color:#000;font-family:Inter,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-weight:600;font-feature-settings:"normal";line-height:1.2}pre{background:#29292e;border-radius:2px;overflow:auto;padding:1rem;color:#eff1f9;line-height:1.42em;font-size:13px}@media screen and (min-width:380px){pre{font-size:15px}}pre code{background:#29292e;color:#eff0f9;white-space:pre}div.highlight pre.highlight code{font-size:inherit;padding:0}div.inner-comment div.body div.highlight pre.highlight{background:#29292e}div.inner-comment div.body div.highlight pre.highlight code{font-size:inherit;white-space:inherit;background:inherit;color:inherit}.highlight .hll{background-color:#49483e}.highlight{background:#29292e;color:#f8f8f2}.highlight .c{color:grey}.highlight .err{text-shadow:0 0 7px #f9690e}.highlight .k{color:#f39c12}.highlight .l{color:plum}.highlight .n{color:#f8f8f2}.highlight .o{color:#f9690e}.highlight .p{color:#f8f8f2}.highlight .c1,.highlight .ch,.highlight .cm,.highlight .cp,.highlight .cpf,.highlight .cs{color:grey}.highlight .gd{color:#f9690e}.highlight .ge{font-style:italic}.highlight .gi{color:#7ed07e}.highlight .gs{font-weight:700}.highlight .gu{color:grey}.highlight .kc,.highlight .kd{color:#f39c12}.highlight .kn{color:#f9690e}.highlight .kp,.highlight .kr,.highlight .kt{color:#f39c12}.highlight .ld{color:#f2ca27}.highlight .m{color:plum}.highlight .s{color:#f2ca27}.highlight .na{color:#7ed07e}.highlight .nb{color:#f8f8f2}.highlight .nc{color:#7ed07e}.highlight .no{color:#f39c12}.highlight .nd{color:#7ed07e}.highlight .ni{color:#f8f8f2}.highlight .ne,.highlight .nf{color:#7ed07e}.highlight .nl,.highlight .nn{color:#f8f8f2}.highlight .nx{color:#7ed07e}.highlight .py{color:#f8f8f2}.highlight .nt{color:#f9690e}.highlight .nv{color:#f8f8f2}.highlight .ow{color:#f9690e}.highlight .w{color:#f8f8f2}.highlight .mb,.highlight .mf,.highlight .mh,.highlight .mi,.highlight .mo{color:plum}.highlight .dl,.highlight .s2,.highlight .sa,.highlight .sb,.highlight .sc,.highlight .sd{color:#f2ca27}.highlight .se{color:plum}.highlight .s1,.highlight .sh,.highlight .si,.highlight .sr,.highlight .ss,.highlight .sx{color:#f2ca27}.highlight .bp{color:#f8f8f2}.highlight .fm{color:#7ed07e}.highlight .vc,.highlight .vg,.highlight .vi,.highlight .vm{color:#f8f8f2}.highlight .il{color:plum}.vue-content-placeholders-heading__img,.vue-content-placeholders-heading__subtitle,.vue-content-placeholders-heading__title,.vue-content-placeholders-img,.vue-content-placeholders-text__line{background:#bfcdec!important}.vue-content-placeholders-is-animated .vue-content-placeholders-heading__img:before,.vue-content-placeholders-is-animated .vue-content-placeholders-heading__subtitle:before,.vue-content-placeholders-is-animated .vue-content-placeholders-heading__title:before,.vue-content-placeholders-is-animated .vue-content-placeholders-img:before,.vue-content-placeholders-is-animated .vue-content-placeholders-text__line:before{background:linear-gradient(90deg,transparent 0,#d3ddf9 15%,transparent 30%)!important}header[data-v-27046cca]{max-width:1280px;margin:auto;padding:1rem;height:6rem;border-bottom:1px solid rgba(0,0,0,.2)}header .logo-wrapper[data-v-27046cca],header[data-v-27046cca]{display:flex;align-items:center;justify-content:space-between}header .logo-wrapper[data-v-27046cca]{margin:0 .5rem}header .logo-wrapper svg[data-v-27046cca]{width:3rem;height:100%}header .logo-wrapper .name-wrapper[data-v-27046cca]{margin-left:.6em}header .logo-wrapper .name-wrapper .subtitle[data-v-27046cca]{font-size:1rem}header .logo-wrapper .name-wrapper .app-name[data-v-27046cca]{font-weight:700;font-size:2.25rem;line-height:1.25}header nav[data-v-27046cca]{letter-spacing:-.025rem;font-weight:600;text-transform:uppercase}header nav ul[data-v-27046cca]{display:flex}header nav ul li[data-v-27046cca]{margin:0 .5rem}header nav ul li a[data-v-27046cca]{box-shadow:-4px -4px 8px #f8fafe,4px 4px 8px #ced2db;padding:.25rem 1rem;border-radius:.5rem;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}header nav ul li a[data-v-27046cca]:hover{background:linear-gradient(135deg,rgba(0,0,0,.09),hsla(0,0%,100%,0))}header nav ul li a.nuxt-link-exact-active[data-v-27046cca]{cursor:default}header nav ul li a.nuxt-link-exact-active[data-v-27046cca],header nav ul li a[data-v-27046cca]:active{background:0 0;box-shadow:inset -4px -4px 8px #f0f3f9,inset 4px 4px 8px #ced2db,inset -1px -1px 4px #8e8e8e}.page-wrapper[data-v-10d06ee8]{max-width:1280px;margin:auto;padding:1rem}.article-content-wrapper[data-v-10d06ee8]{display:flex;flex-direction:column;align-items:center;margin:auto auto 2rem}@media(min-width:1024px){.article-content-wrapper[data-v-10d06ee8]{align-items:normal;flex-direction:row}}.article-content-wrapper .article-block[data-v-10d06ee8]{width:100%;max-width:880px}@media(min-width:1024px){.article-content-wrapper .article-block[data-v-10d06ee8]{margin-right:1rem;width:66.66666%;margin-bottom:2rem}}.article-content-wrapper .aside-username-wrapper[data-v-10d06ee8]{max-width:880px;width:100%;position:relative}@media(min-width:1024px){.article-content-wrapper .aside-username-wrapper[data-v-10d06ee8]{display:block;width:33.33333%}}.article-content-wrapper .aside-username-wrapper .aside-username-block[data-v-10d06ee8]{position:-webkit-sticky;position:sticky;top:1rem}@media(min-width:1280px){.comments-block[data-v-10d06ee8]{margin:.5rem}}article[data-v-70afb46a]{padding:.5rem;border-radius:1rem}header h1[data-v-70afb46a],header[data-v-70afb46a]{margin-bottom:1rem}header h1[data-v-70afb46a]{font-size:2.25rem;letter-spacing:-.025rem}header .tags[data-v-70afb46a]{display:flex;flex-wrap:wrap;margin-bottom:1.5rem}header .tags .tag[data-v-70afb46a]{font-weight:500;line-height:1;padding:.5rem;margin:0 .5rem .5rem 0;border-radius:.25rem;box-shadow:-4px -4px 8px #f8fafe,4px 4px 8px #ced2db}header .tags .tag[data-v-70afb46a]:hover{background:linear-gradient(135deg,rgba(0,0,0,.09),hsla(0,0%,100%,0))}header .tags .tag[data-v-70afb46a]:active{background:0 0;box-shadow:inset -4px -4px 8px #f0f3f9,inset 4px 4px 8px #ced2db,inset -1px -1px 4px #8e8e8e}header .image-wrapper[data-v-70afb46a]{position:relative;padding-bottom:56.25%;background-color:#d4dfe8;margin-bottom:1.5rem;border-radius:.5rem;overflow:hidden}@media(min-width:834px){header .image-wrapper[data-v-70afb46a]{margin-bottom:1.5rem}}header .image-wrapper img[data-v-70afb46a]{position:absolute;top:0;left:0;width:100%;height:100%;-o-object-fit:cover;object-fit:cover}header .meta[data-v-70afb46a]{line-height:1;font-size:.875rem;text-transform:uppercase;font-weight:500;letter-spacing:-.025rem;display:flex;align-items:center;justify-content:space-between}header .meta .scl[data-v-70afb46a]{display:flex}header .meta .scl span[data-v-70afb46a]{display:flex;align-items:center;margin-right:1rem}header .meta .scl span svg[data-v-70afb46a]{margin-right:.25rem}header .meta .scl .comments[data-v-70afb46a]{cursor:pointer}[data-v-70afb46a] .content .ltag__user{display:none}[data-v-70afb46a] .content iframe{max-width:100%}[data-v-70afb46a] .content h1{font-size:1.875rem}[data-v-70afb46a] .content h1,[data-v-70afb46a] .content h2{margin-top:2rem;margin-bottom:1rem;letter-spacing:-.025rem}[data-v-70afb46a] .content h2{font-size:1.5rem}[data-v-70afb46a] .content h3{font-size:1.25rem}[data-v-70afb46a] .content h3,[data-v-70afb46a] .content h4{margin-top:2rem;margin-bottom:1rem;letter-spacing:-.025rem}[data-v-70afb46a] .content h4{font-size:1rem}[data-v-70afb46a] .content a{color:#6e87d2}[data-v-70afb46a] .content p{margin-bottom:1rem;line-height:1.4}[data-v-70afb46a] .content p code{background-color:#d2f3e1;border-radius:.25rem;padding:.25rem}[data-v-70afb46a] .content img{width:100%;border-radius:.5rem}[data-v-70afb46a] .content .highlight{margin-bottom:1rem;border-radius:.5rem}[data-v-70afb46a] .content ul{list-style:numeral;margin-bottom:1rem}[data-v-70afb46a] .content ul li p{margin-bottom:0}[data-v-70afb46a] .content ol{margin-bottom:1rem}aside[data-v-37984f8c]{padding:1rem;background-color:#dfe8ef;border-radius:1rem}aside .username-heading[data-v-37984f8c]{display:flex;margin-bottom:1rem}aside .username-heading[data-v-37984f8c]:hover{color:#6e87d2}aside .username-heading img[data-v-37984f8c]{width:3rem;height:3rem;border-radius:50%;margin-right:1rem}aside .username-heading .text[data-v-37984f8c]{display:flex;flex-direction:column;justify-content:center}aside .username-heading .text a[data-v-37984f8c]{line-height:1}aside .username-heading .text a[data-v-37984f8c]:first-child{font-size:1.25rem;font-weight:500;letter-spacing:-.025rem;margin-bottom:.25rem}aside .username-heading .text a[data-v-37984f8c]:last-child{color:#999;font-size:.875rem}aside .username-heading.loading[data-v-37984f8c]{display:block}aside .f-button[data-v-37984f8c]{display:block;width:100%;padding:.5rem;border-radius:.5rem;box-shadow:-4px -4px 8px #f8fafe,4px 4px 8px #ced2db;text-transform:uppercase;text-align:center;font-weight:600;letter-spacing:-.025rem;margin-bottom:1rem}aside .f-button[data-v-37984f8c]:hover{background:linear-gradient(135deg,rgba(0,0,0,.09),hsla(0,0%,100%,0))}aside .f-button[data-v-37984f8c]:active{background:0 0;box-shadow:inset -4px -4px 8px #f0f3f9,inset 4px 4px 8px #ced2db,inset -1px -1px 4px #8e8e8e}aside .info>div[data-v-37984f8c]{margin-bottom:.5rem}aside .info .title[data-v-37984f8c]{font-size:.666666rem;letter-spacing:-.0125rem;font-weight:500;color:#999;text-transform:uppercase;margin-bottom:.1rem}aside .info .content[data-v-37984f8c]{font-size:.875rem;line-height:1.4}.add-comment[data-v-8c4375bc]{display:block;width:100%;padding:.5rem;border-radius:.5rem;box-shadow:-4px -4px 8px #f8fafe,4px 4px 8px #ced2db;text-transform:uppercase;text-align:center;font-weight:600;letter-spacing:-.025rem;margin-bottom:1rem}.add-comment[data-v-8c4375bc]:hover{background:linear-gradient(135deg,rgba(0,0,0,.09),hsla(0,0%,100%,0))}.add-comment[data-v-8c4375bc]:active{background:0 0;box-shadow:inset -4px -4px 8px #f0f3f9,inset 4px 4px 8px #ced2db,inset -1px -1px 4px #8e8e8e}footer[data-v-22cb8fd0]{padding:2rem;text-align:center;display:flex;align-items:center;justify-content:center}footer span[data-v-22cb8fd0]{display:inline-block;line-height:1;text-transform:uppercase;letter-spacing:-.025rem;font-size:.75rem;font-weight:500}footer a svg[data-v-22cb8fd0]{width:3rem;height:3rem;margin:0 .5rem}footer a .nuxt-icon[data-v-22cb8fd0]{width:2.5rem;height:2.5rem;margin:0 .25rem}</style>
  </head>
  <body>
    <div data-server-rendered="true" id="__nuxt"><div id="__layout"><div><header data-v-27046cca><a href="/nuxtstop/" class="logo-wrapper nuxt-link-active" data-v-27046cca><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-v-27046cca><path d="M13.5599 8.54348L12.8055 9.87164L10.2257 5.3282L2.306 19.274H7.66815C7.66815 20.0075 8.25298 20.6021 8.97441 20.6021H2.306C1.83937 20.6021 1.40822 20.3489 1.17494 19.9379C0.941664 19.527 0.941687 19.0208 1.175 18.6099L9.09469 4.66412C9.32802 4.25316 9.75926 4 10.226 4C10.6926 4 11.1239 4.25316 11.3572 4.66412L13.5599 8.54348V8.54348Z" fill="#00C58E" data-v-27046cca></path><path d="M19.2769 18.6099L14.3143 9.87165L13.5599 8.54348L12.8055 9.87165L7.84343 18.6099C7.61011 19.0208 7.61009 19.527 7.84337 19.9379C8.07665 20.3489 8.50779 20.6021 8.97443 20.6021H18.1443C18.611 20.6021 19.0424 20.3491 19.2758 19.9382C19.5092 19.5272 19.5092 19.0209 19.2758 18.6099H19.2769ZM8.97443 19.274L13.5599 11.1998L18.1443 19.274H8.97443H8.97443Z" fill="#2F495E" data-v-27046cca></path><path d="M22.825 19.938C22.5917 20.3489 22.1606 20.6021 21.694 20.6021H18.1443C18.8657 20.6021 19.4505 20.0075 19.4505 19.274H21.6913L15.3331 8.07696L14.3142 9.87164L13.5599 8.54348L14.2021 7.41287C14.4354 7.00192 14.8667 6.74875 15.3334 6.74875C15.8001 6.74875 16.2313 7.00192 16.4646 7.41287L22.825 18.6099C23.0583 19.0208 23.0583 19.5271 22.825 19.938V19.938Z" fill="#108775" data-v-27046cca></path></svg> <div class="name-wrapper" data-v-27046cca><span class="app-name" data-v-27046cca>Nuxtstop</span> <p class="subtitle" data-v-27046cca>For all things nuxt.js</p></div></a> <nav data-v-27046cca><ul data-v-27046cca><li data-v-27046cca><a href="/nuxtstop/" class="nuxt-link-active" data-v-27046cca>
          New
        </a></li><li data-v-27046cca><a href="/nuxtstop/top" data-v-27046cca>
          Top
        </a></li></ul></nav></header> <div class="page-wrapper" data-v-10d06ee8><div class="article-content-wrapper" data-v-10d06ee8><article data-fetch-key="data-v-70afb46a:0" class="article-block" data-v-70afb46a data-v-10d06ee8><header data-v-70afb46a><h1 data-v-70afb46a>Python Web Scraping Tutorial: Step-By-Step</h1> <div class="tags" data-v-70afb46a><a href="/nuxtstop/t/python" class="tag" data-v-70afb46a>
          #python
        </a><a href="/nuxtstop/t/webscraping" class="tag" data-v-70afb46a>
          #webscraping
        </a><a href="/nuxtstop/t/oxylabs" class="tag" data-v-70afb46a>
          #oxylabs
        </a><a href="/nuxtstop/t/tutorial" class="tag" data-v-70afb46a>
          #tutorial
        </a></div> <div class="image-wrapper" data-v-70afb46a><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ShKqM35E--/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/lincortsuzjm1dd985dj.jpeg" alt="Python Web Scraping Tutorial: Step-By-Step" data-v-70afb46a></div> <div class="meta" data-v-70afb46a><div class="scl" data-v-70afb46a><span data-v-70afb46a><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-v-70afb46a data-v-70afb46a><path d="M16.4444 3C14.6733 3 13.0333 3.94162 12 5.34C10.9667 3.94162 9.32667 3 7.55556 3C4.49222 3 2 5.52338 2 8.625C2 14.8024 11.0267 20.586 11.4122 20.829C11.5922 20.9426 11.7956 21 12 21C12.2044 21 12.4078 20.9426 12.5878 20.829C12.9733 20.586 22 14.8024 22 8.625C22 5.52338 19.5078 3 16.4444 3Z" fill="#FF0000" data-v-70afb46a data-v-70afb46a></path></svg>
            5
          </span> <span class="comments" data-v-70afb46a><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-v-70afb46a data-v-70afb46a><path d="M6.11765 22H4.94118L5.64706 21.05C6.11765 20.3969 6.41176 19.5656 6.58824 18.5563C3.64706 17.1906 2 14.6375 2 11.3125C2 6.20625 5.82353 3 12 3C18.1765 3 22 6.20625 22 11.3125C22 16.5375 18.2353 19.625 12 19.625H11.5882C10.6471 20.7531 9 22 6.11765 22ZM12 4.1875C6.47059 4.1875 3.17647 6.85937 3.17647 11.3125C3.17647 15.1125 5.47059 16.8938 7.41177 17.6656L7.82353 17.8437L7.76471 18.3187C7.64706 19.2687 7.47059 20.1 7.11765 20.8125C9.05882 20.575 10.1765 19.5656 10.8235 18.7344L11 18.4969H12C19.9412 18.4969 20.8235 13.5094 20.8235 11.3719C20.8235 6.85938 17.5294 4.1875 12 4.1875Z" fill="black" data-v-70afb46a data-v-70afb46a></path></svg>
            4
          </span></div> <time data-v-70afb46a>May 18</time></div></header> <div class="content" data-v-70afb46a><p>Getting started in web scraping is simple except when it isn’t, which is probably why you are here. But don’t worry – we’re always ready to help!</p>

<p>This time, check out our step-by-step Python Web Scraping video tutorial on Youtube: <br>
<iframe width="710" height="399" src="https://www.youtube.com/embed/mDveiNIpqyw" allowfullscreen loading="lazy">
</iframe>
</p>

<p>Or read the article below!</p>

<h2>
  <a name="intro" href="#intro">
  </a>
  Intro
</h2>

<p>Python is one of the easiest ways to get started as it is an object-oriented language. Python’s classes and objects are significantly easier to use than in any other language. Additionally, many libraries exist that make building a tool for web scraping in Python an absolute breeze.</p>

<p>In this web scraping Python tutorial, we will outline everything needed to get started with a simple application. It will acquire text-based data from page sources, store it into a file and sort the output according to set parameters. We will also include options for more advanced features when using Python. By following our extensive tutorial, you will be able to understand how to do <a href="https://oxylabs.io/products/scraper-api/web">web scraping</a>.</p>

<p><strong>First, what do we call web scraping?</strong></p>

<blockquote>
<p>Web scraping is an automated process of gathering public data. A web page scraper automatically extracts large amounts of public data from target websites in seconds.</p>
</blockquote>

<p><strong>Note:</strong> This Python web scraping tutorial will work for all operating systems. There will be slight differences when installing either Python or development environments but not in anything else.</p>

<h2>
  <a name="building-a-web-scraper-python-prepwork" href="#building-a-web-scraper-python-prepwork">
  </a>
  Building a web scraper: Python prepwork
</h2>

<p>Throughout this entire web scraping tutorial, <a href="https://www.python.org/downloads/">Python 3.4+ version will be used</a>. Specifically, we used 3.8.3 but any 3.4+ version should work just fine.</p>

<p>For Windows installations, when installing Python make sure to check “PATH installation”. PATH installation adds executables to the default Windows Command Prompt executable search. Windows will then recognize commands like “pip” or “python” without requiring users to point it to the directory of the executable (e.g. C:/tools/python/…/python.exe). If you have already installed Python but did not mark the checkbox, just rerun the installation and select modify. On the second screen select “Add to environment variables”.</p>

<h2>
  <a name="getting-to-the-libraries" href="#getting-to-the-libraries">
  </a>
  Getting to the libraries
</h2>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--07Ov_O0---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/wl21tyirmqdkv0224euj.jpg" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--07Ov_O0---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/wl21tyirmqdkv0224euj.jpg" alt="python libraries used for web scraping" loading="lazy" width="880" height="474"></a></p>

<p>One of the Python advantages is a large selection of libraries for web scraping. These web scraping libraries are part of thousands of Python projects in existence – on <a href="https://pypi.org/">PyPI</a> alone, there are over 300,000 projects today. Notably, there are several types of </p>

<p>Python web scraping libraries from which you can choose:</p>

<ul>
<li>Requests</li>
<li>Beautiful Soup</li>
<li>lxml</li>
<li>Selenium</li>
</ul>

<h2>
  <a name="requests-library" href="#requests-library">
  </a>
  Requests library
</h2>

<p>Web scraping starts with sending HTTP requests, such as POST or GET, to a website’s server, which returns a response containing the needed data. However, standard Python HTTP libraries are difficult to use and, for effectiveness, require bulky lines of code, further compounding an already problematic issue.</p>

<p>Unlike other HTTP libraries, the Requests library simplifies the process of making such requests by reducing the lines of code, in effect making the code easier to understand and debug without impacting its effectiveness. The library can be installed from within the terminal using the pip command:</p>

<p><code>pip install requests<br>
</code><br>
Requests library provides easy methods for sending HTTP GET and POSTrequests. For example, the function to send an HTTP Get request is aptly named get():</p>

<p><code>import requests<br>
response = requests.get("https://oxylabs.io/”)<br>
print(response.text)</code></p>

<p>If there is a need for a form to be posted, it can be done easily using the post() method. The form data can sent as a dictionary as follows:</p>

<p><code>form_data = {'key1': 'value1', 'key2': 'value2'}<br>
response = requests.post("https://oxylabs.io/ ", data=form_data)<br>
print(response.text)</code></p>

<p>Requests library also makes it very easy to use proxies that require authentication.</p>

<p><code>proxies={'http': 'http://user:password@proxy.oxylabs.io'}<br>
response = requests.get('http://httpbin.org/ip', proxies=proxies)<br>
print(response.text)</code></p>

<p>But this library has a limitation in that it does not parse the extracted HTML data, i.e., it cannot convert the data into a more readable format for analysis. Also, it cannot be used to scrape websites that are written using purely JavaScript.</p>

<h2>
  <a name="beautiful-soup" href="#beautiful-soup">
  </a>
  Beautiful Soup
</h2>

<p>Beautiful Soup is a Python library that works with a parser to extract data from HTML and can turn even invalid markup into a parse tree. However, this library is only designed for parsing and cannot request data from web servers in the form of HTML documents/files. For this reason, it is mostly used alongside the Python Requests Library. Note that Beautiful Soup makes it easy to query and navigate the HTML, but still requires a parser. The following example demonstrates the use of the html.parser module, which is part of the Python Standard Library.</p>

<p><strong>#Part 1</strong> – Get the HTML using Requests</p>

<p><code>import requests<br>
url='https://oxylabs.io/blog'<br>
response = requests.get(url)</code></p>

<p><strong>#Part 2</strong> – Find the element</p>

<p><code>from bs4 import BeautifulSoup<br>
soup = BeautifulSoup(response.text, 'html.parser')<br>
print(soup.title)</code></p>

<p>This will print the title element as follows:</p>

<p><code>&lt;h1 class="blog-header">Oxylabs Blog&lt;/h1></code></p>

<p>Due to its simple ways of navigating, searching and modifying the parse tree, Beautiful Soup is ideal even for beginners and usually saves developers hours of work. For example, to print all the blog titles from this page, the findAll()method can be used. On this page, all the blog titles are in h2 elements with class attribute set to blog-card__content-title. This information can be supplied to the findAll method as follows:</p>

<p><code>blog_titles = soup.findAll('h2', attrs={"class":"blog-card__content-title"})<br>
for title in blog_titles:<br>
    print(title.text)</code><br>
<code># Output:</code><br>
<code># Prints all blog tiles on the page</code></p>

<p>BeautifulSoup also makes it easy to work with CSS selectors. If you know a CSS selector, there is no need to learn find() or find_all() methods. The following is the same example, but uses CSS selectors:</p>

<p><code>blog_titles = soup.select('h2.blog-card__content-title')<br>
for title in blog_titles:<br>
    print(title.text)</code></p>

<p>While broken-HTML parsing is one of the main features of this library, it also offers numerous functions, including the fact that it can detect page encoding further increasing the accuracy of the data extracted from the HTML file.</p>

<p>Moreover, it can be easily configured, with just a few lines of code, to extract any custom publicly available data or to identify specific data types. </p>

<h2>
  <a name="lxml" href="#lxml">
  </a>
  lxml
</h2>

<p>lxml is a parsing library. It is a fast, powerful, and easy-to-use library that works with both HTML and XML files. Additionally, lxml is ideal when extracting data from large datasets. However, unlike Beautiful Soup, this library is impacted by poorly designed HTML, making its parsing capabilities impeded.</p>

<p>The lxml library can be installed from the terminal using the pip command:</p>

<p><code>pip install lxml</code></p>

<p>This library contains a module html to work with HTML. However, the lxml library needs the HTML string first. This HTML string can be retrieved using the Requests library as discussed in the previous section. Once the HTML is available, the tree can be built using the fromstring method as follows:</p>

<p><code># After response = requests.get() <br>
from lxml import html<br>
tree = html.fromstring(response.text)</code></p>

<p>This tree object can now be queried using XPath. Continuing the example discussed in the previous section, to get the title of the blogs, the XPath would be as follows:</p>

<p><code>//h2[@class="blog-card__content-title"]/text()</code></p>

<p>This XPath can be given to the tree.xpath() function. This will return all the elements matching this XPath. Notice the text() function in the XPath. This will extract the text within the h2 elements.</p>

<p><code>blog_titles = tree.xpath('//h2[@class="blog-card__content-title"]/text()')<br>
for title in blog_titles:<br>
    print(title)</code></p>

<p>Suppose you are looking to learn how to use this library and integrate it into your web scraping efforts or even gain more knowledge on top of your existing expertise. In that case, our detailed <a href="https://oxylabs.io/blog/lxml-tutorial">lxml tutorial</a> is an excellent place to start.</p>

<h2>
  <a name="selenium" href="#selenium">
  </a>
  Selenium
</h2>

<p>As we already said, some websites are written using JavaScript, a language that allows developers to populate fields and menus dynamically. This creates a problem for Python libraries that can only extract data from static web pages. In fact, the Requests library is not an option when it comes to JavaScript. This is where Selenium web scraping comes in and thrives.</p>

<p>This Python web library is an open-source browser automation tool (web driver) that allows you to automate processes such as logging into a social media platform. Selenium is widely used for the execution of test cases or test scripts on web applications. Its strength during web scraping derives from its ability to initiate rendering web pages, just like any browser, by running JavaScript – standard web crawlers cannot run this programming language. Yet, it is now extensively used by developers.</p>

<p>Selenium requires three components:</p>

<ul>
<li>Web Browser – Supported browsers are Chrome, Edge, Firefox and Safari</li>
<li>Driver for the browser – <a href="https://pypi.org/project/selenium/">See this page</a> for links to the drivers</li>
<li>The selenium package</li>
</ul>

<p>The selenium package can be installed from the terminal:</p>

<p><code>pip install selenium<br>
</code><br>
After installation, you’re ready to import the appropriate class for the browser. Once imported, the object of the class will have to be created. Note that this will require the path of the driver executable. Example for the Chrome browser as follows:</p>

<p><code>from selenium.webdriver import Chrome<br>
driver = Chrome(executable_path='/path/to/driver')</code></p>

<p>Now any page can be loaded in the browser using the get() method.</p>

<p><code>driver.get('https://oxylabs.io/blog')<br>
</code><br>
Selenium allows use of CSS selectors and XPath to extract elements. The following example prints all the blog titles using CSS selectors:</p>

<p><code>blog_titles = driver.get_elements_by_css_selector(' h2.blog-card__content-title')<br>
for title in blog_tiles:<br>
    print(title.text)<br>
driver.quit() # closing the browser</code></p>

<p>Basically, by running JavaScript, Selenium deals with any content being displayed dynamically and subsequently makes the webpage’s content available for parsing by built-in methods or even Beautiful Soup. Moreover, it can mimic human behavior.</p>

<p>The only downside to using Selenium in web scraping is that it slows the process because it must first execute the JavaScript code for each page before making it available for parsing. As a result, it is unideal for large-scale data extraction. But if you wish to extract data at a lower-scale or the lack of speed is not a drawback, Selenium is a great choice.</p>

<h2>
  <a name="web-scraping-python-libraries-compared" href="#web-scraping-python-libraries-compared">
  </a>
  Web scraping Python libraries compared
</h2>

<div class="table-wrapper-paragraph"><table><tbody>
<tr>
<td></td>
<td><strong>Requests</strong></td>
<td><strong>Beautiful Soup</strong></td>
<td><strong>lxml</strong></td>
<td><strong>Selenium</strong></td>
</tr>
<tr>
<td><strong>Purpose</strong></td>
<td>Simplify making HTTP requests</td>
<td>Parsing</td>
<td>Parsing</td>
<td>Simplify making HTTP requests</td>
</tr>
<tr>
<td><strong>Ease-of-use</strong></td>
<td>High</td>
<td>High</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td><strong>Speed</strong></td>
<td>Fast</td>
<td>Fast</td>
<td>Very fast</td>
<td>Slow</td>
</tr>
<tr>
<td><strong>Learning Curve</strong></td>
<td>Very easy (beginner-friendly)</td>
<td>Very easy (beginner-friendly)</td>
<td>Easy</td>
<td>Easy</td>
</tr>
<tr>
<td><strong>Documentation</strong></td>
<td>Excellent</td>
<td>Excellent</td>
<td>Good</td>
<td>Good</td>
</tr>
<tr>
<td><strong>JavaScript Support</strong></td>
<td>None</td>
<td>None</td>
<td>None</td>
<td>Yes</td>
</tr>
<tr>
<td><strong>CPU and Memory Usage</strong></td>
<td>Low</td>
<td>Low</td>
<td>Low</td>
<td>High</td>
</tr>
<tr>
<td><strong>Size of Web Scraping Project Supported</strong></td>
<td>Large and small</td>
<td>Large and small</td>
<td>Large and small</td>
<td>Small</td>
</tr>
</tbody></table></div>

<p>For this Python web scraping tutorial, we’ll be using three important libraries – BeautifulSoup v4, Pandas, and Selenium. In further steps, we assume a successful installation of these libraries. If you receive a “NameError: name * is not defined” it is likely that one of these installations has failed.</p>

<h2>
  <a name="webdrivers-and-browsers" href="#webdrivers-and-browsers">
  </a>
  WebDrivers and browsers
</h2>

<p>Every web scraper uses a browser as it needs to connect to the destination URL. For testing purposes we highly recommend using a regular browser (or not a headless one), especially for newcomers. Seeing how written code interacts with the application allows simple troubleshooting and debugging, and grants a better understanding of the entire process.</p>

<p>Headless browsers can be used later on as they are more efficient for complex tasks. Throughout this tutorial we will be using the Chrome web browser although the entire process is almost identical with Firefox.</p>

<p>To get started, use your preferred search engine to find the “webdriver for Chrome” (or Firefox). Take note of your browser’s current version. Download the webdriver that matches your browser’s version.</p>

<p>If applicable, select the requisite package, download and unzip it. Copy the driver’s executable file to any easily accessible directory. Whether everything was done correctly, we will only be able to find out later on.</p>

<h2>
  <a name="finding-a-cozy-place-for-our-python-web-scraper" href="#finding-a-cozy-place-for-our-python-web-scraper">
  </a>
  Finding a cozy place for our Python web scraper
</h2>

<p>One final step needs to be taken before we can get to the programming part of this web scraping tutorial: using a good coding environment. There are many options, from a simple text editor, with which simply creating a *.py file and writing the code down directly is enough, to a fully-featured IDE (Integrated Development Environment).</p>

<p>If you already have Visual Studio Code installed, picking this IDE would be the simplest option. Otherwise, I’d highly recommend <a href="https://www.jetbrains.com/pycharm/">PyCharm</a> for any newcomer as it has very little barrier to entry and an intuitive UI. We will assume that PyCharm is used for the rest of the web scraping tutorial.</p>

<p>In PyCharm, right click on the project area and “New -> Python File”. Give it a nice name!</p>

<h2>
  <a name="importing-and-using-libraries" href="#importing-and-using-libraries">
  </a>
  Importing and using libraries
</h2>

<p>Time to put all those pips we installed previously to use:</p>

<p><code>import pandas as pd</code></p>

<p><code>from bs4 import BeautifulSoup</code></p>

<p><code>from selenium import webdriver<br>
</code><br>
PyCharm might display these imports in grey as it automatically marks unused libraries. Don’t accept its suggestion to remove unused libs (at least yet).</p>

<p>We should begin by defining our browser. Depending on the webdriver we picked back in “WebDriver and browsers” we should type in:</p>

<p><code>driver = webdriver.Chrome(executable_path='c:\path\to\windows\webdriver\executable.exe')</code></p>

<p>OR</p>

<p><code>driver = webdriver.Firefox(executable_path='/nix/path/to/webdriver/executable')</code></p>

<h2>
  <a name="picking-a-url" href="#picking-a-url">
  </a>
  Picking a URL
</h2>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--AFQmxShD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dh79gkmk0zf1r3mxid1u.jpg" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--AFQmxShD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dh79gkmk0zf1r3mxid1u.jpg" alt="picking a URL to scrape with python" loading="lazy" width="880" height="474"></a></p>

<p>Before performing our first test run, choose a URL. As this web scraping tutorial is intended to create an elementary application, we highly recommended picking a simple target URL:</p>

<ul>
<li>Avoid data hidden in Javascript elements. These sometimes need to be triggered by performing specific actions in order to display the required data. Scraping data from Javascript elements requires more sophisticated use of Python and its logic.</li>
<li>Avoid image scraping. Images can be downloaded directly with Selenium.</li>
<li>Before conducting any scraping activities ensure that you are scraping public data, and are in no way breaching third-party rights. Also, don’t forget to check the robots.txt file for guidance.</li>
</ul>

<p>Select the landing page you want to visit and input the URL into the driver.get(‘URL’) parameter. Selenium requires that the connection protocol is provided. As such, it is always necessary to attach “http://” or “https://” to the URL.</p>

<p><code>driver.get('https://your.url/here?yes=brilliant')<br>
</code><br>
Try doing a test run by clicking the green arrow at the bottom left or by right clicking the coding environment and selecting ‘Run’.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--2WBOoNFh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/v4vtqwhg6t1tco34cjps.jpg" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--2WBOoNFh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/v4vtqwhg6t1tco34cjps.jpg" alt="Python web scraping example" loading="lazy" width="880" height="476"></a></p>

<p>If you receive an error message stating that a file is missing then turn double check if the path provided in the driver “webdriver.*” matches the location of the webdriver executable. If you receive a message that there is a version mismatch redownload the correct webdriver executable.</p>

<h2>
  <a name="defining-objects-and-building-lists" href="#defining-objects-and-building-lists">
  </a>
  Defining objects and building lists
</h2>

<p>Python allows coders to design objects without assigning an exact type. An object can be created by simply typing its title and assigning a value.</p>

<p><code># Object is “results”, brackets make the object an empty list.</code><br>
<code># We will be storing our data here.<br>
results = []</code></p>

<p>Lists in Python are ordered, mutable and allow duplicate members. Other collections, such as sets or dictionaries, can be used but lists are the easiest to use. Time to make more objects!</p>

<p><code># Add the page source to the variable</code>content<code>.<br>
content = driver.page_source</code><br>
<code># Load the contents of the page, its source, into BeautifulSoup</code><br>
<code># class, which analyzes the HTML as a nested data structure and allows to select</code><br>
<code># its elements by using various selectors.<br>
soup = BeautifulSoup(content)</code></p>

<p>Before we go on with, let’s recap on how our code should look so far:</p>

<p><code>import pandas as pd<br>
from bs4 import BeautifulSoup<br>
from selenium import webdriver<br>
driver = webdriver.Chrome(executable_path='/nix/path/to/webdriver/executable')<br>
driver.get('https://your.url/here?yes=brilliant')<br>
results = []<br>
content = driver.page_source<br>
soup = BeautifulSoup(content)</code></p>

<p>Try rerunning the application again. There should be no errors displayed. If any arise, a few possible troubleshooting options were outlined in earlier chapters.</p>

<h2>
  <a name="extracting-data-with-our-python-web-scraper" href="#extracting-data-with-our-python-web-scraper">
  </a>
  Extracting data with our Python web scraper
</h2>

<p>We have finally arrived at the fun and difficult part – extracting data out of the HTML file. Since in almost all cases we are taking small sections out of many different parts of the page and we want to store it into a list, we should process every smaller section and then add it to the list:</p>

<p><code># Loop over all elements returned by the 'findAll' call. It has the filter 'attrs' given</code><br>
<code># to it in order to limit the data returned to those elements with a given class only.<br>
for element in soup.findAll(attrs={'class': 'list-item'}):<br>
    ...</code></p>

<p>“soup.findAll” accepts a wide array of arguments. For the purposes of this tutorial we only use “attrs” (attributes). It allows us to narrow down the search by setting up a statement “if attribute is equal to X is true then…”. Classes are easy to find and use therefore we shall use those.</p>

<p>Let’s visit the chosen URL in a real browser before continuing. Open the page source by using CTRL+U (Chrome) or right click and select “View Page Source”. Find the “closest” class where the data is nested. Another option is to press F12 to open DevTools to select Element Picker. For example, it could be nested as:</p>

<p><code>&lt;h4 class="title"><br>
    &lt;a href="...">This is a Title&lt;/a><br>
&lt;/h4></code></p>

<p>Our attribute, “class”, would then be “title”. If you picked a simple target, in most cases data will be nested in a similar way to the example above. Complex targets might require more effort to get the data out. Let’s get back to coding and add the class we found in the source:</p>

<p><code># Change ‘list-item’ to ‘title’.<br>
for element in soup.findAll(attrs={'class': 'title'}):<br>
  ...</code></p>

<p>Our loop will now go through all objects with the class “title” in the page source. We will process each of them:</p>

<p><code>name = element.find('a')<br>
</code><br>
Let’s take a look at how our loop goes through the HTML:</p>

<p><code>&lt;h4 class="title"><br>
    &lt;a href="...">This is a Title&lt;/a><br>
&lt;/h4></code></p>

<p>Our first statement (in the loop itself) finds all elements that match tags, whose “class” attribute contains “title”. We then execute another search within that class. Our next search finds all the <code>&lt;a></code> tags in the document (<code>&lt;a></code> is included while partial matches like <code>&lt;span></code> are not). Finally, the object is assigned to the variable “name”.</p>

<p>We could then assign the object name to our previously created list array “results” but doing this would bring the entire <code>&lt;a href…></code> tag with the text inside it into one element. In most cases, we would only need the text itself without any additional tags.</p>

<p><code># Add the object of “name” to the list “results”.</code><br>
<code># '&lt;element>.text' extracts the text in the element, omitting the HTML tags.<br>
results.append(name.text)</code></p>

<p>Our loop will go through the entire page source, find all the occurrences of the classes listed above, then append the nested data to our list:</p>

<p><code>import pandas as pd<br>
from bs4 import BeautifulSoup<br>
from selenium import webdriver<br>
driver = webdriver.Chrome(executable_path='/nix/path/to/webdriver/executable')<br>
driver.get('https://your.url/here?yes=brilliant')<br>
results = []<br>
content = driver.page_source<br>
soup = BeautifulSoup(content)<br>
for element in soup.findAll(attrs={'class': 'title'}):<br>
    name = element.find('a')<br>
    results.append(name.text)</code></p>

<p>Note that the two statements after the loop are indented. Loops require indentation to denote nesting. Any consistent indentation will be considered legal. Loops without indentation will output an “IndentationError” with the offending statement pointed out with the “arrow”.</p>

<h2>
  <a name="exporting-the-data" href="#exporting-the-data">
  </a>
  Exporting the data
</h2>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--ohb_PL2U--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kdp2alxmnkf68nhe20ql.jpg" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ohb_PL2U--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kdp2alxmnkf68nhe20ql.jpg" alt="exporting python scraped data" loading="lazy" width="880" height="474"></a></p>

<p>Even if no syntax or runtime errors appear when running our program, there still might be semantic errors. You should check whether we actually get the data assigned to the right object and move to the array correctly.</p>

<p>One of the simplest ways to check if the data you acquired during the previous steps is being collected correctly is to use “print”. Since arrays have many different values, a simple loop is often used to separate each entry to a separate line in the output:</p>

<p><code>for x in results:<br>
   print(x)</code></p>

<p>Both “print” and “for” should be self-explanatory at this point. We are only initiating this loop for quick testing and debugging purposes. It is completely viable to print the results directly:</p>

<p><code>print(results)</code></p>

<p>So far our code should look like this:</p>

<p><code>driver = webdriver.Chrome(executable_path='/nix/path/to/webdriver/executable')<br>
driver.get('https://your.url/here?yes=brilliant')<br>
results = []<br>
content = driver.page_source<br>
soup = BeautifulSoup(content)<br>
for a in soup.findAll(attrs={'class': 'class'}):<br>
    name = a.find('a')<br>
    if name not in results:<br>
        results.append(name.text)<br>
for x in results:<br>
    print(x)</code></p>

<p>Running our program now should display no errors and display acquired data in the debugger window. While “print” is great for testing purposes, it isn’t all that great for parsing and analyzing data.</p>

<p>You might have noticed that “import pandas” is still greyed out so far. We will finally get to put the library to good use. I recommend removing the “print” loop for now as we will be doing something similar but moving our data to a csv file.</p>

<p><code>df = pd.DataFrame({'Names': results})<br>
df.to_csv('names.csv', index=False, encoding='utf-8')</code></p>

<p>Our two new statements rely on the pandas library. Our first statement creates a variable “df” and turns its object into a two-dimensional data table. “Names” is the name of our column while “results” is our list to be printed out. Note that pandas can create multiple columns, we just don’t have enough lists to utilize those parameters (yet).</p>

<p>Our second statement moves the data of variable “df” to a specific file type (in this case “csv”). Our first parameter assigns a name to our soon-to-be file and an extension. Adding an extension is necessary as “pandas” will otherwise output a file without one and it will have to be changed manually. “index” can be used to assign specific starting numbers to columns. “encoding” is used to save data in a specific format. UTF-8 will be enough in almost all cases.</p>

<p><code>import pandas as pd<br>
from bs4 import BeautifulSoup<br>
from selenium import webdriver<br>
driver = webdriver.Chrome(executable_path='/nix/path/to/webdriver/executable')<br>
driver.get('https://your.url/here?yes=brilliant')<br>
results = []<br>
content = driver.page_source<br>
soup = BeautifulSoup(content)<br>
for a in soup.findAll(attrs={'class': 'class'}):<br>
    name = a.find('a')<br>
    if name not in results:<br>
        results.append(name.text)<br>
df = pd.DataFrame({'Names': results})<br>
df.to_csv('names.csv', index=False, encoding='utf-8')</code></p>

<p>No imports should now be greyed out and running our application should output a “names.csv” into our project directory. Note that a “Guessed At Parser” warning remains. We could remove it by installing a third party parser but for the purposes of this Python web scraping tutorial the default HTML option will do just fine.</p>

<h2>
  <a name="more-lists-more" href="#more-lists-more">
  </a>
  More lists. More!
</h2>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--4ghbY36a--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/055ixtk22rjoezgq6yq8.jpg" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--4ghbY36a--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/055ixtk22rjoezgq6yq8.jpg" alt="Python web scraping tutorial" loading="lazy" width="880" height="474"></a></p>

<p>Many web scraping operations will need to acquire several sets of data. For example, extracting just the titles of items listed on an e-commerce website will rarely be useful. In order to gather meaningful information and to draw conclusions from it at least two data points are needed.</p>

<p>For the purposes of this tutorial, we will try something slightly different. Since acquiring data from the same class would just mean appending to an additional list, we should attempt to extract data from a different class but, at the same time, maintain the structure of our table.</p>

<p>Obviously, we will need another list to store our data in.</p>

<p><code>import pandas as pd<br>
from bs4 import BeautifulSoup<br>
from selenium import webdriver<br>
driver =</code><code>webdriver.Chrome(executable_path='/nix/path/to/webdriver/executable')<br>
driver.get('https://your.url/here?yes=brilliant')</code><br>
<code>results = []<br>
other_results = []<br>
for b in soup.findAll(attrs={'class': 'otherclass'}):</code><br>
<code># Assume that data is nested in ‘span’.<br>
    name2 = b.find('span')<br>
    other_results.append(name.text)</code></p>

<p>Since we will be extracting an additional data point from a different part of the HTML, we will need an additional loop. If needed we can also add another “if” conditional to control for duplicate entries:</p>

<p>Finally, we need to change how our data table is formed:</p>

<p><code>df = pd.DataFrame({'Names': results, 'Categories': other_results})</code></p>

<p>So far the newest iteration of our code should look something like this:</p>

<p><code>import pandas as pd<br>
from bs4 import BeautifulSoup<br>
from selenium import webdriver<br>
driver = webdriver.Chrome(executable_path='/nix/path/to/webdriver/executable')<br>
driver.get('https://your.url/here?yes=brilliant')<br>
results = []<br>
other_results = []<br>
content = driver.page_source<br>
for a in soup.findAll(attrs={'class': 'class'}):<br>
    name = a.find('a'﻿)<br>
    if name not in results:<br>
        results.append(name.text)<br>
for b in soup.findAll(attrs={'class': 'otherclass'}):<br>
    name2 = b.find('span')<br>
    other_results.append(name.text)<br>
df = pd.DataFrame({'Names': results, 'Categories': other_results})<br>
df.to_csv('names.csv', index=False, encoding='utf-8')</code></p>

<p>If you are lucky, running this code will output no error. In some cases “pandas” will output an “ValueError: arrays must all be the same length” message. Simply put, the length of the lists “results” and “other_results” is unequal, therefore pandas cannot create a two-dimensional table.</p>

<p>There are dozens of ways to resolve that error message. From padding the shortest list with “empty” values, to creating dictionaries, to creating two series and listing them out. We shall do the third option:</p>

<p><code>series1 = pd.Series(results, name = 'Names')<br>
series2 = pd.Series(other_results, name = 'Categories')<br>
df = pd.DataFrame({'Names': series1, 'Categories': series2})<br>
df.to_csv('names.csv', index=False, encoding='utf-8')</code></p>

<p>Note that data will not be matched as the lists are of uneven length but creating two series is the easiest fix if two data points are needed. Our final code should look something like this:</p>

<p><code>import pandas as pd<br>
from bs4 import BeautifulSoup<br>
from selenium import webdriver<br>
driver = webdriver.Chrome(executable_path='/nix/path/to/webdriver/executable')<br>
driver.get('https://your.url/here?yes=brilliant')<br>
results = []<br>
other_results = []<br>
content = driver.page_source<br>
soup = BeautifulSoup(content)<br>
for a in soup.findAll(attrs={'class': 'class'}):<br>
    name = a.find('a')<br>
    if name not in results:<br>
        results.append(name.text)<br>
for b in soup.findAll(attrs={'class': 'otherclass'}):<br>
    name2 = b.find('span')<br>
    other_results.append(name.text)<br>
series1 = pd.Series(results, name = 'Names')<br>
series2 = pd.Series(other_results, name = 'Categories')<br>
df = pd.DataFrame({'Names': series1, 'Categories': series2})<br>
df.to_csv('names.csv', index=False, encoding='utf-8')</code></p>

<p>Running it should create a csv file named “names” with two columns of data.</p>

<h2>
  <a name="web-scraping-with-python-best-practices" href="#web-scraping-with-python-best-practices">
  </a>
  Web scraping with Python best practices
</h2>

<p>Our first web scraper should now be fully functional. Of course it is so basic and simplistic that performing any serious data acquisition would require significant upgrades. Before moving on to greener pastures, I highly recommend experimenting with some additional features:</p>

<ul>
<li>Create matched data extraction by creating a loop that would make lists of an even length.</li>
<li>Scrape several URLs in one go. There are many ways to implement such a feature. One of the simplest options is to simply repeat the code above and change URLs each time. That would be quite boring. Build a loop and an array of URLs to visit.</li>
<li>Another option is to create several arrays to store different sets of data and output it into one file with different rows. Scraping several different types of information at once is an important part of e-commerce data acquisition.</li>
<li>Once a satisfactory web scraper is running, you no longer need to watch the browser perform its actions. Get headless versions of either Chrome or Firefox browsers and use those to reduce load times.</li>
<li>Create a scraping pattern. Think of how a regular user would browse the internet and try to automate their actions. New libraries will definitely be needed. Use “import time” and “from random import randint” to create wait times between pages. Add “scrollto()” or use specific key inputs to move around the browser. It’s nearly impossible to list all of the possible options when it comes to creating a scraping pattern.</li>
<li>Create a monitoring process. Data on certain websites might be time (or even user) sensitive. Try creating a long-lasting loop that rechecks certain URLs and scrapes data at set intervals. Ensure that your acquired data is always fresh.</li>
<li>Make use of the <a href="https://oxylabs.io/blog/python-requests">Python Requests</a> library. Requests is a powerful asset in any web scraping toolkit as it allows to optimize HTTP methods sent to servers.</li>
<li>Finally, integrate proxies into your web scraper. Using location specific request sources allows you to acquire data that might otherwise be inaccessible.</li>
</ul>

<h2>
  <a name="conclusion" href="#conclusion">
  </a>
  Conclusion
</h2>

<p>So, in this extensive Python tutorial, we outlined every step you need to complete to get started with a simple application. </p>

<p>But, from here onwards, you are on your own. Building web scrapers in Python, acquiring data and drawing conclusions from large amounts of information is inherently an interesting and complicated process.</p>

<p>We hope this tutorial was valuable for you and encourage you to stay tuned for more informative posts from Oxylabs!</p>

</div></article> <div class="aside-username-wrapper" data-v-10d06ee8><aside class="aside-username-block" data-v-37984f8c data-v-10d06ee8><div class="username-heading loading" data-v-37984f8c><div class="vue-content-placeholders vue-content-placeholders-is-animated" data-v-37984f8c><div class="vue-content-placeholders-heading" data-v-37984f8c><div class="vue-content-placeholders-heading__img"></div> <div class="vue-content-placeholders-heading__content"><div class="vue-content-placeholders-heading__title"></div> <div class="vue-content-placeholders-heading__subtitle"></div></div></div></div></div> <div class="info" data-v-37984f8c><div class="vue-content-placeholders vue-content-placeholders-is-animated" data-v-37984f8c><div class="vue-content-placeholders-text" data-v-37984f8c><div class="vue-content-placeholders-text__line"></div><div class="vue-content-placeholders-text__line"></div><div class="vue-content-placeholders-text__line"></div></div></div></div></aside></div></div> <div class="comments-block" data-v-8c4375bc data-v-10d06ee8><!----> <a href="https://dev.to/oxylabs/python-web-scraping-tutorial-step-by-step-507h" target="_blank" rel="nofollow noopener noreferer" class="add-comment" data-v-8c4375bc>
    Add comment
  </a></div></div> <footer data-v-22cb8fd0><span data-v-22cb8fd0>Built with</span> <a href="https://nuxtjs.org" target="_blank" data-v-22cb8fd0><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" class="nuxt-icon" data-v-22cb8fd0 data-v-22cb8fd0><path d="M13.5599 8.54348L12.8055 9.87164L10.2257 5.3282L2.306 19.274H7.66815C7.66815 20.0075 8.25298 20.6021 8.97441 20.6021H2.306C1.83937 20.6021 1.40822 20.3489 1.17494 19.9379C0.941664 19.527 0.941687 19.0208 1.175 18.6099L9.09469 4.66412C9.32802 4.25316 9.75926 4 10.226 4C10.6926 4 11.1239 4.25316 11.3572 4.66412L13.5599 8.54348V8.54348Z" fill="#00C58E" data-v-22cb8fd0 data-v-22cb8fd0></path><path d="M19.2769 18.6099L14.3143 9.87165L13.5599 8.54348L12.8055 9.87165L7.84343 18.6099C7.61011 19.0208 7.61009 19.527 7.84337 19.9379C8.07665 20.3489 8.50779 20.6021 8.97443 20.6021H18.1443C18.611 20.6021 19.0424 20.3491 19.2758 19.9382C19.5092 19.5272 19.5092 19.0209 19.2758 18.6099H19.2769ZM8.97443 19.274L13.5599 11.1998L18.1443 19.274H8.97443H8.97443Z" fill="#2F495E" data-v-22cb8fd0 data-v-22cb8fd0></path><path d="M22.825 19.938C22.5917 20.3489 22.1606 20.6021 21.694 20.6021H18.1443C18.8657 20.6021 19.4505 20.0075 19.4505 19.274H21.6913L15.3331 8.07696L14.3142 9.87164L13.5599 8.54348L14.2021 7.41287C14.4354 7.00192 14.8667 6.74875 15.3334 6.74875C15.8001 6.74875 16.2313 7.00192 16.4646 7.41287L22.825 18.6099C23.0583 19.0208 23.0583 19.5271 22.825 19.938V19.938Z" fill="#108775" data-v-22cb8fd0 data-v-22cb8fd0></path></svg></a> <span data-v-22cb8fd0>&</span> <a href="https://docs.dev.to/api" rel="nofollow noopener" target="_blank" data-v-22cb8fd0><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-v-22cb8fd0 data-v-22cb8fd0><path d="M1.5726 5.13748C1.42945 5.20622 1.2411 5.36661 1.15822 5.48117C1 5.69503 1 5.74849 1 11.8739C1 17.9993 1 18.0528 1.15822 18.2667C1.2411 18.3812 1.42945 18.5416 1.5726 18.6104C1.8137 18.7402 2.46164 18.7478 12 18.7478C21.5384 18.7478 22.1863 18.7402 22.4274 18.6104C22.5706 18.5416 22.7589 18.3812 22.8418 18.2667C23 18.0528 23 17.9993 23 11.8739C23 5.74849 23 5.69503 22.8418 5.48117C22.7589 5.36661 22.5706 5.20622 22.4274 5.13748C22.1863 5.00764 21.5384 5 12 5C2.46164 5 1.8137 5.00764 1.5726 5.13748ZM7.7055 8.2613C8.0822 8.45989 8.59454 9.0098 8.77536 9.40694C8.89589 9.66664 8.91095 9.94922 8.91095 12.0649C8.91095 14.3104 8.90344 14.4478 8.75275 14.7839C8.51919 15.288 8.16506 15.6546 7.68288 15.899C7.26096 16.1052 7.22328 16.1128 5.7315 16.1358L4.20206 16.1663V12.1031V8.04744L5.80684 8.07035C7.27602 8.09327 7.42672 8.10854 7.7055 8.2613ZM13.6952 8.89521V9.73538H12.4521H11.2089V10.4991V11.2629H11.9623H12.7158V12.1031V12.9432H11.9623H11.2089V13.707V14.4708H12.4521H13.6952V15.3109V16.151H12C10.1315 16.151 10.0411 16.1358 9.67191 15.6928L9.47603 15.4484V12.1336C9.47603 8.46752 9.46851 8.49807 9.95069 8.20783C10.1692 8.07035 10.3425 8.05508 11.9473 8.05508H13.6952V8.89521ZM16.5658 10.3769C16.8897 11.6295 17.1685 12.6912 17.176 12.7293C17.1911 12.7675 17.4699 11.7441 17.8014 10.461C18.1254 9.17017 18.4343 8.1009 18.4795 8.08563C18.5247 8.06271 18.9541 8.06271 19.4288 8.07035L20.3028 8.09327L19.376 11.6219C18.8713 13.5542 18.4117 15.2269 18.3664 15.3261C18.0123 16.0135 17.274 16.3343 16.7164 16.0441C16.4528 15.899 16.0911 15.4865 15.9705 15.1887C15.9254 15.0665 15.4884 13.4549 15.0062 11.6142C14.524 9.76593 14.1171 8.20783 14.0945 8.15437C14.0644 8.07035 14.2301 8.05508 15.0212 8.07035L15.9856 8.09327L16.5658 10.3769Z" fill="black" data-v-22cb8fd0 data-v-22cb8fd0></path><path d="M5.93491 12.103V14.4707H6.27394C6.66574 14.4707 7.01983 14.3103 7.1404 14.0965C7.18559 14.0048 7.21575 13.2105 7.21575 12.0648V10.1783L6.99725 9.95683C6.80133 9.76591 6.71847 9.73535 6.35683 9.73535H5.93491V12.103Z" fill="black" data-v-22cb8fd0 data-v-22cb8fd0></path></svg></a></footer></div></div></div><script>window.__NUXT__=function(e,t,n,a,r,i){return n.type_of="article",n.id=1087995,n.title="Python Web Scraping Tutorial: Step-By-Step",n.description="Getting started in web scraping is simple except when it isn’t, which is probably why you are here....",n.readable_publish_date="May 18",n.slug="python-web-scraping-tutorial-step-by-step-507h",n.path="/oxylabs-io/python-web-scraping-tutorial-step-by-step-507h",n.url="https://dev.to/oxylabs-io/python-web-scraping-tutorial-step-by-step-507h",n.comments_count=4,n.public_reactions_count=5,n.collection_id=e,n.published_timestamp=a,n.positive_reactions_count=5,n.cover_image="https://res.cloudinary.com/practicaldev/image/fetch/s--ShKqM35E--/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/lincortsuzjm1dd985dj.jpeg",n.social_image="https://res.cloudinary.com/practicaldev/image/fetch/s--Y60dD7B0--/c_imagga_scale,f_auto,fl_progressive,h_500,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/lincortsuzjm1dd985dj.jpeg",n.canonical_url="https://oxylabs.io/blog/python-web-scraping",n.created_at="2022-05-17T13:40:36Z",n.edited_at=e,n.crossposted_at=e,n.published_at=a,n.last_comment_at="2022-05-23T18:08:38Z",n.reading_time_minutes=19,n.tag_list="python, webscraping, oxylabs, tutorial",n.tags=["python","webscraping",t,"tutorial"],n.body_html='<p>Getting started in web scraping is simple except when it isn’t, which is probably why you are here. But don’t worry – we’re always ready to help!</p>\n\n<p>This time, check out our step-by-step Python Web Scraping video tutorial on Youtube: <br>\n<iframe width="710" height="399" src="https://www.youtube.com/embed/mDveiNIpqyw" allowfullscreen loading="lazy">\n</iframe>\n</p>\n\n<p>Or read the article below!</p>\n\n<h2>\n  <a name="intro" href="#intro">\n  </a>\n  Intro\n</h2>\n\n<p>Python is one of the easiest ways to get started as it is an object-oriented language. Python’s classes and objects are significantly easier to use than in any other language. Additionally, many libraries exist that make building a tool for web scraping in Python an absolute breeze.</p>\n\n<p>In this web scraping Python tutorial, we will outline everything needed to get started with a simple application. It will acquire text-based data from page sources, store it into a file and sort the output according to set parameters. We will also include options for more advanced features when using Python. By following our extensive tutorial, you will be able to understand how to do <a href="https://oxylabs.io/products/scraper-api/web">web scraping</a>.</p>\n\n<p><strong>First, what do we call web scraping?</strong></p>\n\n<blockquote>\n<p>Web scraping is an automated process of gathering public data. A web page scraper automatically extracts large amounts of public data from target websites in seconds.</p>\n</blockquote>\n\n<p><strong>Note:</strong> This Python web scraping tutorial will work for all operating systems. There will be slight differences when installing either Python or development environments but not in anything else.</p>\n\n<h2>\n  <a name="building-a-web-scraper-python-prepwork" href="#building-a-web-scraper-python-prepwork">\n  </a>\n  Building a web scraper: Python prepwork\n</h2>\n\n<p>Throughout this entire web scraping tutorial, <a href="https://www.python.org/downloads/">Python 3.4+ version will be used</a>. Specifically, we used 3.8.3 but any 3.4+ version should work just fine.</p>\n\n<p>For Windows installations, when installing Python make sure to check “PATH installation”. PATH installation adds executables to the default Windows Command Prompt executable search. Windows will then recognize commands like “pip” or “python” without requiring users to point it to the directory of the executable (e.g. C:/tools/python/…/python.exe). If you have already installed Python but did not mark the checkbox, just rerun the installation and select modify. On the second screen select “Add to environment variables”.</p>\n\n<h2>\n  <a name="getting-to-the-libraries" href="#getting-to-the-libraries">\n  </a>\n  Getting to the libraries\n</h2>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--07Ov_O0---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/wl21tyirmqdkv0224euj.jpg" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--07Ov_O0---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/wl21tyirmqdkv0224euj.jpg" alt="python libraries used for web scraping" loading="lazy" width="880" height="474"></a></p>\n\n<p>One of the Python advantages is a large selection of libraries for web scraping. These web scraping libraries are part of thousands of Python projects in existence – on <a href="https://pypi.org/">PyPI</a> alone, there are over 300,000 projects today. Notably, there are several types of </p>\n\n<p>Python web scraping libraries from which you can choose:</p>\n\n<ul>\n<li>Requests</li>\n<li>Beautiful Soup</li>\n<li>lxml</li>\n<li>Selenium</li>\n</ul>\n\n<h2>\n  <a name="requests-library" href="#requests-library">\n  </a>\n  Requests library\n</h2>\n\n<p>Web scraping starts with sending HTTP requests, such as POST or GET, to a website’s server, which returns a response containing the needed data. However, standard Python HTTP libraries are difficult to use and, for effectiveness, require bulky lines of code, further compounding an already problematic issue.</p>\n\n<p>Unlike other HTTP libraries, the Requests library simplifies the process of making such requests by reducing the lines of code, in effect making the code easier to understand and debug without impacting its effectiveness. The library can be installed from within the terminal using the pip command:</p>\n\n<p><code>pip install requests<br>\n</code><br>\nRequests library provides easy methods for sending HTTP GET and POSTrequests. For example, the function to send an HTTP Get request is aptly named get():</p>\n\n<p><code>import requests<br>\nresponse = requests.get("https://oxylabs.io/”)<br>\nprint(response.text)</code></p>\n\n<p>If there is a need for a form to be posted, it can be done easily using the post() method. The form data can sent as a dictionary as follows:</p>\n\n<p><code>form_data = {\'key1\': \'value1\', \'key2\': \'value2\'}<br>\nresponse = requests.post("https://oxylabs.io/ ", data=form_data)<br>\nprint(response.text)</code></p>\n\n<p>Requests library also makes it very easy to use proxies that require authentication.</p>\n\n<p><code>proxies={\'http\': \'http://user:password@proxy.oxylabs.io\'}<br>\nresponse = requests.get(\'http://httpbin.org/ip\', proxies=proxies)<br>\nprint(response.text)</code></p>\n\n<p>But this library has a limitation in that it does not parse the extracted HTML data, i.e., it cannot convert the data into a more readable format for analysis. Also, it cannot be used to scrape websites that are written using purely JavaScript.</p>\n\n<h2>\n  <a name="beautiful-soup" href="#beautiful-soup">\n  </a>\n  Beautiful Soup\n</h2>\n\n<p>Beautiful Soup is a Python library that works with a parser to extract data from HTML and can turn even invalid markup into a parse tree. However, this library is only designed for parsing and cannot request data from web servers in the form of HTML documents/files. For this reason, it is mostly used alongside the Python Requests Library. Note that Beautiful Soup makes it easy to query and navigate the HTML, but still requires a parser. The following example demonstrates the use of the html.parser module, which is part of the Python Standard Library.</p>\n\n<p><strong>#Part 1</strong> – Get the HTML using Requests</p>\n\n<p><code>import requests<br>\nurl=\'https://oxylabs.io/blog\'<br>\nresponse = requests.get(url)</code></p>\n\n<p><strong>#Part 2</strong> – Find the element</p>\n\n<p><code>from bs4 import BeautifulSoup<br>\nsoup = BeautifulSoup(response.text, \'html.parser\')<br>\nprint(soup.title)</code></p>\n\n<p>This will print the title element as follows:</p>\n\n<p><code>&lt;h1 class="blog-header"&gt;Oxylabs Blog&lt;/h1&gt;</code></p>\n\n<p>Due to its simple ways of navigating, searching and modifying the parse tree, Beautiful Soup is ideal even for beginners and usually saves developers hours of work. For example, to print all the blog titles from this page, the findAll()method can be used. On this page, all the blog titles are in h2 elements with class attribute set to blog-card__content-title. This information can be supplied to the findAll method as follows:</p>\n\n<p><code>blog_titles = soup.findAll(\'h2\', attrs={"class":"blog-card__content-title"})<br>\nfor title in blog_titles:<br>\n    print(title.text)</code><br>\n<code># Output:</code><br>\n<code># Prints all blog tiles on the page</code></p>\n\n<p>BeautifulSoup also makes it easy to work with CSS selectors. If you know a CSS selector, there is no need to learn find() or find_all() methods. The following is the same example, but uses CSS selectors:</p>\n\n<p><code>blog_titles = soup.select(\'h2.blog-card__content-title\')<br>\nfor title in blog_titles:<br>\n    print(title.text)</code></p>\n\n<p>While broken-HTML parsing is one of the main features of this library, it also offers numerous functions, including the fact that it can detect page encoding further increasing the accuracy of the data extracted from the HTML file.</p>\n\n<p>Moreover, it can be easily configured, with just a few lines of code, to extract any custom publicly available data or to identify specific data types. </p>\n\n<h2>\n  <a name="lxml" href="#lxml">\n  </a>\n  lxml\n</h2>\n\n<p>lxml is a parsing library. It is a fast, powerful, and easy-to-use library that works with both HTML and XML files. Additionally, lxml is ideal when extracting data from large datasets. However, unlike Beautiful Soup, this library is impacted by poorly designed HTML, making its parsing capabilities impeded.</p>\n\n<p>The lxml library can be installed from the terminal using the pip command:</p>\n\n<p><code>pip install lxml</code></p>\n\n<p>This library contains a module html to work with HTML. However, the lxml library needs the HTML string first. This HTML string can be retrieved using the Requests library as discussed in the previous section. Once the HTML is available, the tree can be built using the fromstring method as follows:</p>\n\n<p><code># After response = requests.get() <br>\nfrom lxml import html<br>\ntree = html.fromstring(response.text)</code></p>\n\n<p>This tree object can now be queried using XPath. Continuing the example discussed in the previous section, to get the title of the blogs, the XPath would be as follows:</p>\n\n<p><code>//h2[@class="blog-card__content-title"]/text()</code></p>\n\n<p>This XPath can be given to the tree.xpath() function. This will return all the elements matching this XPath. Notice the text() function in the XPath. This will extract the text within the h2 elements.</p>\n\n<p><code>blog_titles = tree.xpath(\'//h2[@class="blog-card__content-title"]/text()\')<br>\nfor title in blog_titles:<br>\n    print(title)</code></p>\n\n<p>Suppose you are looking to learn how to use this library and integrate it into your web scraping efforts or even gain more knowledge on top of your existing expertise. In that case, our detailed <a href="https://oxylabs.io/blog/lxml-tutorial">lxml tutorial</a> is an excellent place to start.</p>\n\n<h2>\n  <a name="selenium" href="#selenium">\n  </a>\n  Selenium\n</h2>\n\n<p>As we already said, some websites are written using JavaScript, a language that allows developers to populate fields and menus dynamically. This creates a problem for Python libraries that can only extract data from static web pages. In fact, the Requests library is not an option when it comes to JavaScript. This is where Selenium web scraping comes in and thrives.</p>\n\n<p>This Python web library is an open-source browser automation tool (web driver) that allows you to automate processes such as logging into a social media platform. Selenium is widely used for the execution of test cases or test scripts on web applications. Its strength during web scraping derives from its ability to initiate rendering web pages, just like any browser, by running JavaScript – standard web crawlers cannot run this programming language. Yet, it is now extensively used by developers.</p>\n\n<p>Selenium requires three components:</p>\n\n<ul>\n<li>Web Browser – Supported browsers are Chrome, Edge, Firefox and Safari</li>\n<li>Driver for the browser – <a href="https://pypi.org/project/selenium/">See this page</a> for links to the drivers</li>\n<li>The selenium package</li>\n</ul>\n\n<p>The selenium package can be installed from the terminal:</p>\n\n<p><code>pip install selenium<br>\n</code><br>\nAfter installation, you’re ready to import the appropriate class for the browser. Once imported, the object of the class will have to be created. Note that this will require the path of the driver executable. Example for the Chrome browser as follows:</p>\n\n<p><code>from selenium.webdriver import Chrome<br>\ndriver = Chrome(executable_path=\'/path/to/driver\')</code></p>\n\n<p>Now any page can be loaded in the browser using the get() method.</p>\n\n<p><code>driver.get(\'https://oxylabs.io/blog\')<br>\n</code><br>\nSelenium allows use of CSS selectors and XPath to extract elements. The following example prints all the blog titles using CSS selectors:</p>\n\n<p><code>blog_titles = driver.get_elements_by_css_selector(\' h2.blog-card__content-title\')<br>\nfor title in blog_tiles:<br>\n    print(title.text)<br>\ndriver.quit() # closing the browser</code></p>\n\n<p>Basically, by running JavaScript, Selenium deals with any content being displayed dynamically and subsequently makes the webpage’s content available for parsing by built-in methods or even Beautiful Soup. Moreover, it can mimic human behavior.</p>\n\n<p>The only downside to using Selenium in web scraping is that it slows the process because it must first execute the JavaScript code for each page before making it available for parsing. As a result, it is unideal for large-scale data extraction. But if you wish to extract data at a lower-scale or the lack of speed is not a drawback, Selenium is a great choice.</p>\n\n<h2>\n  <a name="web-scraping-python-libraries-compared" href="#web-scraping-python-libraries-compared">\n  </a>\n  Web scraping Python libraries compared\n</h2>\n\n<div class="table-wrapper-paragraph"><table><tbody>\n<tr>\n<td></td>\n<td><strong>Requests</strong></td>\n<td><strong>Beautiful Soup</strong></td>\n<td><strong>lxml</strong></td>\n<td><strong>Selenium</strong></td>\n</tr>\n<tr>\n<td><strong>Purpose</strong></td>\n<td>Simplify making HTTP requests</td>\n<td>Parsing</td>\n<td>Parsing</td>\n<td>Simplify making HTTP requests</td>\n</tr>\n<tr>\n<td><strong>Ease-of-use</strong></td>\n<td>High</td>\n<td>High</td>\n<td>Medium</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td><strong>Speed</strong></td>\n<td>Fast</td>\n<td>Fast</td>\n<td>Very fast</td>\n<td>Slow</td>\n</tr>\n<tr>\n<td><strong>Learning Curve</strong></td>\n<td>Very easy (beginner-friendly)</td>\n<td>Very easy (beginner-friendly)</td>\n<td>Easy</td>\n<td>Easy</td>\n</tr>\n<tr>\n<td><strong>Documentation</strong></td>\n<td>Excellent</td>\n<td>Excellent</td>\n<td>Good</td>\n<td>Good</td>\n</tr>\n<tr>\n<td><strong>JavaScript Support</strong></td>\n<td>None</td>\n<td>None</td>\n<td>None</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td><strong>CPU and Memory Usage</strong></td>\n<td>Low</td>\n<td>Low</td>\n<td>Low</td>\n<td>High</td>\n</tr>\n<tr>\n<td><strong>Size of Web Scraping Project Supported</strong></td>\n<td>Large and small</td>\n<td>Large and small</td>\n<td>Large and small</td>\n<td>Small</td>\n</tr>\n</tbody></table></div>\n\n<p>For this Python web scraping tutorial, we’ll be using three important libraries – BeautifulSoup v4, Pandas, and Selenium. In further steps, we assume a successful installation of these libraries. If you receive a “NameError: name * is not defined” it is likely that one of these installations has failed.</p>\n\n<h2>\n  <a name="webdrivers-and-browsers" href="#webdrivers-and-browsers">\n  </a>\n  WebDrivers and browsers\n</h2>\n\n<p>Every web scraper uses a browser as it needs to connect to the destination URL. For testing purposes we highly recommend using a regular browser (or not a headless one), especially for newcomers. Seeing how written code interacts with the application allows simple troubleshooting and debugging, and grants a better understanding of the entire process.</p>\n\n<p>Headless browsers can be used later on as they are more efficient for complex tasks. Throughout this tutorial we will be using the Chrome web browser although the entire process is almost identical with Firefox.</p>\n\n<p>To get started, use your preferred search engine to find the “webdriver for Chrome” (or Firefox). Take note of your browser’s current version. Download the webdriver that matches your browser’s version.</p>\n\n<p>If applicable, select the requisite package, download and unzip it. Copy the driver’s executable file to any easily accessible directory. Whether everything was done correctly, we will only be able to find out later on.</p>\n\n<h2>\n  <a name="finding-a-cozy-place-for-our-python-web-scraper" href="#finding-a-cozy-place-for-our-python-web-scraper">\n  </a>\n  Finding a cozy place for our Python web scraper\n</h2>\n\n<p>One final step needs to be taken before we can get to the programming part of this web scraping tutorial: using a good coding environment. There are many options, from a simple text editor, with which simply creating a *.py file and writing the code down directly is enough, to a fully-featured IDE (Integrated Development Environment).</p>\n\n<p>If you already have Visual Studio Code installed, picking this IDE would be the simplest option. Otherwise, I’d highly recommend <a href="https://www.jetbrains.com/pycharm/">PyCharm</a> for any newcomer as it has very little barrier to entry and an intuitive UI. We will assume that PyCharm is used for the rest of the web scraping tutorial.</p>\n\n<p>In PyCharm, right click on the project area and “New -&gt; Python File”. Give it a nice name!</p>\n\n<h2>\n  <a name="importing-and-using-libraries" href="#importing-and-using-libraries">\n  </a>\n  Importing and using libraries\n</h2>\n\n<p>Time to put all those pips we installed previously to use:</p>\n\n<p><code>import pandas as pd</code></p>\n\n<p><code>from bs4 import BeautifulSoup</code></p>\n\n<p><code>from selenium import webdriver<br>\n</code><br>\nPyCharm might display these imports in grey as it automatically marks unused libraries. Don’t accept its suggestion to remove unused libs (at least yet).</p>\n\n<p>We should begin by defining our browser. Depending on the webdriver we picked back in “WebDriver and browsers” we should type in:</p>\n\n<p><code>driver = webdriver.Chrome(executable_path=\'c:\\path\\to\\windows\\webdriver\\executable.exe\')</code></p>\n\n<p>OR</p>\n\n<p><code>driver = webdriver.Firefox(executable_path=\'/nix/path/to/webdriver/executable\')</code></p>\n\n<h2>\n  <a name="picking-a-url" href="#picking-a-url">\n  </a>\n  Picking a URL\n</h2>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--AFQmxShD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dh79gkmk0zf1r3mxid1u.jpg" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--AFQmxShD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dh79gkmk0zf1r3mxid1u.jpg" alt="picking a URL to scrape with python" loading="lazy" width="880" height="474"></a></p>\n\n<p>Before performing our first test run, choose a URL. As this web scraping tutorial is intended to create an elementary application, we highly recommended picking a simple target URL:</p>\n\n<ul>\n<li>Avoid data hidden in Javascript elements. These sometimes need to be triggered by performing specific actions in order to display the required data. Scraping data from Javascript elements requires more sophisticated use of Python and its logic.</li>\n<li>Avoid image scraping. Images can be downloaded directly with Selenium.</li>\n<li>Before conducting any scraping activities ensure that you are scraping public data, and are in no way breaching third-party rights. Also, don’t forget to check the robots.txt file for guidance.</li>\n</ul>\n\n<p>Select the landing page you want to visit and input the URL into the driver.get(‘URL’) parameter. Selenium requires that the connection protocol is provided. As such, it is always necessary to attach “http://” or “https://” to the URL.</p>\n\n<p><code>driver.get(\'https://your.url/here?yes=brilliant\')<br>\n</code><br>\nTry doing a test run by clicking the green arrow at the bottom left or by right clicking the coding environment and selecting ‘Run’.</p>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--2WBOoNFh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/v4vtqwhg6t1tco34cjps.jpg" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--2WBOoNFh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/v4vtqwhg6t1tco34cjps.jpg" alt="Python web scraping example" loading="lazy" width="880" height="476"></a></p>\n\n<p>If you receive an error message stating that a file is missing then turn double check if the path provided in the driver “webdriver.*” matches the location of the webdriver executable. If you receive a message that there is a version mismatch redownload the correct webdriver executable.</p>\n\n<h2>\n  <a name="defining-objects-and-building-lists" href="#defining-objects-and-building-lists">\n  </a>\n  Defining objects and building lists\n</h2>\n\n<p>Python allows coders to design objects without assigning an exact type. An object can be created by simply typing its title and assigning a value.</p>\n\n<p><code># Object is “results”, brackets make the object an empty list.</code><br>\n<code># We will be storing our data here.<br>\nresults = []</code></p>\n\n<p>Lists in Python are ordered, mutable and allow duplicate members. Other collections, such as sets or dictionaries, can be used but lists are the easiest to use. Time to make more objects!</p>\n\n<p><code># Add the page source to the variable</code>content<code>.<br>\ncontent = driver.page_source</code><br>\n<code># Load the contents of the page, its source, into BeautifulSoup</code><br>\n<code># class, which analyzes the HTML as a nested data structure and allows to select</code><br>\n<code># its elements by using various selectors.<br>\nsoup = BeautifulSoup(content)</code></p>\n\n<p>Before we go on with, let’s recap on how our code should look so far:</p>\n\n<p><code>import pandas as pd<br>\nfrom bs4 import BeautifulSoup<br>\nfrom selenium import webdriver<br>\ndriver = webdriver.Chrome(executable_path=\'/nix/path/to/webdriver/executable\')<br>\ndriver.get(\'https://your.url/here?yes=brilliant\')<br>\nresults = []<br>\ncontent = driver.page_source<br>\nsoup = BeautifulSoup(content)</code></p>\n\n<p>Try rerunning the application again. There should be no errors displayed. If any arise, a few possible troubleshooting options were outlined in earlier chapters.</p>\n\n<h2>\n  <a name="extracting-data-with-our-python-web-scraper" href="#extracting-data-with-our-python-web-scraper">\n  </a>\n  Extracting data with our Python web scraper\n</h2>\n\n<p>We have finally arrived at the fun and difficult part – extracting data out of the HTML file. Since in almost all cases we are taking small sections out of many different parts of the page and we want to store it into a list, we should process every smaller section and then add it to the list:</p>\n\n<p><code># Loop over all elements returned by the \'findAll\' call. It has the filter \'attrs\' given</code><br>\n<code># to it in order to limit the data returned to those elements with a given class only.<br>\nfor element in soup.findAll(attrs={\'class\': \'list-item\'}):<br>\n    ...</code></p>\n\n<p>“soup.findAll” accepts a wide array of arguments. For the purposes of this tutorial we only use “attrs” (attributes). It allows us to narrow down the search by setting up a statement “if attribute is equal to X is true then…”. Classes are easy to find and use therefore we shall use those.</p>\n\n<p>Let’s visit the chosen URL in a real browser before continuing. Open the page source by using CTRL+U (Chrome) or right click and select “View Page Source”. Find the “closest” class where the data is nested. Another option is to press F12 to open DevTools to select Element Picker. For example, it could be nested as:</p>\n\n<p><code>&lt;h4 class="title"&gt;<br>\n    &lt;a href="..."&gt;This is a Title&lt;/a&gt;<br>\n&lt;/h4&gt;</code></p>\n\n<p>Our attribute, “class”, would then be “title”. If you picked a simple target, in most cases data will be nested in a similar way to the example above. Complex targets might require more effort to get the data out. Let’s get back to coding and add the class we found in the source:</p>\n\n<p><code># Change ‘list-item’ to ‘title’.<br>\nfor element in soup.findAll(attrs={\'class\': \'title\'}):<br>\n  ...</code></p>\n\n<p>Our loop will now go through all objects with the class “title” in the page source. We will process each of them:</p>\n\n<p><code>name = element.find(\'a\')<br>\n</code><br>\nLet’s take a look at how our loop goes through the HTML:</p>\n\n<p><code>&lt;h4 class="title"&gt;<br>\n    &lt;a href="..."&gt;This is a Title&lt;/a&gt;<br>\n&lt;/h4&gt;</code></p>\n\n<p>Our first statement (in the loop itself) finds all elements that match tags, whose “class” attribute contains “title”. We then execute another search within that class. Our next search finds all the <code>&lt;a&gt;</code> tags in the document (<code>&lt;a&gt;</code> is included while partial matches like <code>&lt;span&gt;</code> are not). Finally, the object is assigned to the variable “name”.</p>\n\n<p>We could then assign the object name to our previously created list array “results” but doing this would bring the entire <code>&lt;a href…&gt;</code> tag with the text inside it into one element. In most cases, we would only need the text itself without any additional tags.</p>\n\n<p><code># Add the object of “name” to the list “results”.</code><br>\n<code># \'&lt;element&gt;.text\' extracts the text in the element, omitting the HTML tags.<br>\nresults.append(name.text)</code></p>\n\n<p>Our loop will go through the entire page source, find all the occurrences of the classes listed above, then append the nested data to our list:</p>\n\n<p><code>import pandas as pd<br>\nfrom bs4 import BeautifulSoup<br>\nfrom selenium import webdriver<br>\ndriver = webdriver.Chrome(executable_path=\'/nix/path/to/webdriver/executable\')<br>\ndriver.get(\'https://your.url/here?yes=brilliant\')<br>\nresults = []<br>\ncontent = driver.page_source<br>\nsoup = BeautifulSoup(content)<br>\nfor element in soup.findAll(attrs={\'class\': \'title\'}):<br>\n    name = element.find(\'a\')<br>\n    results.append(name.text)</code></p>\n\n<p>Note that the two statements after the loop are indented. Loops require indentation to denote nesting. Any consistent indentation will be considered legal. Loops without indentation will output an “IndentationError” with the offending statement pointed out with the “arrow”.</p>\n\n<h2>\n  <a name="exporting-the-data" href="#exporting-the-data">\n  </a>\n  Exporting the data\n</h2>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--ohb_PL2U--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kdp2alxmnkf68nhe20ql.jpg" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ohb_PL2U--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kdp2alxmnkf68nhe20ql.jpg" alt="exporting python scraped data" loading="lazy" width="880" height="474"></a></p>\n\n<p>Even if no syntax or runtime errors appear when running our program, there still might be semantic errors. You should check whether we actually get the data assigned to the right object and move to the array correctly.</p>\n\n<p>One of the simplest ways to check if the data you acquired during the previous steps is being collected correctly is to use “print”. Since arrays have many different values, a simple loop is often used to separate each entry to a separate line in the output:</p>\n\n<p><code>for x in results:<br>\n   print(x)</code></p>\n\n<p>Both “print” and “for” should be self-explanatory at this point. We are only initiating this loop for quick testing and debugging purposes. It is completely viable to print the results directly:</p>\n\n<p><code>print(results)</code></p>\n\n<p>So far our code should look like this:</p>\n\n<p><code>driver = webdriver.Chrome(executable_path=\'/nix/path/to/webdriver/executable\')<br>\ndriver.get(\'https://your.url/here?yes=brilliant\')<br>\nresults = []<br>\ncontent = driver.page_source<br>\nsoup = BeautifulSoup(content)<br>\nfor a in soup.findAll(attrs={\'class\': \'class\'}):<br>\n    name = a.find(\'a\')<br>\n    if name not in results:<br>\n        results.append(name.text)<br>\nfor x in results:<br>\n    print(x)</code></p>\n\n<p>Running our program now should display no errors and display acquired data in the debugger window. While “print” is great for testing purposes, it isn’t all that great for parsing and analyzing data.</p>\n\n<p>You might have noticed that “import pandas” is still greyed out so far. We will finally get to put the library to good use. I recommend removing the “print” loop for now as we will be doing something similar but moving our data to a csv file.</p>\n\n<p><code>df = pd.DataFrame({\'Names\': results})<br>\ndf.to_csv(\'names.csv\', index=False, encoding=\'utf-8\')</code></p>\n\n<p>Our two new statements rely on the pandas library. Our first statement creates a variable “df” and turns its object into a two-dimensional data table. “Names” is the name of our column while “results” is our list to be printed out. Note that pandas can create multiple columns, we just don’t have enough lists to utilize those parameters (yet).</p>\n\n<p>Our second statement moves the data of variable “df” to a specific file type (in this case “csv”). Our first parameter assigns a name to our soon-to-be file and an extension. Adding an extension is necessary as “pandas” will otherwise output a file without one and it will have to be changed manually. “index” can be used to assign specific starting numbers to columns. “encoding” is used to save data in a specific format. UTF-8 will be enough in almost all cases.</p>\n\n<p><code>import pandas as pd<br>\nfrom bs4 import BeautifulSoup<br>\nfrom selenium import webdriver<br>\ndriver = webdriver.Chrome(executable_path=\'/nix/path/to/webdriver/executable\')<br>\ndriver.get(\'https://your.url/here?yes=brilliant\')<br>\nresults = []<br>\ncontent = driver.page_source<br>\nsoup = BeautifulSoup(content)<br>\nfor a in soup.findAll(attrs={\'class\': \'class\'}):<br>\n    name = a.find(\'a\')<br>\n    if name not in results:<br>\n        results.append(name.text)<br>\ndf = pd.DataFrame({\'Names\': results})<br>\ndf.to_csv(\'names.csv\', index=False, encoding=\'utf-8\')</code></p>\n\n<p>No imports should now be greyed out and running our application should output a “names.csv” into our project directory. Note that a “Guessed At Parser” warning remains. We could remove it by installing a third party parser but for the purposes of this Python web scraping tutorial the default HTML option will do just fine.</p>\n\n<h2>\n  <a name="more-lists-more" href="#more-lists-more">\n  </a>\n  More lists. More!\n</h2>\n\n<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--4ghbY36a--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/055ixtk22rjoezgq6yq8.jpg" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--4ghbY36a--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/055ixtk22rjoezgq6yq8.jpg" alt="Python web scraping tutorial" loading="lazy" width="880" height="474"></a></p>\n\n<p>Many web scraping operations will need to acquire several sets of data. For example, extracting just the titles of items listed on an e-commerce website will rarely be useful. In order to gather meaningful information and to draw conclusions from it at least two data points are needed.</p>\n\n<p>For the purposes of this tutorial, we will try something slightly different. Since acquiring data from the same class would just mean appending to an additional list, we should attempt to extract data from a different class but, at the same time, maintain the structure of our table.</p>\n\n<p>Obviously, we will need another list to store our data in.</p>\n\n<p><code>import pandas as pd<br>\nfrom bs4 import BeautifulSoup<br>\nfrom selenium import webdriver<br>\ndriver =</code><code>webdriver.Chrome(executable_path=\'/nix/path/to/webdriver/executable\')<br>\ndriver.get(\'https://your.url/here?yes=brilliant\')</code><br>\n<code>results = []<br>\nother_results = []<br>\nfor b in soup.findAll(attrs={\'class\': \'otherclass\'}):</code><br>\n<code># Assume that data is nested in ‘span’.<br>\n    name2 = b.find(\'span\')<br>\n    other_results.append(name.text)</code></p>\n\n<p>Since we will be extracting an additional data point from a different part of the HTML, we will need an additional loop. If needed we can also add another “if” conditional to control for duplicate entries:</p>\n\n<p>Finally, we need to change how our data table is formed:</p>\n\n<p><code>df = pd.DataFrame({\'Names\': results, \'Categories\': other_results})</code></p>\n\n<p>So far the newest iteration of our code should look something like this:</p>\n\n<p><code>import pandas as pd<br>\nfrom bs4 import BeautifulSoup<br>\nfrom selenium import webdriver<br>\ndriver = webdriver.Chrome(executable_path=\'/nix/path/to/webdriver/executable\')<br>\ndriver.get(\'https://your.url/here?yes=brilliant\')<br>\nresults = []<br>\nother_results = []<br>\ncontent = driver.page_source<br>\nfor a in soup.findAll(attrs={\'class\': \'class\'}):<br>\n    name = a.find(\'a\'\ufeff)<br>\n    if name not in results:<br>\n        results.append(name.text)<br>\nfor b in soup.findAll(attrs={\'class\': \'otherclass\'}):<br>\n    name2 = b.find(\'span\')<br>\n    other_results.append(name.text)<br>\ndf = pd.DataFrame({\'Names\': results, \'Categories\': other_results})<br>\ndf.to_csv(\'names.csv\', index=False, encoding=\'utf-8\')</code></p>\n\n<p>If you are lucky, running this code will output no error. In some cases “pandas” will output an “ValueError: arrays must all be the same length” message. Simply put, the length of the lists “results” and “other_results” is unequal, therefore pandas cannot create a two-dimensional table.</p>\n\n<p>There are dozens of ways to resolve that error message. From padding the shortest list with “empty” values, to creating dictionaries, to creating two series and listing them out. We shall do the third option:</p>\n\n<p><code>series1 = pd.Series(results, name = \'Names\')<br>\nseries2 = pd.Series(other_results, name = \'Categories\')<br>\ndf = pd.DataFrame({\'Names\': series1, \'Categories\': series2})<br>\ndf.to_csv(\'names.csv\', index=False, encoding=\'utf-8\')</code></p>\n\n<p>Note that data will not be matched as the lists are of uneven length but creating two series is the easiest fix if two data points are needed. Our final code should look something like this:</p>\n\n<p><code>import pandas as pd<br>\nfrom bs4 import BeautifulSoup<br>\nfrom selenium import webdriver<br>\ndriver = webdriver.Chrome(executable_path=\'/nix/path/to/webdriver/executable\')<br>\ndriver.get(\'https://your.url/here?yes=brilliant\')<br>\nresults = []<br>\nother_results = []<br>\ncontent = driver.page_source<br>\nsoup = BeautifulSoup(content)<br>\nfor a in soup.findAll(attrs={\'class\': \'class\'}):<br>\n    name = a.find(\'a\')<br>\n    if name not in results:<br>\n        results.append(name.text)<br>\nfor b in soup.findAll(attrs={\'class\': \'otherclass\'}):<br>\n    name2 = b.find(\'span\')<br>\n    other_results.append(name.text)<br>\nseries1 = pd.Series(results, name = \'Names\')<br>\nseries2 = pd.Series(other_results, name = \'Categories\')<br>\ndf = pd.DataFrame({\'Names\': series1, \'Categories\': series2})<br>\ndf.to_csv(\'names.csv\', index=False, encoding=\'utf-8\')</code></p>\n\n<p>Running it should create a csv file named “names” with two columns of data.</p>\n\n<h2>\n  <a name="web-scraping-with-python-best-practices" href="#web-scraping-with-python-best-practices">\n  </a>\n  Web scraping with Python best practices\n</h2>\n\n<p>Our first web scraper should now be fully functional. Of course it is so basic and simplistic that performing any serious data acquisition would require significant upgrades. Before moving on to greener pastures, I highly recommend experimenting with some additional features:</p>\n\n<ul>\n<li>Create matched data extraction by creating a loop that would make lists of an even length.</li>\n<li>Scrape several URLs in one go. There are many ways to implement such a feature. One of the simplest options is to simply repeat the code above and change URLs each time. That would be quite boring. Build a loop and an array of URLs to visit.</li>\n<li>Another option is to create several arrays to store different sets of data and output it into one file with different rows. Scraping several different types of information at once is an important part of e-commerce data acquisition.</li>\n<li>Once a satisfactory web scraper is running, you no longer need to watch the browser perform its actions. Get headless versions of either Chrome or Firefox browsers and use those to reduce load times.</li>\n<li>Create a scraping pattern. Think of how a regular user would browse the internet and try to automate their actions. New libraries will definitely be needed. Use “import time” and “from random import randint” to create wait times between pages. Add “scrollto()” or use specific key inputs to move around the browser. It’s nearly impossible to list all of the possible options when it comes to creating a scraping pattern.</li>\n<li>Create a monitoring process. Data on certain websites might be time (or even user) sensitive. Try creating a long-lasting loop that rechecks certain URLs and scrapes data at set intervals. Ensure that your acquired data is always fresh.</li>\n<li>Make use of the <a href="https://oxylabs.io/blog/python-requests">Python Requests</a> library. Requests is a powerful asset in any web scraping toolkit as it allows to optimize HTTP methods sent to servers.</li>\n<li>Finally, integrate proxies into your web scraper. Using location specific request sources allows you to acquire data that might otherwise be inaccessible.</li>\n</ul>\n\n<h2>\n  <a name="conclusion" href="#conclusion">\n  </a>\n  Conclusion\n</h2>\n\n<p>So, in this extensive Python tutorial, we outlined every step you need to complete to get started with a simple application. </p>\n\n<p>But, from here onwards, you are on your own. Building web scrapers in Python, acquiring data and drawing conclusions from large amounts of information is inherently an interesting and complicated process.</p>\n\n<p>We hope this tutorial was valuable for you and encourage you to stay tuned for more informative posts from Oxylabs!</p>\n\n',n.body_markdown="Getting started in web scraping is simple except when it isn’t, which is probably why you are here. But don’t worry – we’re always ready to help!\n\nThis time, check out our step-by-step Python Web Scraping video tutorial on Youtube: \n{% embed https://www.youtube.com/watch?v=mDveiNIpqyw %}\n\nOr read the article below!\n\n## Intro\n\nPython is one of the easiest ways to get started as it is an object-oriented language. Python’s classes and objects are significantly easier to use than in any other language. Additionally, many libraries exist that make building a tool for web scraping in Python an absolute breeze.\n\nIn this web scraping Python tutorial, we will outline everything needed to get started with a simple application. It will acquire text-based data from page sources, store it into a file and sort the output according to set parameters. We will also include options for more advanced features when using Python. By following our extensive tutorial, you will be able to understand how to do [web scraping](https://oxylabs.io/products/scraper-api/web).\n\n**First, what do we call web scraping?**\n\n> Web scraping is an automated process of gathering public data. A web page scraper automatically extracts large amounts of public data from target websites in seconds.\n\n**Note:** This Python web scraping tutorial will work for all operating systems. There will be slight differences when installing either Python or development environments but not in anything else.\n\n## Building a web scraper: Python prepwork\n\nThroughout this entire web scraping tutorial, [Python 3.4+ version will be used](https://www.python.org/downloads/). Specifically, we used 3.8.3 but any 3.4+ version should work just fine.\n\nFor Windows installations, when installing Python make sure to check “PATH installation”. PATH installation adds executables to the default Windows Command Prompt executable search. Windows will then recognize commands like “pip” or “python” without requiring users to point it to the directory of the executable (e.g. C:/tools/python/…/python.exe). If you have already installed Python but did not mark the checkbox, just rerun the installation and select modify. On the second screen select “Add to environment variables”.\n\n## Getting to the libraries\n\n![python libraries used for web scraping](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/wl21tyirmqdkv0224euj.jpg)\n\nOne of the Python advantages is a large selection of libraries for web scraping. These web scraping libraries are part of thousands of Python projects in existence – on [PyPI](https://pypi.org/) alone, there are over 300,000 projects today. Notably, there are several types of \n\nPython web scraping libraries from which you can choose:\n\n- Requests\n- Beautiful Soup\n- lxml\n- Selenium\n\n## Requests library\n\nWeb scraping starts with sending HTTP requests, such as POST or GET, to a website’s server, which returns a response containing the needed data. However, standard Python HTTP libraries are difficult to use and, for effectiveness, require bulky lines of code, further compounding an already problematic issue.\n\nUnlike other HTTP libraries, the Requests library simplifies the process of making such requests by reducing the lines of code, in effect making the code easier to understand and debug without impacting its effectiveness. The library can be installed from within the terminal using the pip command:\n\n`pip install requests\n`\nRequests library provides easy methods for sending HTTP GET and POSTrequests. For example, the function to send an HTTP Get request is aptly named get():\n\n`import requests\nresponse = requests.get(\"https://oxylabs.io/”)\nprint(response.text)`\n\nIf there is a need for a form to be posted, it can be done easily using the post() method. The form data can sent as a dictionary as follows:\n\n`form_data = {'key1': 'value1', 'key2': 'value2'}\nresponse = requests.post(\"https://oxylabs.io/ \", data=form_data)\nprint(response.text)`\n\nRequests library also makes it very easy to use proxies that require authentication.\n\n`proxies={'http': 'http://user:password@proxy.oxylabs.io'}\nresponse = requests.get('http://httpbin.org/ip', proxies=proxies)\nprint(response.text)`\n\nBut this library has a limitation in that it does not parse the extracted HTML data, i.e., it cannot convert the data into a more readable format for analysis. Also, it cannot be used to scrape websites that are written using purely JavaScript.\n\n## Beautiful Soup\n\nBeautiful Soup is a Python library that works with a parser to extract data from HTML and can turn even invalid markup into a parse tree. However, this library is only designed for parsing and cannot request data from web servers in the form of HTML documents/files. For this reason, it is mostly used alongside the Python Requests Library. Note that Beautiful Soup makes it easy to query and navigate the HTML, but still requires a parser. The following example demonstrates the use of the html.parser module, which is part of the Python Standard Library.\n\n**#Part 1** – Get the HTML using Requests\n\n`import requests\nurl='https://oxylabs.io/blog'\nresponse = requests.get(url)`\n\n**#Part 2** – Find the element\n\n`from bs4 import BeautifulSoup\nsoup = BeautifulSoup(response.text, 'html.parser')\nprint(soup.title)`\n\nThis will print the title element as follows:\n\n`<h1 class=\"blog-header\">Oxylabs Blog</h1>`\n\nDue to its simple ways of navigating, searching and modifying the parse tree, Beautiful Soup is ideal even for beginners and usually saves developers hours of work. For example, to print all the blog titles from this page, the findAll()method can be used. On this page, all the blog titles are in h2 elements with class attribute set to blog-card__content-title. This information can be supplied to the findAll method as follows:\n\n`blog_titles = soup.findAll('h2', attrs={\"class\":\"blog-card__content-title\"})\nfor title in blog_titles:\n    print(title.text)`\n`# Output:`\n`# Prints all blog tiles on the page`\n\nBeautifulSoup also makes it easy to work with CSS selectors. If you know a CSS selector, there is no need to learn find() or find_all() methods. The following is the same example, but uses CSS selectors:\n\n`blog_titles = soup.select('h2.blog-card__content-title')\nfor title in blog_titles:\n    print(title.text)`\n\nWhile broken-HTML parsing is one of the main features of this library, it also offers numerous functions, including the fact that it can detect page encoding further increasing the accuracy of the data extracted from the HTML file.\n\nMoreover, it can be easily configured, with just a few lines of code, to extract any custom publicly available data or to identify specific data types. \n\n## lxml\n\nlxml is a parsing library. It is a fast, powerful, and easy-to-use library that works with both HTML and XML files. Additionally, lxml is ideal when extracting data from large datasets. However, unlike Beautiful Soup, this library is impacted by poorly designed HTML, making its parsing capabilities impeded.\n\nThe lxml library can be installed from the terminal using the pip command:\n\n`pip install lxml`\n\nThis library contains a module html to work with HTML. However, the lxml library needs the HTML string first. This HTML string can be retrieved using the Requests library as discussed in the previous section. Once the HTML is available, the tree can be built using the fromstring method as follows:\n\n`# After response = requests.get() \nfrom lxml import html\ntree = html.fromstring(response.text)`\n\nThis tree object can now be queried using XPath. Continuing the example discussed in the previous section, to get the title of the blogs, the XPath would be as follows:\n\n`//h2[@class=\"blog-card__content-title\"]/text()`\n\nThis XPath can be given to the tree.xpath() function. This will return all the elements matching this XPath. Notice the text() function in the XPath. This will extract the text within the h2 elements.\n\n`blog_titles = tree.xpath('//h2[@class=\"blog-card__content-title\"]/text()')\nfor title in blog_titles:\n    print(title)`\n\nSuppose you are looking to learn how to use this library and integrate it into your web scraping efforts or even gain more knowledge on top of your existing expertise. In that case, our detailed [lxml tutorial](https://oxylabs.io/blog/lxml-tutorial) is an excellent place to start.\n\n## Selenium\n\nAs we already said, some websites are written using JavaScript, a language that allows developers to populate fields and menus dynamically. This creates a problem for Python libraries that can only extract data from static web pages. In fact, the Requests library is not an option when it comes to JavaScript. This is where Selenium web scraping comes in and thrives.\n\nThis Python web library is an open-source browser automation tool (web driver) that allows you to automate processes such as logging into a social media platform. Selenium is widely used for the execution of test cases or test scripts on web applications. Its strength during web scraping derives from its ability to initiate rendering web pages, just like any browser, by running JavaScript – standard web crawlers cannot run this programming language. Yet, it is now extensively used by developers.\n\nSelenium requires three components:\n\n- Web Browser – Supported browsers are Chrome, Edge, Firefox and Safari\n- Driver for the browser – [See this page](https://pypi.org/project/selenium/) for links to the drivers\n- The selenium package\n\nThe selenium package can be installed from the terminal:\n\n`pip install selenium\n`\nAfter installation, you’re ready to import the appropriate class for the browser. Once imported, the object of the class will have to be created. Note that this will require the path of the driver executable. Example for the Chrome browser as follows:\n\n`from selenium.webdriver import Chrome\ndriver = Chrome(executable_path='/path/to/driver')`\n\nNow any page can be loaded in the browser using the get() method.\n\n`driver.get('https://oxylabs.io/blog')\n`\nSelenium allows use of CSS selectors and XPath to extract elements. The following example prints all the blog titles using CSS selectors:\n\n`blog_titles = driver.get_elements_by_css_selector(' h2.blog-card__content-title')\nfor title in blog_tiles:\n    print(title.text)\ndriver.quit() # closing the browser`\n\nBasically, by running JavaScript, Selenium deals with any content being displayed dynamically and subsequently makes the webpage’s content available for parsing by built-in methods or even Beautiful Soup. Moreover, it can mimic human behavior.\n\nThe only downside to using Selenium in web scraping is that it slows the process because it must first execute the JavaScript code for each page before making it available for parsing. As a result, it is unideal for large-scale data extraction. But if you wish to extract data at a lower-scale or the lack of speed is not a drawback, Selenium is a great choice.\n\n## Web scraping Python libraries compared\n<figure class=\"wp-block-table\"><table><tbody><tr><td></td><td><strong>Requests</strong></td><td><strong>Beautiful Soup</strong></td><td><strong>lxml</strong></td><td><strong>Selenium</strong></td></tr><tr><td><strong>Purpose</strong></td><td>Simplify making HTTP requests</td><td>Parsing</td><td>Parsing</td><td>Simplify making HTTP requests</td></tr><tr><td><strong>Ease-of-use</strong></td><td>High</td><td>High</td><td>Medium</td><td>Medium</td></tr><tr><td><strong>Speed</strong></td><td>Fast</td><td>Fast</td><td>Very fast</td><td>Slow</td></tr><tr><td><strong>Learning Curve</strong></td><td>Very easy (beginner-friendly)</td><td>Very easy (beginner-friendly)</td><td>Easy</td><td>Easy</td></tr><tr><td><strong>Documentation</strong></td><td>Excellent</td><td>Excellent</td><td>Good</td><td>Good</td></tr><tr><td><strong>JavaScript Support</strong></td><td>None</td><td>None</td><td>None</td><td>Yes</td></tr><tr><td><strong>CPU and Memory Usage</strong></td><td>Low</td><td>Low</td><td>Low</td><td>High</td></tr><tr><td><strong>Size of Web Scraping Project Supported</strong></td><td>Large and small</td><td>Large and small</td><td>Large and small</td><td>Small</td></tr></tbody></table></figure>\n\nFor this Python web scraping tutorial, we’ll be using three important libraries – BeautifulSoup v4, Pandas, and Selenium. In further steps, we assume a successful installation of these libraries. If you receive a “NameError: name * is not defined” it is likely that one of these installations has failed.\n\n## WebDrivers and browsers\n\nEvery web scraper uses a browser as it needs to connect to the destination URL. For testing purposes we highly recommend using a regular browser (or not a headless one), especially for newcomers. Seeing how written code interacts with the application allows simple troubleshooting and debugging, and grants a better understanding of the entire process.\n\nHeadless browsers can be used later on as they are more efficient for complex tasks. Throughout this tutorial we will be using the Chrome web browser although the entire process is almost identical with Firefox.\n\nTo get started, use your preferred search engine to find the “webdriver for Chrome” (or Firefox). Take note of your browser’s current version. Download the webdriver that matches your browser’s version.\n\nIf applicable, select the requisite package, download and unzip it. Copy the driver’s executable file to any easily accessible directory. Whether everything was done correctly, we will only be able to find out later on.\n\n## Finding a cozy place for our Python web scraper\n\nOne final step needs to be taken before we can get to the programming part of this web scraping tutorial: using a good coding environment. There are many options, from a simple text editor, with which simply creating a *.py file and writing the code down directly is enough, to a fully-featured IDE (Integrated Development Environment).\n\nIf you already have Visual Studio Code installed, picking this IDE would be the simplest option. Otherwise, I’d highly recommend [PyCharm](https://www.jetbrains.com/pycharm/) for any newcomer as it has very little barrier to entry and an intuitive UI. We will assume that PyCharm is used for the rest of the web scraping tutorial.\n\nIn PyCharm, right click on the project area and “New -> Python File”. Give it a nice name!\n\n## Importing and using libraries\n\nTime to put all those pips we installed previously to use:\n\n`import pandas as pd`\n\n`from bs4 import BeautifulSoup`\n\n`from selenium import webdriver\n`\nPyCharm might display these imports in grey as it automatically marks unused libraries. Don’t accept its suggestion to remove unused libs (at least yet).\n\nWe should begin by defining our browser. Depending on the webdriver we picked back in “WebDriver and browsers” we should type in:\n\n`driver = webdriver.Chrome(executable_path='c:\\path\\to\\windows\\webdriver\\executable.exe')`\n\nOR\n\n`driver = webdriver.Firefox(executable_path='/nix/path/to/webdriver/executable')`\n\n## Picking a URL\n\n![picking a URL to scrape with python](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dh79gkmk0zf1r3mxid1u.jpg)\n\nBefore performing our first test run, choose a URL. As this web scraping tutorial is intended to create an elementary application, we highly recommended picking a simple target URL:\n\n- Avoid data hidden in Javascript elements. These sometimes need to be triggered by performing specific actions in order to display the required data. Scraping data from Javascript elements requires more sophisticated use of Python and its logic.\n- Avoid image scraping. Images can be downloaded directly with Selenium.\n- Before conducting any scraping activities ensure that you are scraping public data, and are in no way breaching third-party rights. Also, don’t forget to check the robots.txt file for guidance.\n\nSelect the landing page you want to visit and input the URL into the driver.get(‘URL’) parameter. Selenium requires that the connection protocol is provided. As such, it is always necessary to attach “http://” or “https://” to the URL.\n\n`driver.get('https://your.url/here?yes=brilliant')\n`\nTry doing a test run by clicking the green arrow at the bottom left or by right clicking the coding environment and selecting ‘Run’.\n\n![Python web scraping example](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/v4vtqwhg6t1tco34cjps.jpg)\n\nIf you receive an error message stating that a file is missing then turn double check if the path provided in the driver “webdriver.*” matches the location of the webdriver executable. If you receive a message that there is a version mismatch redownload the correct webdriver executable.\n\n## Defining objects and building lists\n\nPython allows coders to design objects without assigning an exact type. An object can be created by simply typing its title and assigning a value.\n\n`# Object is “results”, brackets make the object an empty list.`\n`# We will be storing our data here.\nresults = []`\n\nLists in Python are ordered, mutable and allow duplicate members. Other collections, such as sets or dictionaries, can be used but lists are the easiest to use. Time to make more objects!\n\n`# Add the page source to the variable `content`.\ncontent = driver.page_source`\n`# Load the contents of the page, its source, into BeautifulSoup `\n`# class, which analyzes the HTML as a nested data structure and allows to select`\n`# its elements by using various selectors.\nsoup = BeautifulSoup(content)`\n\nBefore we go on with, let’s recap on how our code should look so far:\n\n`import pandas as pd\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\ndriver = webdriver.Chrome(executable_path='/nix/path/to/webdriver/executable')\ndriver.get('https://your.url/here?yes=brilliant')\nresults = []\ncontent = driver.page_source\nsoup = BeautifulSoup(content)`\n\nTry rerunning the application again. There should be no errors displayed. If any arise, a few possible troubleshooting options were outlined in earlier chapters.\n\n## Extracting data with our Python web scraper\n\nWe have finally arrived at the fun and difficult part – extracting data out of the HTML file. Since in almost all cases we are taking small sections out of many different parts of the page and we want to store it into a list, we should process every smaller section and then add it to the list:\n\n`# Loop over all elements returned by the 'findAll' call. It has the filter 'attrs' given`\n`# to it in order to limit the data returned to those elements with a given class only.\nfor element in soup.findAll(attrs={'class': 'list-item'}):\n    ...`\n\n“soup.findAll” accepts a wide array of arguments. For the purposes of this tutorial we only use “attrs” (attributes). It allows us to narrow down the search by setting up a statement “if attribute is equal to X is true then…”. Classes are easy to find and use therefore we shall use those.\n\nLet’s visit the chosen URL in a real browser before continuing. Open the page source by using CTRL+U (Chrome) or right click and select “View Page Source”. Find the “closest” class where the data is nested. Another option is to press F12 to open DevTools to select Element Picker. For example, it could be nested as:\n\n`<h4 class=\"title\">\n    <a href=\"...\">This is a Title</a>\n</h4>`\n\nOur attribute, “class”, would then be “title”. If you picked a simple target, in most cases data will be nested in a similar way to the example above. Complex targets might require more effort to get the data out. Let’s get back to coding and add the class we found in the source:\n\n`# Change ‘list-item’ to ‘title’.\nfor element in soup.findAll(attrs={'class': 'title'}):\n  ...`\n\nOur loop will now go through all objects with the class “title” in the page source. We will process each of them:\n\n`name = element.find('a')\n`\nLet’s take a look at how our loop goes through the HTML:\n\n`<h4 class=\"title\">\n    <a href=\"...\">This is a Title</a>\n</h4>`\n\nOur first statement (in the loop itself) finds all elements that match tags, whose “class” attribute contains “title”. We then execute another search within that class. Our next search finds all the `<a>` tags in the document (`<a>` is included while partial matches like `<span>` are not). Finally, the object is assigned to the variable “name”.\n\nWe could then assign the object name to our previously created list array “results” but doing this would bring the entire `<a href…>` tag with the text inside it into one element. In most cases, we would only need the text itself without any additional tags.\n\n`# Add the object of “name” to the list “results”.`\n`# '<element>.text' extracts the text in the element, omitting the HTML tags.\nresults.append(name.text)`\n\nOur loop will go through the entire page source, find all the occurrences of the classes listed above, then append the nested data to our list:\n\n`import pandas as pd\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\ndriver = webdriver.Chrome(executable_path='/nix/path/to/webdriver/executable')\ndriver.get('https://your.url/here?yes=brilliant')\nresults = []\ncontent = driver.page_source\nsoup = BeautifulSoup(content)\nfor element in soup.findAll(attrs={'class': 'title'}):\n    name = element.find('a')\n    results.append(name.text)`\n\nNote that the two statements after the loop are indented. Loops require indentation to denote nesting. Any consistent indentation will be considered legal. Loops without indentation will output an “IndentationError” with the offending statement pointed out with the “arrow”.\n\n## Exporting the data\n\n![exporting python scraped data](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kdp2alxmnkf68nhe20ql.jpg)\n\nEven if no syntax or runtime errors appear when running our program, there still might be semantic errors. You should check whether we actually get the data assigned to the right object and move to the array correctly.\n\nOne of the simplest ways to check if the data you acquired during the previous steps is being collected correctly is to use “print”. Since arrays have many different values, a simple loop is often used to separate each entry to a separate line in the output:\n\n`for x in results:\n   print(x)`\n\nBoth “print” and “for” should be self-explanatory at this point. We are only initiating this loop for quick testing and debugging purposes. It is completely viable to print the results directly:\n\n`print(results)`\n\nSo far our code should look like this:\n\n`driver = webdriver.Chrome(executable_path='/nix/path/to/webdriver/executable')\ndriver.get('https://your.url/here?yes=brilliant')\nresults = []\ncontent = driver.page_source\nsoup = BeautifulSoup(content)\nfor a in soup.findAll(attrs={'class': 'class'}):\n    name = a.find('a')\n    if name not in results:\n        results.append(name.text)\nfor x in results:\n    print(x)`\n\nRunning our program now should display no errors and display acquired data in the debugger window. While “print” is great for testing purposes, it isn’t all that great for parsing and analyzing data.\n\nYou might have noticed that “import pandas” is still greyed out so far. We will finally get to put the library to good use. I recommend removing the “print” loop for now as we will be doing something similar but moving our data to a csv file.\n\n`df = pd.DataFrame({'Names': results})\ndf.to_csv('names.csv', index=False, encoding='utf-8')`\n\nOur two new statements rely on the pandas library. Our first statement creates a variable “df” and turns its object into a two-dimensional data table. “Names” is the name of our column while “results” is our list to be printed out. Note that pandas can create multiple columns, we just don’t have enough lists to utilize those parameters (yet).\n\nOur second statement moves the data of variable “df” to a specific file type (in this case “csv”). Our first parameter assigns a name to our soon-to-be file and an extension. Adding an extension is necessary as “pandas” will otherwise output a file without one and it will have to be changed manually. “index” can be used to assign specific starting numbers to columns. “encoding” is used to save data in a specific format. UTF-8 will be enough in almost all cases.\n\n`import pandas as pd\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\ndriver = webdriver.Chrome(executable_path='/nix/path/to/webdriver/executable')\ndriver.get('https://your.url/here?yes=brilliant')\nresults = []\ncontent = driver.page_source\nsoup = BeautifulSoup(content)\nfor a in soup.findAll(attrs={'class': 'class'}):\n    name = a.find('a')\n    if name not in results:\n        results.append(name.text)\ndf = pd.DataFrame({'Names': results})\ndf.to_csv('names.csv', index=False, encoding='utf-8')`\n\nNo imports should now be greyed out and running our application should output a “names.csv” into our project directory. Note that a “Guessed At Parser” warning remains. We could remove it by installing a third party parser but for the purposes of this Python web scraping tutorial the default HTML option will do just fine.\n\n## More lists. More!\n\n\n![Python web scraping tutorial](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/055ixtk22rjoezgq6yq8.jpg)\n\nMany web scraping operations will need to acquire several sets of data. For example, extracting just the titles of items listed on an e-commerce website will rarely be useful. In order to gather meaningful information and to draw conclusions from it at least two data points are needed.\n\nFor the purposes of this tutorial, we will try something slightly different. Since acquiring data from the same class would just mean appending to an additional list, we should attempt to extract data from a different class but, at the same time, maintain the structure of our table.\n\nObviously, we will need another list to store our data in.\n\n`import pandas as pd\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\ndriver = ``webdriver.Chrome(executable_path='/nix/path/to/webdriver/executable')\ndriver.get('https://your.url/here?yes=brilliant')`\n`results = []\nother_results = []\nfor b in soup.findAll(attrs={'class': 'otherclass'}):`\n`# Assume that data is nested in ‘span’.\n    name2 = b.find('span')\n    other_results.append(name.text)`\n\nSince we will be extracting an additional data point from a different part of the HTML, we will need an additional loop. If needed we can also add another “if” conditional to control for duplicate entries:\n\nFinally, we need to change how our data table is formed:\n\n`df = pd.DataFrame({'Names': results, 'Categories': other_results})`\n\nSo far the newest iteration of our code should look something like this:\n\n`import pandas as pd\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\ndriver = webdriver.Chrome(executable_path='/nix/path/to/webdriver/executable')\ndriver.get('https://your.url/here?yes=brilliant')\nresults = []\nother_results = []\ncontent = driver.page_source\nfor a in soup.findAll(attrs={'class': 'class'}):\n    name = a.find('a'\ufeff)\n    if name not in results:\n        results.append(name.text)\nfor b in soup.findAll(attrs={'class': 'otherclass'}):\n    name2 = b.find('span')\n    other_results.append(name.text)\ndf = pd.DataFrame({'Names': results, 'Categories': other_results})\ndf.to_csv('names.csv', index=False, encoding='utf-8')`\n\nIf you are lucky, running this code will output no error. In some cases “pandas” will output an “ValueError: arrays must all be the same length” message. Simply put, the length of the lists “results” and “other_results” is unequal, therefore pandas cannot create a two-dimensional table.\n\nThere are dozens of ways to resolve that error message. From padding the shortest list with “empty” values, to creating dictionaries, to creating two series and listing them out. We shall do the third option:\n\n`series1 = pd.Series(results, name = 'Names')\nseries2 = pd.Series(other_results, name = 'Categories')\ndf = pd.DataFrame({'Names': series1, 'Categories': series2})\ndf.to_csv('names.csv', index=False, encoding='utf-8')`\n\nNote that data will not be matched as the lists are of uneven length but creating two series is the easiest fix if two data points are needed. Our final code should look something like this:\n\n`import pandas as pd\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\ndriver = webdriver.Chrome(executable_path='/nix/path/to/webdriver/executable')\ndriver.get('https://your.url/here?yes=brilliant')\nresults = []\nother_results = []\ncontent = driver.page_source\nsoup = BeautifulSoup(content)\nfor a in soup.findAll(attrs={'class': 'class'}):\n    name = a.find('a')\n    if name not in results:\n        results.append(name.text)\nfor b in soup.findAll(attrs={'class': 'otherclass'}):\n    name2 = b.find('span')\n    other_results.append(name.text)\nseries1 = pd.Series(results, name = 'Names')\nseries2 = pd.Series(other_results, name = 'Categories')\ndf = pd.DataFrame({'Names': series1, 'Categories': series2})\ndf.to_csv('names.csv', index=False, encoding='utf-8')`\n\nRunning it should create a csv file named “names” with two columns of data.\n\n## Web scraping with Python best practices\n\nOur first web scraper should now be fully functional. Of course it is so basic and simplistic that performing any serious data acquisition would require significant upgrades. Before moving on to greener pastures, I highly recommend experimenting with some additional features:\n\n- Create matched data extraction by creating a loop that would make lists of an even length.\n- Scrape several URLs in one go. There are many ways to implement such a feature. One of the simplest options is to simply repeat the code above and change URLs each time. That would be quite boring. Build a loop and an array of URLs to visit.\n- Another option is to create several arrays to store different sets of data and output it into one file with different rows. Scraping several different types of information at once is an important part of e-commerce data acquisition.\n- Once a satisfactory web scraper is running, you no longer need to watch the browser perform its actions. Get headless versions of either Chrome or Firefox browsers and use those to reduce load times.\n- Create a scraping pattern. Think of how a regular user would browse the internet and try to automate their actions. New libraries will definitely be needed. Use “import time” and “from random import randint” to create wait times between pages. Add “scrollto()” or use specific key inputs to move around the browser. It’s nearly impossible to list all of the possible options when it comes to creating a scraping pattern.\n- Create a monitoring process. Data on certain websites might be time (or even user) sensitive. Try creating a long-lasting loop that rechecks certain URLs and scrapes data at set intervals. Ensure that your acquired data is always fresh.\n- Make use of the [Python Requests](https://oxylabs.io/blog/python-requests) library. Requests is a powerful asset in any web scraping toolkit as it allows to optimize HTTP methods sent to servers.\n- Finally, integrate proxies into your web scraper. Using location specific request sources allows you to acquire data that might otherwise be inaccessible.\n\n## Conclusion\n\nSo, in this extensive Python tutorial, we outlined every step you need to complete to get started with a simple application. \n\nBut, from here onwards, you are on your own. Building web scrapers in Python, acquiring data and drawing conclusions from large amounts of information is inherently an interesting and complicated process.\n\nWe hope this tutorial was valuable for you and encourage you to stay tuned for more informative posts from Oxylabs!",n.user={name:r,username:t,twitter_username:t,github_username:e,website_url:"https://oxylabs.io/",profile_image:"https://res.cloudinary.com/practicaldev/image/fetch/s--1JrN4Wv8--/c_fill,f_auto,fl_progressive,h_640,q_auto,w_640/https://dev-to-uploads.s3.amazonaws.com/uploads/user/profile_image/842926/06bdacf5-c05e-4bfd-a43f-f0733fe4f46f.png",profile_image_90:"https://res.cloudinary.com/practicaldev/image/fetch/s--wXLkwoZr--/c_fill,f_auto,fl_progressive,h_90,q_auto,w_90/https://dev-to-uploads.s3.amazonaws.com/uploads/user/profile_image/842926/06bdacf5-c05e-4bfd-a43f-f0733fe4f46f.png"},n.organization={name:r,username:i,slug:i,profile_image:"https://res.cloudinary.com/practicaldev/image/fetch/s--tonKEJV5--/c_fill,f_auto,fl_progressive,h_640,q_auto,w_640/https://dev-to-uploads.s3.amazonaws.com/uploads/organization/profile_image/5508/3bfe0802-036d-41ab-841c-8741a9da989c.png",profile_image_90:"https://res.cloudinary.com/practicaldev/image/fetch/s--9u8NTdiO--/c_fill,f_auto,fl_progressive,h_90,q_auto,w_90/https://dev-to-uploads.s3.amazonaws.com/uploads/organization/profile_image/5508/3bfe0802-036d-41ab-841c-8741a9da989c.png"},{layout:"default",data:[{}],fetch:{"data-v-70afb46a:0":{article:n}},error:e,state:{currentArticle:n},serverRendered:!0,routePath:"/oxylabs/1087995",config:{_app:{basePath:"/nuxtstop/",assetsPath:"/nuxtstop/_nuxt/",cdnURL:e}}}}(null,"oxylabs",{},"2022-05-18T07:00:57Z","Oxylabs","oxylabs-io")</script><script src="/nuxtstop/_nuxt/f6e87fb.js" defer></script><script src="/nuxtstop/_nuxt/dc9ce94.js" defer></script><script src="/nuxtstop/_nuxt/6474719.js" defer></script><script src="/nuxtstop/_nuxt/9b75090.js" defer></script><script src="/nuxtstop/_nuxt/18df600.js" defer></script>
  </body>
</html>
